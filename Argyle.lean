import Mathlib.Tactic.LibrarySearch
import Mathlib.Tactic.NthRewrite
import Mathlib.Mathport.Syntax
import Std.Tactic.ShowTerm
import Lean.Meta.Tactic.Simp.Main
import Mathlib.Tactic.Basic
import Mathlib.Data.List.Sigma

import Lean.Parser.Tactic
-- import Graph.Graph
-- import Graph.TopologicalSort
import Mathlib.Init.Set
import Mathlib.Data.List.Defs
import Mathlib.Init.Propext
import Mathlib.Data.Set.Basic
import Mathlib.Logic.Basic
import Mathlib.Logic.Function.Basic

-- open Graph
open Set
open Tactic
open Classical

-- Doesn't detect inefficient code!
set_option maxHeartbeats 0

-------------------------------------------------
-- Goofing about with inductive types
-------------------------------------------------

inductive my_lte : â„• â†’ â„• â†’ Prop where
  | reflexive : my_lte n n
  | from_succ : my_lte m x â†’ (n = x + 1) â†’ my_lte m n

-- #eval my_lte 1 3





-------------------------------------------------
-- List comprehensions,
-- courtesy of lovettchris
-- See: 
--   https://github.com/leanprover/lean4-samples/blob/main/ListComprehension/ListComprehension.lean
-------------------------------------------------

declare_syntax_cat compClause
syntax "for " term " in " term : compClause
syntax "if " term : compClause

syntax "[" term " | " compClause,* "]" : term

def List.map' (xs : List Î±) (f : Î± â†’ Î²) : List Î² := List.map f xs

macro_rules
  | `([$t:term |]) => `([$t])
  | `([$t:term | for $x in $xs]) => `(List.map' $xs  (Î» $x => $t))
  | `([$t:term | if $x]) => `(if $x then [$t] else [])
  | `([$t:term | $c, $cs,*]) => `(List.join [[$t | $cs,*] | $c])

def prod_comprehens (xs : List Î±) (ys : List Î²) : List (Î± Ã— Î²) := 
  [(x, y) | for x in xs, for y in ys]

#eval [(x, y) | for x in [1, 2], for y in [3, 4]]


-------------------------------------------------
-- Weighted Directed Graphs
-- 
-- Since graphs are internally represented by lists,
-- we can just do induction on this inner list.
-- 
-- It's common for graph theory proofs to go by
-- induction on the graph:  "Suppose P holds for the
-- subgraph, and now add a new node."  In this case
-- what you probably want is to do induction on
-- *Graph.vertices.reverse*, so that each new node
-- can only "see" those nodes that come *before* it.
-- 
-- NOTE: For hygiene, we expect that the vertices
-- are natural numbers given in order, i.e. 0, 1, 2, ...
-- In principle, you can pick any other labels for your 
-- vertices, but I will be relying on the fact that they come
-- in this order.  My apologies.
-- 
-- WARNING: We will also assume that graphs are acyclic.
-- But NOTHING in this file directly enforces that.
-- Whenever I construct a new graph, I will check that
-- it preserves acyclic-ness.  But when making a graph
-- from scratch, kindly refrain from creating cycles.
-- 
-- These graphs are based on Peter Kementzey's 
-- implementation, but modified slightly.
-------------------------------------------------

-- Î± is the type of the nodes
-- Î² is the type of the weights
structure Vertex (Î± Î² : Type) where
  label : Î± 
  successors : List (â„• Ã— Î²)
deriving Repr

instance [Inhabited Î±] : Inhabited (Vertex Î± Î²) := 
  âŸ¨ { label := default, successors := default } âŸ©

structure Graph (Î± : Type) (Î² : Type) where
  vertices : List (Vertex Î± Î²) := []
deriving Repr

-- Notice that this graph is acyclic, since each predecessor
-- list only refers to nodes above the current node.  This
-- is foreshadowing.
def graphA : Graph â„• Float :=
  âŸ¨[âŸ¨0, [âŸ¨1, 0.5âŸ©, âŸ¨2, 0.6âŸ©, âŸ¨3, 0.7âŸ©]âŸ©, 
    âŸ¨1, [âŸ¨2, 0.8âŸ©, âŸ¨3, 0.9âŸ©]âŸ©, 
    âŸ¨2, [âŸ¨3, 1.0âŸ©, âŸ¨3, 3.0âŸ©]âŸ©, 
    âŸ¨3, []âŸ©]âŸ©

#check graphA
#eval graphA

-------------------------------------------------
-- Graph functions
-------------------------------------------------

-- TODO: for convenience, define 'map', 'foldl', 'filter',
-- 'find?', 'zip', 'some', 'any', and 'all' over graph vertices.

-- TODO: Make graph functions computable! (this shouldn't be
-- hard -- right now it just depends on 'Prop')
namespace Graph

def get_vertices (g : Graph â„• Float) : List â„• :=
  List.map (fun âŸ¨label, _âŸ© => label) g.vertices

-- Helper function to collect the List of pairs of n's successors
def successor_pairs (vertices : List (Vertex â„• Float)) (n : â„•) : List (â„• Ã— Float) :=
  match vertices with
  | [] => []
  | âŸ¨vertex, succâŸ© :: rest => 
    if vertex = n then succ 
    else successor_pairs rest n

-- We get all vertices that are in n's successor list
def successors (g : Graph â„• Float) (n : â„•) : List â„• :=
  List.filter 
    (fun m => m âˆˆ (successor_pairs g.vertices n).unzip.1) 
    g.get_vertices

  -- List.get n g.vertices -- successors.unzip.1
  -- g.vertices[n]!.successors.unzip.1


def predecessors (g : Graph â„• Float) (n : â„•) : List â„• :=
  List.filter (fun v => n âˆˆ (g.successors v)) g.get_vertices
  
  -- List.map (fun âŸ¨label, _âŸ© => label) 
  --   (List.filter (fun v => n âˆˆ (g.successors v)) g.vertices)

  -- List.filter 
  --   (fun m => n âˆˆ (g.successors m)) 
  --   g.get_vertices

-- Using 'predecessors' is slower than 'successors',
-- but we will tend to look backwards from a node rather
-- than forwards.
def hasEdge (g : Graph â„• Float) (m n : â„•) : Bool :=
  m âˆˆ (g.predecessors n)

-- Returns the weight of the edge m âŸ¶ n, if it exists.
-- If it does not exist, we say the weight is 0.0
-- TODO: In the future, it's better to use Option here
-- and return none if none!!!
def getEdgeWeight (g : Graph â„• Float) (m n : â„•) : Float :=
  match g.vertices[m]!.successors.find? (fun âŸ¨label, _âŸ© => label = n) with
  | some âŸ¨_, weightâŸ© => weight
  | none => 0.0

-- Some sanity checks
#eval get_vertices graphA
#eval successors graphA 0
#eval successors graphA 1
#eval successors graphA 2
#eval successors graphA 3
#eval predecessors graphA 0
#eval predecessors graphA 1
#eval predecessors graphA 2
#eval predecessors graphA 3
#eval hasEdge graphA 1 2
#eval hasEdge graphA 1 3
#eval hasEdge graphA 4 2
#eval getEdgeWeight graphA 1 2
#eval getEdgeWeight graphA 1 3
#eval getEdgeWeight graphA 4 2


inductive hasPath (g : Graph â„• Float) : â„• â†’ â„• â†’ Prop where
  | trivial {u : â„•} :
      hasPath g u u
  | from_path {u v w : â„•} : 
      hasPath g u v â†’ hasEdge g v w â†’ hasPath g u w

/-
TODO for later:  Make 'hasPath' computable so that we can execute
this code:
> #eval hasPath graphA 1 3

Some old code when I was trying to do this:

instance decPath : Decidable (hasPath g u v) :=
  sorry -- this should implement BFS!!!
  -- if h : u = v then
  --   isTrue (Eq.subst h hasPath.trivial)
  -- else if h : hasEdge g u v then
  --   isTrue (hasPath.from_path (hasPath.trivial) h)
  -- else
  --   sorry

instance decLte : Decidable (my_lte m n) :=
  if h : m = n then
    .isTrue (h â–¸ .trivial)
  else
    match n with
    | x + 1 =>
      have := @decLte m x
      decidable_of_iff (my_lte m x) âŸ¨(.from_path Â· rfl), fun h => by
        cases h with
        | trivial => cases h rfl
        | from_path h e => exact Nat.succ.inj e â–¸ hâŸ©
    | 0 => .isFalse fun h => by
      cases h with
      | trivial => exact h rfl
      | from_path h e => cases e
-/


--------------------------------------------------------------------
theorem hasPath_trans {u v w : â„•} (g : Graph â„• Float) :
  hasPath g u v â†’ hasPath g v w â†’ hasPath g u w := by
--------------------------------------------------------------------
  intro (hâ‚ : hasPath g u v)
  intro (hâ‚‚ : hasPath g v w)

  induction hâ‚‚
  case trivial => exact hâ‚
  case from_path x y _ edge_xy path_ux => 
    exact hasPath.from_path path_ux edge_xy


def is_refl (g : Graph â„• Float) : Prop := âˆ€ (u : â„•), 
  u âˆˆ g.get_vertices â†’ g.hasEdge u u

def is_symm (g : Graph â„• Float) : Prop := âˆ€ (u v : â„•), 
  g.hasEdge u v â†’ g.hasEdge v u

def is_trans (g : Graph â„• Float) : Prop := âˆ€ (u v w : â„•),
  g.hasEdge u v â†’ g.hasEdge v w â†’ g.hasEdge u w

def is_acyclic (g : Graph â„• Float) : Prop := âˆ€ (u v : â„•),
  g.hasPath u v â†’ g.hasPath v u â†’ u = v

end Graph

/-
TODO:  Define 'acyclic' as:  Recursively on graph.vertices,
every vertex can only "see" the vertices ahead of it.

TODO: We want to be able to check if a graph is acyclic by
just "computing" it -- i.e. we call Topological Sort on the
graph, and if successful we know it is acyclic.

So here is some old code I was using to try to do topological
sort.  I'll need to come back to this when I want to make
everything in this library computable.
namespace TopologicalSort

-- @[simp]
-- def topol_sort (g : Graph â„• Float) :=
--   (topSortUnsafe g).toList.reverse

-- holds iff u precedes v in array
-- note that we assume lst elements are all distinct
def list_precedes (lst : List â„•) (u v : â„•) : Bool :=
  match lst with
    | List.nil => false
    | List.cons x xs =>
      -- If we find 'u' first, and v is in the rest, true
      if x = u âˆ§ v âˆˆ xs then 
        true
      else 
        list_precedes xs u v

def listA : List â„• :=
  [2, 4, 9, 8, 5]

-- a couple of unit tests for good measure
#eval list_precedes listA 4 8 -- true
#eval list_precedes listA 2 8 -- true
#eval list_precedes listA 2 4 -- true
#eval list_precedes listA 2 9 -- true
#eval list_precedes listA 9 5 -- true

#eval list_precedes listA 8 2 -- should be false, is true
#eval list_precedes listA 5 9 -- should be false, is true

#eval list_precedes listA 1 7 -- undefined (false)
#eval list_precedes listA 9 9 -- false, makes sure an element
                              -- does not precede itself.

-- The ordering induced by Topological Sort
-- TODO: Rewrite as an inductive data type!
/-
def topOrder (g : Graph â„• Î²) (u v : â„•) : Prop :=
  match (topSort g) with
    | some sorted => list_precedes sorted.toList u v
    | none => sorry
-/

-- inductive TopologicalOrdering (g : Graph â„• Î²) (u : â„•) where
--   | constr1 : TopologicalOrdering g u
--   | constr2 (x : â„•) : TopologicalOrdering g u

-- inductive graph_â‰º (g : Graph â„• Î²) (u v : â„•) where
--   | constr1 : sorry
--   | constr2 : sorry

-- Says that Topological Sort is actually correct, i.e.
-- if there is an edge from x to y, then x â‰º y in the ordering.
-- theorem topSort_is_ordered (g : Graph â„• Î²) (u v : â„•) :
--   g.hasEdge u v â†’ topOrder g u v := by

--   intro (hâ‚ : hasEdge g u v)
--   rw [topOrder]
--   sorry

end TopologicalSort
-/

-------------------------------------------------
-- Example:  Our graphA is acyclic
-- (We should just be able to call 'Topological Sort'
-- on the graph and check if that is successful.)
-------------------------------------------------
-- Put in examples file! (and figure it out later, we don't need
-- it right now)
-- 
-- theorem graphA_is_acyclic : graphA.is_acyclic := by
--   intro (u : â„•) (v : â„•)
--         (path_uv : hasPath graphA u v)
--         (path_vu : hasPath graphA v u)

--   sorry

-------------------------------------------------
-- Activation functions
-------------------------------------------------
def binary_step (x : Float) : Float :=
  if x > 0.0 then
    1.0
  else
    0.0

/-
TODO: If I want to do this the *right* way, I should define
a type of Real numbers that wrap around Floats.  I can *compute*
things at the Float level, and *prove* things at the Reals level,
but I should refuse to let the user *prove* things at the Float
level. i.e. they cannot prove things by evaluating Floats; this
is where Lean's Floats run into contradictions.
-/
axiom le_refl_float : âˆ€ (x : Float), x â‰¤ x
axiom lt_or_ge_float : âˆ€ (x y : Float), x < y âˆ¨ x â‰¥ y
axiom le_not_lt_float : âˆ€ (x y : Float), x â‰¤ y â†’ Â¬ (y < x)
axiom lt_le_lt_float : âˆ€ (x y z : Float), x < y â†’ y â‰¤ z â†’ x < z
axiom le_eq_le_float : âˆ€ (x y z : Float), x â‰¤ y â†’ y = z â†’ x â‰¤ z
axiom eq_le_le_float : âˆ€ (x y z : Float), x = y â†’ y â‰¤ z â†’ x â‰¤ z
axiom zero_lt_one_float : 0.0 < 1.0
axiom le_of_lt_float : âˆ€ (x y : Float), x < y â†’ x â‰¤ y
axiom not_le_float : âˆ€ (x y : Float), x < y â†’ Â¬ (y â‰¤ x)
axiom zero_mult : âˆ€ (x : Float), x * 0.0 = 0.0
axiom one_mult : âˆ€ (x : Float), x * 1.0 = x
axiom zero_plus : âˆ€ (x : Float), x + 0.0 = x
axiom commutative_mult_float : âˆ€ (x y : Float), x * y = y * x

theorem binary_step_is_binary (x : Float) :
    (binary_step x = 0.0) âˆ¨ (binary_step x = 1.0) :=
    by
      -- simp [binary_step]

      cases (lt_or_ge_float 0.0 x) with

      -- Case 1: 0.0 < x
      | inl case1 =>
          have (h : binary_step x = 1.0) :=
            by
              simp only [binary_step]
              rw [(if_pos case1)]
          exact Or.inr h

      -- Case 2: Â¬ (0.0 < x)
      | inr case2 =>
          have (h : binary_step x = 0.0) := 
            by 
              simp only [binary_step]
              rw [(if_neg (le_not_lt_float x 0.0 case2))]
          exact Or.inl h

-- Proof that binary_step is nondecreasing
-- This is also a 'hello world' to see if I can
-- reason about a branching program.
theorem binary_step_nondecr (xâ‚ xâ‚‚ : Float) (hyp : xâ‚ â‰¤ xâ‚‚) :
  (binary_step xâ‚ â‰¤ binary_step xâ‚‚) := 
  by
    -- Simplify by applying the definition of binary_step.
    simp [binary_step]
    
    cases (lt_or_ge_float 0.0 xâ‚) with
    | inl case1 =>
      cases (lt_or_ge_float 0.0 xâ‚‚) with
      | inl case11 => 
          -- Both sides evaluate to 1.0,
          -- so we just prove that 1.0 â‰¤ 1.0.
          rw [(if_pos case1)]
          rw [(if_pos case11)]
          exact le_refl_float 1.0
      | inr case12 => 
          -- We have 0.0 < xâ‚ â‰¤ xâ‚‚ < 0.0,
          -- so this case is absurd. 
          exact absurd
            (lt_le_lt_float 0.0 xâ‚ xâ‚‚ case1 hyp) -- library_search!!! 
            (le_not_lt_float xâ‚‚ 0.0 case12)
    | inr case2 => 
      cases (lt_or_ge_float 0.0 xâ‚‚) with
      | inl case21 => 
          -- We are in the second and first cases.
          rw [(if_neg (le_not_lt_float xâ‚ 0.0 case2))]
          rw [(if_pos case21)]
          exact (le_of_lt_float _ _ zero_lt_one_float)
      | inr case22 => 
          rw [(if_neg (le_not_lt_float xâ‚ 0.0 case2))]
          rw [(if_neg (le_not_lt_float xâ‚‚ 0.0 case22))]
          exact le_refl_float 0.0 -- library_search!!!

-------------------------------------------------
-- Feedforward neural nets
-------------------------------------------------
structure Net where
  graph : Graph â„• Float
  activation : Float â†’ Float
  rate : Float -- learning rate

structure BFNN extends Net where 
  -- The activation function is binary
  binary : âˆ€ (x : Float), 
    (activation x = 0.0) âˆ¨ (activation x = 1.0)

  -- The activation function is nondecreasing
  activ_nondecr : âˆ€ (xâ‚ xâ‚‚ : Float),
    xâ‚ â‰¤ xâ‚‚ â†’ activation xâ‚ â‰¤ activation xâ‚‚

  -- There is *some* x for which the activation is 1.0
  activ_pos : âˆƒ (x : Float), activation x = 1.0


-- Because our activation function is bounded above by 1.0,
-- if act(xâ‚) = 1.0
-- then any act(xâ‚‚) greater than act(xâ‚) also = 1.0
--------------------------------------------------------------------
lemma activation_from_inequality (net : BFNN) (xâ‚ xâ‚‚ : Float) :
  net.activation xâ‚ â‰¤ net.activation xâ‚‚
  â†’ net.activation xâ‚ = 1.0 â†’ net.activation xâ‚‚ = 1.0 := by
--------------------------------------------------------------------
  intro hâ‚ hâ‚‚
  cases net.binary xâ‚‚
  case inr actxâ‚‚_is_one => exact actxâ‚‚_is_one
  case inl actxâ‚‚_is_zero => 
    -- This case is impossible! 1 is not â‰¤ 0!
    rw [hâ‚‚] at hâ‚
    rw [actxâ‚‚_is_zero] at hâ‚
    exact absurd hâ‚ (not_le_float _ _ zero_lt_one_float)

-- Put in examples file!  (We don't need to figure it out
-- right now!)
-- def myBFNN : BFNN :=
--   {
--     graph := graphA
--     activation := binary_step
--     rate := 1.0

--     binary := binary_step_is_binary
--     -- sort := (topSortUnsafe graphA).toList.reverse
--     acyclic := sorry -- graphA_is_acyclic
--     activ_nondecr := binary_step_nondecr
--     activ_pos := sorry
--   }

-- inductive Layer (net : BFNN) : List â„• â†’ Prop where
--   | initial_layer : Layer net Nâ‚€
--   | next_layer : âˆ€ (n : â„•), (n âˆˆ N â†’ 
--     âˆƒ (m : â„•), m âˆˆ Náµ¢ âˆ§ Layer net Náµ¢) â†’ Layer net N

variable (net : BFNN)

-------------------------------------------------
-- Playing around with Sets
-------------------------------------------------

def setA : Set â„• :=
  {n | n â‰¤ 10}

def setB : Set â„• :=
  {n âˆˆ setA | n > 5}

def setC : Set â„• :=
  {n | n â‰¤ 5}

#check setA

-- Example proof of a subset, just to make
-- sure I can do it.
example : setB âŠ† setA := by
  intro (n : â„•)
  intro (h : n âˆˆ setB)

  exact show n âˆˆ setA from h.left

-- Another example proof of a subset, this
-- time using the RHS of the set comprehension.
example : setC âŠ† setA := by
  intro (n : â„•)
  intro (hâ‚ : n âˆˆ setC)

  have (hâ‚‚ : n â‰¤ 5) := hâ‚
  have (hâ‚ƒ : 5 â‰¤ 10) := (by native_decide)
  exact show n âˆˆ setA from le_trans hâ‚‚ hâ‚ƒ

-- Prove that a set is contained in its powerset
example : âˆ€ (S : Set Î±), S âˆˆ ğ’« S := by
  intro (S : Set Î±)
  intro (a : Î±)
  intro (h : a âˆˆ S)
  exact h

-- TODO Next: Define graph reachability and propagate
-- Prove that the above BFNN is acyclic, just to make sure
-- we have the right tools for the job.


theorem setExample : 3 âˆˆ setC := by 
  have (hâ‚ : 3 â‰¤ 4) := by native_decide
  constructor
  exact hâ‚

/-â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Forward propagation in a net
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•-/

def weighted_sum (weights : List Float) (lst : List Float) : Float :=
  List.sum [w * x | for w in weights, for x in lst]

#eval weighted_sum [] []
#eval weighted_sum [1.0] [3.0]
#eval weighted_sum [1.0, 2.0, 3.0] [5.0, 5.0, 5.0]

-- Not well-defined behavior (we expect the weights and lst to be of equal size,
-- but this is left implicit.)
#eval weighted_sum [1.0, 2.0] [3.0]

/-
(List.bind (List.range (List.length (Graph.predecessors net.toNet.graph n))) fun i =>
        pure (Graph.getEdgeWeight (hebb_star net A).toNet.graph (List.get! (Graph.predecessors net.toNet.graph n) i) n))
-/

lemma weighted_sum_eq (fwâ‚ fwâ‚‚ fxâ‚ fxâ‚‚ : â„• â†’ Float) (ls : List â„•) :
  let xâ‚ : List Float := do
    let i â† List.range (List.length ls)
    return fxâ‚ i
  let xâ‚‚ : List Float := do
    let i â† List.range (List.length ls)
    return fxâ‚‚ i
  let wâ‚ : List Float := do
    let i â† List.range (List.length ls)
    return fwâ‚ i
  let wâ‚‚ : List Float := do
    let i â† List.range (List.length ls)
    return fwâ‚‚ i
  
  (âˆ€ i, (fwâ‚ i) * (fxâ‚ i) = (fwâ‚‚ i) * (fxâ‚‚ i))
  â†’ weighted_sum wâ‚ xâ‚ = weighted_sum wâ‚‚ xâ‚‚ := by

  intro xâ‚ xâ‚‚ wâ‚ wâ‚‚ hâ‚
  simp only [weighted_sum]
  congr 2
  sorry


lemma weighted_sum_le (wâ‚ wâ‚‚ xâ‚ xâ‚‚ : List Float) :
  (âˆ€ i, (wâ‚.get! i) * (xâ‚.get! i) â‰¤ (wâ‚‚.get! i) * (xâ‚‚.get! i)) 
  â†’ weighted_sum wâ‚ xâ‚ â‰¤ weighted_sum wâ‚‚ xâ‚‚ := by

  intro hâ‚
  simp only [weighted_sum]
  sorry
  

-- WARNING:
-- This is actually FALSE!  For infinite sets, l[i] is not provably
-- in l (as far as I can figure.)
-- TODO: In the future, when making all this computable, I will
-- be using finite sets, and then I can use get instead of get!,
-- and get_mem in the standard library.
axiom get!_mem {Î± : Type} [Inhabited Î±] : 
  âˆ€ (l : List Î±) i, (l.get! i) âˆˆ l

@[simp]
def preds (net : BFNN) (n : â„•): List â„• :=
  net.graph.predecessors n

--------------------------------------------------------------------
theorem edge_from_preds (net : BFNN) (m n : â„•) :
  m âˆˆ preds net n â†” net.graph.hasEdge m n := by
--------------------------------------------------------------------
  simp only [preds, Graph.hasEdge]
  rw [Bool.decide_iff]


-- (Weightless) *minimal* graph distance from node m to n.  Just count
-- the number of edges, i.e. don't apply weights.
-- (just here in order to define layer -- but if there's
--  a better way, I should use it!)
-- def distance (graph : Graph â„• Float) (m n : â„•) : â„• :=
--   sorry

/-
def my_argmax (f : Î± â†’ Î²) (l : List Î±) : Option Î± :=
  l.foldl (argAux fun b c => f c < f b) none
-/

-- The layer of n.
-- We get all predecessors of n, and take the predecessor m
-- with the *maximum* layer.  Then layer(n) = layer(m) + 1.
-- If n has no predecessors, then layer(n) = 0.
-- 
-- TODO: Prove terminating if I can!  (What exactly is decreasing
--       here...)
-- def layer (net : BFNN) (n : â„•) : â„• :=
--   sorry
-- partial

-- TODO: I can do away with this axiom, and define 'layer'
-- more naturally if I define acyclic graphs recursively
-- in the first place!  Then I can do induction on 'net.graph'!
axiom graph_ascending_order : âˆ€ (g : Graph â„• Float) (m n : â„•), 
  m âˆˆ g.predecessors n â†’ m < n

-- Accumulator-style helper function for 'layer'
-- Defined recursively on the *reverse* of the vertex list
-- (this means we are looking at vertices backwards -- each
--  vertex can only "see" the vertices preceding it.)
def layer_acc (net : BFNN) (n : â„•) (ls : List â„•) : â„• :=
  match ls with
  | [] => 0
  | v :: rest =>
    if v = n then
      let layers := List.map (fun x => layer_acc net x rest) (preds net n)
      let max := layers.maximum

      match max with
      | some L => L + 1
      | none => 0

    else layer_acc net n rest

def layer (net : BFNN) (n : â„•) : â„• :=
  layer_acc net n (net.graph.get_vertices.reverse)

-- The layer relation layer[m] â‰¤ layer[n] is well-founded
-- (i.e. it has a least element)
--------------------------------------------------------------------
lemma layer_wellfounded (net : BFNN) : 
  WellFounded (fun x y => layer net x â‰¤ layer net y) := by
--------------------------------------------------------------------
  exact WellFounded.wellFounded_iff_has_min.mpr 
    (fun S hS => sorry)


/-
-- Put in test file!
-- 0 jumps to 2, 1 jumps to 3, short connection from 1 to 2
def layer_test_graph : Graph â„• Float :=
  âŸ¨[âŸ¨0, [âŸ¨2, 0.5âŸ©]âŸ©, -- âŸ¨4, 0.5âŸ©
    âŸ¨1, [âŸ¨2, 0.5âŸ©, âŸ¨3, 0.5âŸ©]âŸ©, -- âŸ¨4, 0.5âŸ©
    âŸ¨2, []âŸ©, -- âŸ¨4, 0.5âŸ©
    âŸ¨3, [âŸ¨4, 0.5âŸ©]âŸ©, -- remove âŸ¨4, 0.5âŸ©
    
    -- Add a new edge, and toggle which previous edge jumps to it.
    âŸ¨4, []âŸ©]âŸ©

def layer_test_net : BFNN :=
  { graph := layer_test_graph, activation := binary_step, rate := 1.0,
    binary := binary_step_is_binary,
    activ_nondecr := binary_step_nondecr, activ_pos := sorry
  }

#eval layer layer_test_net 0
#eval layer layer_test_net 1
#eval layer layer_test_net 2
#eval layer layer_test_net 3
#eval layer layer_test_net 4
-/

-- AXIOM: We assume the net is fully connected!
-- This is essentially the statement we need, which might
-- follow from being fully connected.
-- TODO: Put this in the definition of BFNN!!!
axiom connected : âˆ€ (net : BFNN) (m n : â„•), 
  layer net m < layer net n â†’ net.graph.hasEdge m n

-- If m is a predecessor of n, then it must be in a previous layer.
-- Proof idea:
-- layer(m)  â‰¤  max({layer(v) | v âˆˆ preds(n)})  <  layer(n)
--------------------------------------------------------------------
lemma preds_decreasing (net : BFNN) (m n : â„•) :
  m âˆˆ preds net n 
  â†’ layer net m < layer net n := by
--------------------------------------------------------------------
  intro hâ‚
  simp only [layer]

  generalize hls : (List.reverse (Graph.get_vertices net.toNet.graph)) = ls
  induction ls
  case nil =>
    -- This case is impossible;
    -- m âˆˆ preds(n) means that there is *something* in the graph.
    -- This contradicts the fact that the graph is empty!
    simp [preds, Graph.predecessors, Graph.get_vertices] at hâ‚
    simp [Graph.get_vertices] at hls
    rw [hls] at hâ‚
    rw [List.map_nil] at hâ‚
    rw [List.filter_nil] at hâ‚
    exact False.elim ((List.mem_nil_iff _).mp hâ‚)
    

  case cons v rest IH =>
    simp only [layer_acc]
    generalize hmax_m : (List.map (fun x => layer_acc net x rest) (preds net m)).maximum = max_m
    generalize hmax_n : (List.map (fun x => layer_acc net x rest) (preds net n)).maximum = max_n

    -- We branch out all of the possible cases
    -- (we have 4 branches from the 'if-then-else's, 
    -- and more for the 'match'es)
    by_cases v = m
    case pos => 
      rw [if_pos h]
      by_cases v = n
      case pos => 
        rw [if_pos h]
        
        match max_n with
        | none => -- This case is also impossible;
          -- m âˆˆ preds(n) means that there is *some* maximum in preds(n),
          -- which contradicts the fact that the max is empty.
          sorry

        | some L => 
          match max_m with
          | none => exact Nat.succ_pos L
          | some K => -- This is the tricky case!
            simp
            sorry

      case neg => 
        rw [if_neg h]
        match max_m with
        | none => 
          simp
          sorry
        | some K => sorry

    case neg =>
      rw [if_neg h]
      by_cases v = n
      case pos => 
        rw [if_pos h]
        match max_n with
        | none => sorry
        | some L => sorry

      case neg => 
        rw [if_neg h]
        exact IH sorry

noncomputable
def activ (net : BFNN) (prev_activ : List Float) (n : â„•) : Prop :=
  let preds := preds net n
  let weights := do
    let i <- List.range preds.length
    let m := preds.get! i
    return net.graph.getEdgeWeight m n
  let weight_sum := weighted_sum weights prev_activ
  let curr_activ := net.activation weight_sum
  curr_activ = 1.0


-- FORWARD PROPAGATION IN A NET
-- By recursion on the layers of our feedforward network.
-- (_acc indicates that we are using the layer as an accumulator
--  to do recursion.)
-- 
-- n âˆˆ propagate_acc(S) iff either:
--   Base Case (L=0): n âˆˆ S
--   Rcrs Case (Lâ‰¥0): n âˆˆ S, or the nodes m preceding n activate n.
--     (We check their activation values via propagate_acc on m)
-- 
-- TODO: Make this computable!!!
-- change return type to 'Bool' instead of 'Prop'
-- and change 'Set' to be a finite set
-- and change net.graph to be finite as well!
-- 
-- Then unit-test all this with #eval!
@[simp]
def propagate_acc (net : BFNN) (S : Set â„•) (n : â„•) (L : â„•) : Prop :=
  match L with
  | Nat.zero => n âˆˆ S
  | Nat.succ _ =>
    let preds := preds net n
    let prev_activ := do
      let i <- List.range preds.length
      let m := preds.get! i
      return if propagate_acc net S m (layer net m) then 1.0 else 0.0
    n âˆˆ S âˆ¨ activ net prev_activ n
termination_by propagate_acc net S n L => layer net n
decreasing_by exact preds_decreasing net m n (get!_mem preds i)


-- Set variation of propagate
@[simp]
def propagate (net : BFNN) (S : Set â„•) : Set â„• :=
  fun n => propagate_acc net S n (layer net n)

-------------------------------------------------
-- Some helper lemmas
-- (just to clean up the monstrous proofs ahead!)
-- 
-- TODO: Clean these up with nicer @simp lemmas about
-- propagate and activ
-------------------------------------------------

--------------------------------------------------------------------
lemma simp_propagate_acc (net : BFNN) :
  n âˆ‰ S
  â†’ propagate_acc net S n (Nat.succ L) =
  let preds := preds net n
  let prev_activ := do
    let i <- List.range preds.length
    let m := preds.get! i
    return if propagate_acc net S m (layer net m) then 1.0 else 0.0
  activ net prev_activ n := by
--------------------------------------------------------------------
  intro (hâ‚ : n âˆ‰ S)
  apply Iff.to_eq
  apply Iff.intro

  case mp => 
    intro hâ‚‚
    simp only [propagate_acc]
    simp only [propagate_acc] at hâ‚‚
    
    cases hâ‚‚
    case inl hâ‚ƒ => contradiction
    case inr hâ‚ƒ => exact hâ‚ƒ 
  
  case mpr => 
    intro hâ‚‚
    simp only [propagate_acc]
    simp only [propagate_acc] at hâ‚‚
    exact Or.inr hâ‚‚


-- If A and B agree on all the predecessors of n, then they agree on n.
-- This lemma is extremely inefficient to use.  In order to make it
-- remotely usable, we've simplified everything down to unreadable
-- garbage.  In order to make use of it, I recommend:
--   - Applying 'simp' to your 'activ' hypotheses (get rid of any let's)
--   - Use 'exact' instead of 'apply' (exit tactic mode)
-- 
-- Here is the original unsimplified statement of the lemma:
/-
--------------------------------------------------------------------
lemma activ_agree (net : BFNN) (A B : Set â„•) (n : â„•) :
  let preds := preds net n
  let prevâ‚ := do
    let i <- List.range preds.length
    let m := preds.get! i
    return if m âˆˆ A then 1.0 else 0.0
  let prevâ‚‚ := do
    let i <- List.range preds.length
    let m := preds.get! i
    return if m âˆˆ B then 1.0 else 0.0

  (âˆ€ (m : â„•), m âˆˆ preds â†’ (m âˆˆ A â†” m âˆˆ B))
  â†’ activ net prevâ‚ n
  â†’ activ net prevâ‚‚ n := by
--------------------------------------------------------------------
-/
--------------------------------------------------------------------
lemma activ_agree (net : BFNN) (A B : Set â„•) (n : â„•) :
  (âˆ€ (m : â„•), m âˆˆ preds net n â†’ (m âˆˆ A â†” m âˆˆ B))
  â†’ activ net (List.bind (List.range (List.length (Graph.predecessors net.toNet.graph n))) fun i =>
    pure
      (if A (List.get! (Graph.predecessors net.toNet.graph n) i) 
      then 1.0 else 0.0)) n
  â†’ activ net (List.bind (List.range (List.length (Graph.predecessors net.toNet.graph n))) fun i =>
    pure
      (if B (List.get! (Graph.predecessors net.toNet.graph n) i) 
      then 1.0 else 0.0)) n := by
--------------------------------------------------------------------
  intro hâ‚ hâ‚‚
  simp only [activ]
  simp only [activ] at hâ‚‚
  convert â† hâ‚‚ using 7

  rename_i i
  let m := (preds net n).get! i
  have hâ‚ƒ : m âˆˆ (preds net n) := get!_mem (preds net n) i
  exact hâ‚ m hâ‚ƒ


/-â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Properties of Propagation
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•-/

--------------------------------------------------------------------
lemma prop_layer_zero (net : BFNN) : âˆ€ (S : Set â„•) (n : â„•),
  layer net n = 0
  â†’ n âˆˆ propagate net S
  â†’ n âˆˆ S := by
--------------------------------------------------------------------
  intro (S : Set â„•) (n : â„•)
        (hL : layer net n = 0)
        (hâ‚ : n âˆˆ propagate net S)

  simp only [propagate, Membership.mem, Set.Mem] at hâ‚
  rw [hL] at hâ‚
  simp only [propagate_acc] at hâ‚
  exact hâ‚

--------------------------------------------------------------------
theorem propagate_is_extens : 
  âˆ€ (S : Set â„•),
  S âŠ† propagate net S := by
--------------------------------------------------------------------
  intro (S : Set â„•)
        (n : â„•) (hâ‚ : n âˆˆ S)
  simp only [Membership.mem, Set.Mem, propagate]

  -- By induction on the layer of the net containing n
  generalize hL : layer net n = L
  induction L

  -- Base Step
  case zero => 
    simp only [propagate_acc]
    exact hâ‚
  
  -- Inductive Step
  case succ k _ => 
    simp only [propagate_acc]
    exact Or.inl hâ‚

-- Corollary: The same statement, but for propagate_acc
-- (this is a helper lemma for the properties that follow.)
-------------------------------------------------
lemma propagate_acc_is_extens :
  âˆ€ (S : Set â„•),
  n âˆˆ S â†’ propagate_acc net S n (layer net n) := by
-------------------------------------------------
  intro (S : Set â„•)
  intro (hâ‚ : n âˆˆ S)
  have hâ‚‚ : n âˆˆ propagate net S := propagate_is_extens _ _ hâ‚
  simp only [Membership.mem, Set.Mem, propagate] at hâ‚‚
  exact hâ‚‚
  

--------------------------------------------------------------------
theorem propagate_is_idempotent : 
  âˆ€ (S : Set â„•),
  propagate net S = 
    propagate net (propagate net S) := by
--------------------------------------------------------------------
  intro (S : Set â„•)
  apply ext
  intro (n : â„•)
  simp only [Membership.mem, Set.Mem, propagate]

  -- By strong induction on the layer of the net containing n
  generalize hL : layer net n = L
  induction L using Nat.case_strong_induction_on generalizing n

  -- Base Step
  case hz =>
    simp only [Membership.mem, Set.Mem, propagate_acc]
    conv in (layer net n) => rw [hL]
    simp only [propagate_acc]
    exact âŸ¨fun x => x, fun x => xâŸ©
  
  -- Inductive Step
  case hi L IH =>
    apply Iff.intro
    
    -- Forward direction is easy, just apply extensive
    case mp =>
      intro hâ‚
      rw [symm hL]
      rw [symm hL] at hâ‚
      exact @propagate_acc_is_extens net _ _ hâ‚

    -- Backwards direction is trickier
    case mpr => 
      intro hâ‚
      
      -- By cases; either n âˆˆ S or n âˆ‰ S
      -- Again; either n âˆˆ propagate S or n âˆ‰ propagate S 
      by_cases n âˆˆ S
      case pos => 
        rw [symm hL]
        exact @propagate_acc_is_extens net _ _ h
      case neg => 
        by_cases propagate_acc net S n (layer net n)
        case pos => 
          rw [symm hL]
          exact h
        case neg =>
          -- Just some simplifications and rewriting definitions
          rename_i n_not_in_S
          rw [simp_propagate_acc net n_not_in_S]
          
          have hâ‚‚ : Â¬n âˆˆ propagate net S := h
          simp [propagate] at hâ‚‚
          rw [simp_propagate_acc net hâ‚‚] at hâ‚

          -- We abbreviate the two 'simp'ed sets for convenience.
          let Sâ‚ := fun m => propagate_acc net (fun n => 
            propagate_acc net S n (layer net n)) m (layer net m)
          let Sâ‚‚ := fun m => propagate_acc net S m (layer net m)
          
          -- Apply IH to the predecessors
          have hâ‚‚ : âˆ€ (m : â„•), m âˆˆ preds net n â†’ (Sâ‚ m â†” Sâ‚‚ m) := by
            simp only [Membership.mem] -- really just want to unfold the let
            intro m hm
            generalize hLm : layer net m = Lm
            have hâ‚ƒ : Lm â‰¤ L := by
              rw [symm hLm]
              apply Nat.lt_succ.mp
              rw [symm hL]
              exact preds_decreasing net m n hm
            exact (symm (IH Lm hâ‚ƒ m hLm).to_eq).to_iff

          -- Apply the activ_agree lemma
          simp
          simp at hâ‚
          exact activ_agree _ Sâ‚ Sâ‚‚ _ hâ‚‚ hâ‚


-- This is essentially Hannes' proof.
-- TODO: For consistency's sake, replace inner proof with
--   application of 'activ_agree'
--------------------------------------------------------------------
theorem propagate_is_cumulative :
  âˆ€ (A B : Set â„•), A âŠ† B
  â†’ B âŠ† propagate net A
  â†’ propagate net A = propagate net B := by
--------------------------------------------------------------------
  intro (A : Set â„•) (B : Set â„•)
        (hâ‚ : A âŠ† B)
        (hâ‚‚ : B âŠ† propagate net A)
  apply ext
  intro (n : â„•)
  simp only [Membership.mem, Set.Mem, propagate]
  simp only [Membership.mem, Set.Mem, propagate] at hâ‚‚

  -- By induction on the layer of the net containing n
  generalize hL : layer net n = L
  induction L using Nat.case_strong_induction_on generalizing n

  -- Base Step
  case hz =>
    simp only [propagate_acc]
    apply Iff.intro
    case mp => exact fun hâ‚ƒ => hâ‚ hâ‚ƒ
    case mpr =>
      intro hâ‚ƒ
      have hâ‚„ : propagate_acc net A n (layer net n) := hâ‚‚ hâ‚ƒ
      conv at hâ‚„ in (layer net n) => rw [hL]
      simp only [propagate_acc] at hâ‚„
      exact hâ‚„

  -- Inductive Step
  case hi k IH => 
    apply Iff.intro

    -- Forward Direction
    case mp => 
      intro hâ‚ƒ

      -- By cases; either n âˆˆ B or n âˆ‰ B.
      -- Similarly, either n âˆˆ A or n âˆ‰ A. 
      by_cases n âˆˆ B
      case pos =>
        rw [symm hL]
        exact @propagate_acc_is_extens net _ _ h -- TODO: replace acc variation
      case neg =>
        by_cases n âˆˆ A
        case pos => 
          rename_i n_not_in_B 
          exact absurd (hâ‚ h) n_not_in_B
        case neg => 
          -- Just some simplifications and rewriting definitions
          rename_i n_not_in_B
          rw [simp_propagate_acc net h] at hâ‚ƒ
          rw [simp_propagate_acc net n_not_in_B]

          -- Just try to prove it and turn it into an 'activ_agree'
          -- lemma later!
          -- Go into 'prev_activ' and substitute using our IH.
          -- Then try to prove what's left.
          simp
          simp at hâ‚ƒ
          convert hâ‚ƒ using 5
          rename_i i
          generalize hm : List.get! (net.graph.predecessors n) i = m
          generalize hLm : layer net m = Lm

          -- Apply the inductive hypothesis!
          have hâ‚ƒ : m âˆˆ preds net n := by
            rw [symm hm]
            simp [preds]
            exact get!_mem (net.graph.predecessors n) i
          have hâ‚„ : Lm â‰¤ k := by 
            rw [symm hLm]
            apply Nat.lt_succ.mp
            rw [symm hL]
            exact preds_decreasing net m n hâ‚ƒ
          exact (symm (IH Lm hâ‚„ m hLm).to_eq).to_iff

    -- Backwards Direction (should be very similar)
    case mpr => 
      intro hâ‚ƒ

      -- By cases; either n âˆˆ A or n âˆ‰ A
      by_cases n âˆˆ A
      case pos => 
        rw [symm hL]
        exact @propagate_acc_is_extens net _ _ h -- TODO: replace acc variation
      case neg => 
        by_cases n âˆˆ B
        case pos => 
          rename_i n_not_in_A
          rw [symm hL]
          exact hâ‚‚ h
        case neg => 
          -- Just some simplifications and rewriting definitions
          rename_i n_not_in_A
          rw [simp_propagate_acc net h] at hâ‚ƒ
          rw [simp_propagate_acc net n_not_in_A]

          -- Just try to prove it and turn it into an 'activ_agree'
          -- lemma later!
          -- Go into 'prev_activ' and substitute using our IH.
          -- Then try to prove what's left.
          simp
          simp at hâ‚ƒ
          convert hâ‚ƒ using 5
          rename_i i
          generalize hm : List.get! (net.graph.predecessors n) i = m
          generalize hLm : layer net m = Lm

          -- Apply the inductive hypothesis!
          have hâ‚ƒ : m âˆˆ preds net n := by
            rw [symm hm]
            simp [preds]
            exact get!_mem (net.graph.predecessors n) i
          have hâ‚„ : Lm â‰¤ k := by 
            rw [symm hLm]
            apply Nat.lt_succ.mp
            rw [symm hL]
            exact preds_decreasing net m n hâ‚ƒ
          exact IH Lm hâ‚„ m hLm


-- #check propagate myBFNN {n : â„• | n â‰¤ 4}
-- #eval propagate myBFNN {n : â„• | n â‰¤ 4}
-- need to make sets finite in order to evaluate???
-- 
-- It's important for everything to be evaluatable, since:
-- 1) I will want to verify that a *specific*
--    neural network has certain properties
-- 2) #eval helps me debug errors


/-â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Conditional Graph Reachability
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•-/
-- reachable(A, B) is the set of all nodes reachable from B
-- using a path where all nodes are inside A (i.e. there is a 
-- focusedPath from B to n).
-- 
-- This is *precisely* what Van Benthem refers to as "conditional
-- common knowledge" (although here we don't need the word "common"
-- because we don't have group dynamics.)  
-- Quote:
-- CG(A, B) "is true in all worlds reachable via some finite path
-- of accessibilities running entirely through worlds satisfying A"
-- [Van Benthem, Belief Revision and Dynamic Logic, page 6]
-- In this paper, he also talks about "pre-encoding" future
-- information in order to get a reduction, which is exactly
-- what we're doing here!
-- 
-- I don't know what the complete axioms are for this conditional
-- knowledge.  But this isn't the main focus here.  I'll just
-- prove a few sound things to give an idea about what it's like.

-- A focused path is a path where every node is contained
-- within the set S.
inductive focusedPath (g : Graph â„• Float) (S : Set â„•) : â„• â†’ â„• â†’ Prop where
  | trivial {u : â„•} :
      u âˆˆ S â†’ focusedPath g S u u
  | from_path {u v w : â„•} : 
      focusedPath g S u v â†’ g.hasEdge v w â†’ w âˆˆ S â†’ focusedPath g S u w

-- focusedPath is transitive
--------------------------------------------------------------------
theorem focusedPath_trans {u v w : â„•} (g : Graph â„• Float) (A : Set â„•) :
  focusedPath g A u v â†’ focusedPath g A v w â†’ focusedPath g A u w := by
--------------------------------------------------------------------
  intro (hâ‚ : focusedPath g A u v)
  intro (hâ‚‚ : focusedPath g A v w)

  induction hâ‚‚
  case trivial _ => exact hâ‚
  case from_path x y _ edge_xy hy path_ux => 
    exact focusedPath.from_path path_ux edge_xy hy

-- focusedPath is contained in A
--------------------------------------------------------------------
theorem focusedPath_subset {u v : â„•} (g : Graph â„• Float) (A : Set â„•) :
  focusedPath g A u v â†’ u âˆˆ A := by
--------------------------------------------------------------------
  intro hâ‚

  induction hâ‚
  case trivial hA => exact hA
  case from_path _ _ _ _ _ hA => exact hA

-- If there's a path from m to n,
-- then m's layer in the net cannot be bigger than n's layer.
--------------------------------------------------------------------
lemma focusedPath_layer {m n : â„•} (net : BFNN) (A : Set â„•) :
  focusedPath net.graph A m n
  â†’ layer net m â‰¤ layer net n := by
--------------------------------------------------------------------
  sorry

-- This is the set of all nodes reachable from B using
-- paths where *every* node in the path is within A
-- (including the initial and final nodes)
def reachable (net : BFNN) (A B : Set â„•) : Set â„• :=
  fun (n : â„•) =>
    âˆƒ (m : â„•), m âˆˆ B âˆ§ focusedPath net.graph A m n

-- Argument: If there is a path from B to n, but n is in
-- layer 0 -- there are *no* incoming nodes -- then the path
-- must be of length 0.  So n must be that n âˆˆ B with
-- a path to n, i.e. n âˆˆ B.
--------------------------------------------------------------------
lemma reach_layer_zero (net : BFNN) : âˆ€ (A B : Set â„•) (n : â„•),
  layer net n = 0
  â†’ n âˆˆ reachable net A B
  â†’ n âˆˆ B := by
--------------------------------------------------------------------
  intro (A : Set â„•)
        (B : Set â„•)
        (n : â„•) (hL : layer net n = 0)
        (hâ‚ : n âˆˆ reachable net A B)
  
  match hâ‚ with
  | âŸ¨m, hâ‚‚âŸ© => 
    -- By induction on the length of the path from B to n.
    --   path length = 0 => m âˆˆ B means n âˆˆ B
    --   path length â‰¥ 0 => this case should be impossible,
    --                      because at layer 0 n has *no predecessors*! 
    induction hâ‚‚.2
    case trivial _ => exact hâ‚‚.1
    case from_path x y _ edge_xy _ _ =>
      -- Contradiction; y's layer is 0, but there is an edge from x to y!
      -- (layer net x < layer net y, but that means layer net x < 0) 
      have hâ‚ƒ : layer net x < layer net y :=
        preds_decreasing net x y ((edge_from_preds _ _ _).mpr edge_xy)
      
      exact absurd hL (Nat.not_eq_zero_of_lt hâ‚ƒ)

--------------------------------------------------------------------
theorem reach_subset (net : BFNN) : âˆ€ (A B : Set â„•),
  reachable net A B âŠ† A := by
--------------------------------------------------------------------
  intro (A : Set â„•)
        (B : Set â„•)
        (n : â„•) (hâ‚ : n âˆˆ reachable net A B)
  
  simp only [Membership.mem, Set.Mem] at hâ‚
  match hâ‚ with
  | âŸ¨m, hmâŸ© => 
    induction hm.2
    case trivial hbase => exact hbase
    case from_path _ y _ _ hy _ => exact hy 


-- This is like propag_is_extens, except we also have to ensure
-- that n âˆˆ A.
--------------------------------------------------------------------
theorem reach_is_extens (net : BFNN) : âˆ€ (A B : Set â„•),
  (A âˆ© B) âŠ† reachable net A B := by
--------------------------------------------------------------------
  intro (A : Set â„•)
        (B : Set â„•)
        (n : â„•) (hâ‚ : n âˆˆ A âˆ© B)

  have (hâ‚‚ : focusedPath net.graph A n n) := 
    focusedPath.trivial hâ‚.1
  exact âŸ¨n, âŸ¨hâ‚.2, hâ‚‚âŸ©âŸ©


--------------------------------------------------------------------
theorem reach_is_idempotent (net : BFNN) : âˆ€ (A B : Set â„•),
  reachable net A B = reachable net A (reachable net A B) := by
--------------------------------------------------------------------
  intro (A : Set â„•)
        (B : Set â„•)
  
  exact Set.ext (fun (n : â„•) =>
    -- âŠ† direction; easy, just apply reach_subset and reach_is_extens
    âŸ¨(fun (hâ‚ : n âˆˆ reachable net A B) => 
      have hâ‚‚ : n âˆˆ A := reach_subset _ _ _ hâ‚
      reach_is_extens _ _ _ âŸ¨hâ‚‚, hâ‚âŸ©),

    -- âŠ‡ direction
    (fun (hâ‚ : n âˆˆ reachable net A (reachable net A B)) =>
      match hâ‚ with
      | âŸ¨x, hâ‚‚âŸ© => 
        match hâ‚‚.1 with
        | âŸ¨m, hâ‚ƒâŸ© =>
          âŸ¨m, âŸ¨hâ‚ƒ.1, focusedPath_trans _ A hâ‚ƒ.2 hâ‚‚.2âŸ©âŸ©)âŸ©)


--------------------------------------------------------------------
theorem reach_is_monotone (net : BFNN) : âˆ€ (S A B : Set â„•),
  A âŠ† B â†’ reachable net S A âŠ† reachable net S B := by
--------------------------------------------------------------------
  intro (S : Set â„•)
        (A : Set â„•)
        (B : Set â„•)
        (hâ‚ : A âŠ† B)
        (n : â„•) (hâ‚‚ : n âˆˆ reachable net S A)
  
  exact match hâ‚‚ with
    | âŸ¨m, hmâŸ© => âŸ¨m, âŸ¨hâ‚ hm.1, hm.2âŸ©âŸ©


-- Reach is closed under union
-- (This is really a consequence of monotonicity)
--------------------------------------------------------------------
theorem reach_union (net : BFNN) : âˆ€ (S A B : Set â„•),
  reachable net S (A âˆª B) = (reachable net S A) âˆª (reachable net S B) := by
--------------------------------------------------------------------
  intro S A B
  apply ext
  intro n
  apply Iff.intro
  
  -- Backwards direction
  -- (easy; A, B âŠ† A âˆª B, so we just apply monotonicity)
  case mpr => 
    intro hâ‚
    cases hâ‚
    case inl hâ‚‚ => exact reach_is_monotone _ _ _ _ (subset_union_left _ _) hâ‚‚
    case inr hâ‚‚ => exact reach_is_monotone _ _ _ _ (subset_union_right _ _) hâ‚‚

  -- Forward direction
  case mp => 
    intro hâ‚

    -- We have a path from m âˆˆ A âˆª B to n;
    -- from here we go by cases; either m âˆˆ A or m âˆˆ B.
    -- In either case, the path from m âŸ¶ n gives us n âˆˆ Reach(_).
    match hâ‚ with
    | âŸ¨m, hmâŸ© => 
      cases hm.1
      case inl hâ‚‚ => exact Or.inl âŸ¨m, âŸ¨hâ‚‚, hm.2âŸ©âŸ©
      case inr hâ‚‚ => exact Or.inr âŸ¨m, âŸ¨hâ‚‚, hm.2âŸ©âŸ©


-- A simple interaction between graph reachability and propagation
--------------------------------------------------------------------
theorem reach_propagate (net : BFNN) : âˆ€ (A B : Set â„•),
  reachable net A (propagate net B) âŠ† reachable net A B := by
--------------------------------------------------------------------
  intro A B n hâ‚
  
  -- By induction on the layer of the net containing n
  generalize hL : layer net n = L
  induction L using Nat.case_strong_induction_on generalizing n

  --------------------------------
  -- Base Step
  --------------------------------
  case hz => 
    have hâ‚‚ : n âˆˆ propagate net B := reach_layer_zero _ _ _ _ hL hâ‚
    simp only [propagate, Membership.mem, Set.Mem] at hâ‚‚
    rw [hL] at hâ‚‚
    simp only [propagate_acc] at hâ‚‚

    -- Moreover, n âˆˆ A (since n âˆˆ Reach(A, B))
    have hâ‚ƒ : n âˆˆ A := reach_subset _ _ _ hâ‚

    -- We have n âˆˆ B, and so n âˆˆ Reach(A, B). 
    exact âŸ¨n, âŸ¨hâ‚‚, focusedPath.trivial hâ‚ƒâŸ©âŸ© 

  --------------------------------
  -- Inductive Step
  --------------------------------
  case hi L IH =>
    -- simp [propagate] at hâ‚

    match hâ‚ with
    | âŸ¨m, hmâŸ© =>
      -- First, apply our IH to m
      have hâ‚‚ : m âˆˆ A := focusedPath_subset _ _ hm.2
      have hâ‚ƒ : (layer net m) â‰¤ L := sorry
      have hâ‚„ : m âˆˆ reachable net A (propagate net B) := by
        exact âŸ¨m, âŸ¨hm.1, focusedPath.trivial hâ‚‚âŸ©âŸ©
      have hâ‚… : m âˆˆ reachable net A B := IH (layer net m) hâ‚ƒ hâ‚„ rfl

      -- Now we have a path from some xâŸ¶m.
      match hâ‚… with
      | âŸ¨x, hxâŸ© => 
        -- We show n âˆˆ Reach(A, B)
        -- by providing a path x âŸ¶ m âŸ¶ n
        exact âŸ¨x, âŸ¨hx.1, focusedPath_trans _ _ hx.2 hm.2âŸ©âŸ©
      

/-â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Reach-Prop Interaction Properties
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•-/

-- --------------------------------------------------------------------
-- theorem propagate_reach_inclusion (net : BFNN) : âˆ€ (S : Set â„•),
--   propagate net S âŠ† reachable net S := by
-- --------------------------------------------------------------------
--   sorry

-- --------------------------------------------------------------------
-- lemma minimal_cause_helper (net : BFNN) : âˆ€ (A B : Set â„•), âˆ€ (n : â„•),
--   n âˆˆ reachedby net B
--   â†’ (n âˆˆ propagate net A
--   â†” n âˆˆ propagate net (A âˆ© reachedby net B)) := by
-- --------------------------------------------------------------------
--   intro (A : Set â„•) (B : Set â„•)
--   intro (n : â„•)
--   intro (hâ‚ : n âˆˆ reachedby net B)
--   simp only [Membership.mem, Set.Mem, propagate]

--   -- By induction on the layer of the net containing n
--   generalize hL : layer net n = L
--   induction L using Nat.case_strong_induction_on generalizing n
  
--   -- Base Step
--   case hz => 
--     apply Iff.intro
--     case mp => 
--       intro hâ‚‚
--       simp only [propagate_acc]
--       simp only [propagate_acc] at hâ‚‚
--       exact âŸ¨hâ‚‚, hâ‚âŸ©

--     case mpr => 
--       intro hâ‚‚
--       simp only [propagate_acc]
--       simp only [propagate_acc] at hâ‚‚
--       exact hâ‚‚.1

--   -- Inductive Step
--   case hi k IH => 
--     apply Iff.intro

--     -- Forward Direction
--     case mp => 
--       intro hâ‚‚

--       -- By cases; either n âˆˆ A or not.
--       by_cases n âˆˆ A
--       case pos => 
--         -- This case is trivial (just apply Extens)
--         rw [symm hL]
--         have hâ‚ƒ : n âˆˆ A âˆ© reachedby net B := âŸ¨h, hâ‚âŸ© 
--         exact @propagate_acc_is_extens net _ _ hâ‚ƒ
--       case neg => 
--         -- If n âˆ‰ A, then n âˆ‰ A âˆ© reachedby net B
--         have hâ‚ƒ : n âˆ‰ A âˆ© reachedby net B := 
--           fun n_in_A => absurd n_in_A.1 h
        
--         -- Just some simplifications and rewriting definitions
--         rw [simp_propagate_acc net h] at hâ‚‚
--         rw [simp_propagate_acc net hâ‚ƒ]

--         -- TODO: This is the stuff that should go in the activ_agree lemma!
--         simp
--         simp at hâ‚‚
--         convert hâ‚‚ using 5
--         rename_i i
--         generalize hm : List.get! (predecessors net.graph n).data i = m
--         generalize hLm : layer net m = Lm

--         -- Apply the inductive hypothesis!
--         have hâ‚„ : m âˆˆ preds net n := by
--           rw [symm hm]
--           simp [preds]
--           exact get!_mem (predecessors net.graph n).data i
--         have hâ‚… : Lm â‰¤ k := by
--           rw [symm hLm]
--           apply Nat.lt_succ.mp
--           rw [symm hL]
--           exact preds_decreasing net m n hâ‚„
--         have hâ‚† : m âˆˆ reachedby net B :=
--           match hâ‚ with
--           | âŸ¨x, hxâŸ© => âŸ¨x, âŸ¨hx.1, hasPath_trans _ (preds_path _ hâ‚„) hx.2âŸ©âŸ©
--         exact (symm (IH Lm hâ‚… m hâ‚† hLm).to_eq).to_iff

--     -- Backwards Direction (should be similar)
--     case mpr =>
--       intro hâ‚‚

--       -- By cases; either n âˆˆ A or not.
--       by_cases n âˆˆ A
--       case pos => 
--         -- This case is trivial (just apply Extens)
--         rw [symm hL]
--         exact @propagate_acc_is_extens net _ _ h
--       case neg => 
--         -- If n âˆ‰ A, then n âˆ‰ A âˆ© reachedby net B
--         have hâ‚ƒ : n âˆ‰ A âˆ© reachedby net B := 
--           fun n_in_A => absurd n_in_A.1 h
        
--         -- Just some simplifications and rewriting definitions
--         rw [simp_propagate_acc net hâ‚ƒ] at hâ‚‚
--         rw [simp_propagate_acc net h]

--         -- TODO: This is the stuff that should go in the activ_agree lemma!
--         simp
--         simp at hâ‚‚
--         convert hâ‚‚ using 5
--         rename_i i
--         generalize hm : List.get! (predecessors net.graph n).data i = m
--         generalize hLm : layer net m = Lm

--         -- Apply the inductive hypothesis!
--         have hâ‚„ : m âˆˆ preds net n := by
--           rw [symm hm]
--           simp [preds]
--           exact get!_mem (predecessors net.graph n).data i
--         have hâ‚… : Lm â‰¤ k := by
--           rw [symm hLm]
--           apply Nat.lt_succ.mp
--           rw [symm hL]
--           exact preds_decreasing net m n hâ‚„
--         have hâ‚† : m âˆˆ reachedby net B :=
--           match hâ‚ with
--           | âŸ¨x, hxâŸ© => âŸ¨x, âŸ¨hx.1, hasPath_trans _ (preds_path _ hâ‚„) hx.2âŸ©âŸ©
--         exact IH Lm hâ‚… m hâ‚† hLm


-- -- This is the actual property I want, re-written with conditionals
-- -- in mind
-- --------------------------------------------------------------------
-- theorem minimal_cause (net : BFNN) : âˆ€ (A B : Set â„•),
--   B âŠ† propagate net A
--   â†” B âŠ† propagate net (A âˆ© reachedby net B) := by
-- --------------------------------------------------------------------
--   intro (A : Set â„•) (B : Set â„•)
--   apply Iff.intro
--   case mp => exact fun hâ‚ n hâ‚‚ => (minimal_cause_helper net _ _ _ 
--     (reachedby_is_extens _ _ hâ‚‚)).mp (hâ‚ hâ‚‚)
--   case mpr => exact fun hâ‚ n hâ‚‚ => (minimal_cause_helper net _ _ _ 
--     (reachedby_is_extens _ _ hâ‚‚)).mpr (hâ‚ hâ‚‚)

/-â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Update Policy: "Make neurons fire together"
  (without regard for the edges of the net)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•-/

/-
*Notes*
The basic idea is that we take a subset A of the graph, and
increase our preference for nodes in A as much as possible.
We do this by: 1) completing the graph A, and 2) maximizing all
of the edges within this completed graph.  The overall effect
is that for all neurons m, n âˆˆ A, m fires exactly when n fires.

But this operation results in a graph with cycles -- so [A] Prop(B)
is not well-defined!  But we can do something equivalent:
Take all the nodes in A, and quotient them.  This results in a
net where all the nodes m, n âˆˆ A "fire as one".

So we need:
  [Def] Define 'complete_and_max' update
  [Def] Define 'fire_together' update (the quotient structure)
  [Thm] Prove that the two updates are equivalent.
        (We can't use Prop for this -- maybe I can prove
         that the 'activ' for both nets agrees?)
  [Thm] Find a reduction for 'fire_together' and prove it.
        (It should look like the dual of Lexicographic Upgrade)
  [Cor] 'fire_together' is the dual of Lexicographic Upgrade
    [Depends] 
      Preference Models
      Lexicographic Upgrade
      Lexicographic Upgrade reduction
      Model translation from pref models âŸ· nets
-/

-- def complete_and_max (net : BFNN) (A : Set â„•) : BFNN :=
--   sorry

-- def fire_together (net : BFNN) (A : Set â„•) : BFNN :=
-- { net with
--   graph := sorry
--   activation := sorry
--   rate := sorry

--   -- binary := sorry
--   binary := sorry
--   acyclic := sorry
--   activ_nondecr := sorry
--   activ_pos := sorry
-- }



/-â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Naive (Unstable) Hebbian Update
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•-/

-- A function to map edges in the graph.  
-- We update the edge weight xâ‚ âŸ¶ xâ‚‚, but we also allow information 
-- about the *nodes* xâ‚ and xâ‚‚.
-- Credit to Peter Kementzey for the original mapEdges function.
def map_edges (g : Graph â„• Float) (f : â„• â†’ â„• â†’ Float â†’ Float) : Graph â„• Float := âŸ¨
  g.vertices.map (fun vertex => 
    { vertex with successors := vertex.successors.map (fun 
      âŸ¨target, weightâŸ© => âŸ¨target, f vertex.label target weightâŸ©
  )})
âŸ©

--------------------------------------------------------------------
lemma map_edges_apply (g : Graph â„• Float) (f : â„• â†’ â„• â†’ Float â†’ Float) (m n : â„•) :
  (map_edges g f).getEdgeWeight m n = (f m n (g.getEdgeWeight m n)) := by
--------------------------------------------------------------------
  -- I have no idea... this one's tough, and it's hard to see
  -- what's going on.
  simp only [Graph.getEdgeWeight]

  match List.find? (fun x => decide (x.fst = n)) g.vertices[m]!.successors with
  | none => sorry
  | some âŸ¨_, weightâŸ© => sorry
  
  /-
  split
  case _ opâ‚ targetâ‚ weightâ‚ hopâ‚ => 
    split
    case _ opâ‚‚ targetâ‚‚ weightâ‚‚ hopâ‚‚ => sorry
    case _ opâ‚‚ hopâ‚‚ => sorry
  case _ opâ‚ hopâ‚ => 
    split
    case _ opâ‚‚ targetâ‚‚ weightâ‚‚ hopâ‚‚ => sorry
    case _ opâ‚‚ hopâ‚‚ => sorry
  -/

-- For every m âŸ¶ n where m, n âˆˆ Prop(S), increase the weight
-- of that edge by Î· * act(m) * act(n).
noncomputable
def graph_update (net : BFNN) (g : Graph â„• Float) (S : Set â„•) : Graph â„• Float :=
  map_edges g (fun m n weight => 
    let activ_m := if m âˆˆ propagate net S then 1.0 else 0.0
    let activ_n := if n âˆˆ propagate net S then 1.0 else 0.0
    weight + (net.rate * activ_m * activ_n))



-- This graph update does not affect the vertices of the graph.
--------------------------------------------------------------------
lemma graph_update_vertices (net : BFNN) (g : Graph â„• Float) (S : Set â„•) :
  (graph_update net g S).get_vertices = g.get_vertices := by
--------------------------------------------------------------------
  simp only [graph_update, map_edges]
  simp only [Graph.get_vertices]

  -- Go in and compose the maps.  This seems to do the trick.
  conv => lhs; rw [List.map_map]


-- This graph update does not affect the *successor/edge* structure
-- of the graph (it only affects weights!!!)
--------------------------------------------------------------------
lemma graph_update_successors (net : BFNN) (g : Graph â„• Float) (S : Set â„•) (n : â„•) :
  (graph_update net g S).successors n = g.successors n := by
--------------------------------------------------------------------
  -- Simplify definitions
  simp only [Graph.successors]
  rw [graph_update_vertices]
  simp only [graph_update, map_edges]
  
  -- Rewrite 'unzip's as 'map's
  rw [List.unzip_eq_map]
  rw [List.unzip_eq_map]
  simp only [Prod.fst]

  -- Get to the heart of the expression
  congr
  funext m
  apply Bool.decide_congr

  -- Our goal is to now show that when we *only* consider the target
  -- vertices, the graph_update is essentially the identity function.
  -- By induction on the graph's list of vertices:
  induction g.vertices
  case nil => 
    simp only [Graph.successor_pairs]

  case cons v rest IH => 
    simp only [Graph.successor_pairs]

    -- By cases; either v.label = n or not.  If so, we go in
    -- and compose the maps.  Otherwise we just apply our IH.
    by_cases (v.label = n)
    case pos => 
      rw [if_pos h]
      rw [if_pos h]
      rw [List.map_map]
      simp

    case neg =>
      rw [if_neg h]
      rw [if_neg h]
      exact IH

-- This graph update preserves whether the graph is acyclic.
--------------------------------------------------------------------
lemma graph_update_acyclic (net : BFNN) (g : Graph â„• Float) (S : Set â„•) :
  (graph_update net g S).is_acyclic â†” g.is_acyclic := by
--------------------------------------------------------------------
  simp only [Graph.is_acyclic]
  apply Iff.intro
  
  case mp => sorry
  case mpr =>
    intro g_acyclic
    intro u v
    intro path_uv
    intro path_vu

    -- We have a path in the updated graph from u to v
    -- and another path from v to u.
    -- We now need to show that u = v.
    -- By induction on the length of this first path.
    induction path_uv
    case trivial => exact rfl
    case from_path x y path_ux edge_xy IH => 
      sorry


-- A single step of Hebbian update.
-- Propagate S through the net, and then increase the weights
-- of all the edges xâ‚ âŸ¶ xâ‚‚ involved in that propagation
-- by Î· * xâ‚ * xâ‚‚.
noncomputable
def hebb (net : BFNN) (S : Set â„•) : BFNN :=
{ net with
  graph := graph_update net net.graph S
}


/-â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Properties of Unstable Hebbian Update
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•-/

-- A net netâ‚ is a subnet of netâ‚‚ (netâ‚ â‰¼ netâ‚‚) iff for all
-- sets S, every node activated in the propagation of S
-- in netâ‚ is activated in the propagation of S in netâ‚‚.
-- (They both have the same *propagation structure*)
def subnet (netâ‚ netâ‚‚ : BFNN) : Prop :=
  âˆ€ (S : Set â„•), propagate netâ‚ S âŠ† propagate netâ‚‚ S

infixl:65   " â‰¼ " => subnet


-- Two nets are equivalent if they have the same 
-- *propagation structure* (i.e. if their propagations agree 
-- for every set S)
def net_eq (netâ‚ netâ‚‚ : BFNN) : Prop :=
  netâ‚ â‰¼ netâ‚‚ âˆ§ netâ‚‚ â‰¼ netâ‚

infixl:65   " â‰¡ " => net_eq


-- A super easy example, just to briefly test â‰¼ and â‰¡
example : example_net â‰¡ example_net :=
  âŸ¨fun _ _ h => h, fun _ _ h => hâŸ©  


/-â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Properties of Single-Iteration Hebbian Update
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•-/

-- First, we check that a single round of hebb preserves whether
-- the graph is acyclic. (This is a rehash of graph_update_acyclic,
-- but it helps to write it out so we can lift it later to hebb_star.)
--------------------------------------------------------------------
lemma hebb_once_acyclic (net : BFNN) (S : Set â„•) : 
  (hebb net S).graph.is_acyclic = net.graph.is_acyclic := by
--------------------------------------------------------------------
  simp only [hebb]
  rw [graph_update_acyclic]

-- A single round of Hebbian update does not affect the vertices
-- of the graph.
--------------------------------------------------------------------
theorem hebb_once_vertices (net : BFNN) (S : Set â„•) : 
  (hebb net S).graph.get_vertices = net.graph.get_vertices := by
--------------------------------------------------------------------
  simp only [hebb]
  rw [graph_update_vertices]

-- A single round of Hebbian update does not affect the predecessors
-- of any node.  To prove this, we just show that Hebbian update
-- does not affect the vertices of the graph, or the successor
-- structure of the graph.
--------------------------------------------------------------------
theorem hebb_once_preds (net : BFNN) (S : Set â„•) (n : â„•) : 
  preds (hebb net S) n = preds net n := by
--------------------------------------------------------------------
  simp only [preds, hebb]
  simp only [Graph.predecessors]
  congr 1

  -- graph_update doesn't affect successors of a node
  case _ => 
    funext v
    congr 2
    exact graph_update_successors _ (net.graph) _ v

  -- graph_update doesn't affect vertices of the graph
  case _ => exact graph_update_vertices _ (net.graph) _

  
-- A single round of Hebbian update hebb does not affect which 
-- neurons are on which layer of the net.
--------------------------------------------------------------------
theorem hebb_once_layers (net : BFNN) (S : Set â„•) : 
  layer (hebb net S) n = layer net n := by
--------------------------------------------------------------------
  simp only [layer]
  rw [hebb_once_vertices net S] -- vertices are the same

  -- By induction on the reverse of the graph's vertex list
  -- (We are looking at vertices backwards -- each
  --  vertex can only "see" the vertices preceding it.)
  induction List.reverse (Graph.get_vertices net.toNet.graph) generalizing n
  case nil => 
    simp only [layer_acc]
  case cons v rest IH => 
    simp only [layer_acc]

    -- By cases; either vertex.label = n or not.
    -- If so, this follows from our IH and the fact that the predecessors
    -- are the same in both nets.
    by_cases v = n
    case pos => 
      rw [if_pos h]
      rw [if_pos h]
      rw [hebb_once_preds]
      congr
      funext m
      exact @IH m

    -- If not, just apply our IH
    case neg => 
      rw [if_neg h]
      rw [if_neg h]
      exact IH

-- A single round of Hebbian update hebb does not affect the 
-- activation function.
--------------------------------------------------------------------
theorem hebb_once_activation (net : BFNN) (S : Set â„•) : 
  (hebb net S).activation = net.activation := by 
--------------------------------------------------------------------
  exact rfl

-- A single round of Hebbian update hebb does not affect graph 
-- reachability (It only affects the edge weights)
--------------------------------------------------------------------
theorem hebb_once_reach (net : BFNN) (A B : Set â„•) : 
  reachable (hebb net A) A B = reachable net A B := by 
--------------------------------------------------------------------
  apply ext
  intro (n : â„•)
  -- simp only [reachable]
  
  apply Iff.intro
  case mp => 
    intro hâ‚

    -- There is some m with focused path from m to n in the *updated* net
    match hâ‚ with
    | âŸ¨m, hmâŸ© =>
      induction hm.2
      case trivial hma => exact reach_is_extens _ _ _ âŸ¨hma, hm.1âŸ©
      case from_path x y path_mx edge_xy hy IH =>
        -- First, apply our IH to get x âˆˆ Reach(A, B)
        have hâ‚‚ : x âˆˆ reachable (hebb net A) A B := âŸ¨m, âŸ¨hm.1, path_mxâŸ©âŸ©
        have hâ‚ƒ : x âˆˆ reachable net A B := IH hâ‚‚ âŸ¨hm.1, path_mxâŸ©

        -- So there is some u âˆˆ B with focused path from u to x
        -- (in the *original* net)
        -- We extend this path with the edge from x to y.
        match hâ‚ƒ with
        | âŸ¨u, huâŸ© =>

          -- We have an edge from x to y in the *updated* net,
          -- but we have to bring it down to the *original* net.
          have hâ‚„ : Graph.hasEdge net.graph x y = true := by
            rw [â† edge_from_preds]
            rw [â† edge_from_preds] at edge_xy
            convert edge_xy using 1
            exact symm (hebb_once_preds _ _ _)

          exact âŸ¨u, âŸ¨hu.1, (focusedPath.from_path hu.2 hâ‚„ hy)âŸ©âŸ©

  -- This direction is very similar.
  case mpr => 
    intro hâ‚

    -- There is some m with focused path from m to n in the *original* net
    match hâ‚ with
    | âŸ¨m, hmâŸ© =>
      induction hm.2
      case trivial hma => exact reach_is_extens _ _ _ âŸ¨hma, hm.1âŸ©
      case from_path x y path_mx edge_xy hy IH =>
        -- First, apply our IH to get x âˆˆ Reach(A, B)
        have hâ‚‚ : x âˆˆ reachable net A B := âŸ¨m, âŸ¨hm.1, path_mxâŸ©âŸ©
        have hâ‚ƒ : x âˆˆ reachable (hebb net A) A B := IH hâ‚‚ âŸ¨hm.1, path_mxâŸ©
        
        -- So there is some u âˆˆ B with focused path from u to x
        -- (in the *updated* net)
        -- We extend this path with the edge from x to y.
        match hâ‚ƒ with
        | âŸ¨u, huâŸ© =>

          -- We have an edge from x to y in the *original* net,
          -- but we have to bring it down to the *updated* net.
          have hâ‚„ : Graph.hasEdge (hebb net A).graph x y = true := by
            rw [â† edge_from_preds]
            rw [â† edge_from_preds] at edge_xy
            convert edge_xy using 1
            exact hebb_once_preds _ _ _
            
          exact âŸ¨u, âŸ¨hu.1, (focusedPath.from_path hu.2 hâ‚„ hy)âŸ©âŸ©


-- If n âˆ‰ Prop(A), then the weights in the *once* updated net are 
-- the same as the weights in the original net.
--------------------------------------------------------------------
theorem hebb_once_weightsâ‚ (net : BFNN) : 
  n âˆ‰ propagate net A
  â†’ âˆ€ (i : â„•),
    ((hebb net A).toNet.graph.getEdgeWeight ((preds net n).get! i) n =
    net.graph.getEdgeWeight ((preds net n).get! i) n) := by
--------------------------------------------------------------------
  intro hâ‚
  intro i
  simp only [hebb, graph_update]
  rw [map_edges_apply _ _ _ _]

  -- n âˆ‰ propagate net A, and so the term that we're updating by
  -- reduces to weight + 0 = weight.
  rw [if_neg hâ‚]
  rw [zero_mult]
  rw [zero_plus]

-- If m âˆ‰ Prop(A), then the weight of m âŸ¶ n in the *once* updated
-- net is the same as in the original net.
--------------------------------------------------------------------
theorem hebb_once_weightsâ‚‚ (net : BFNN) :
  m âˆ‰ propagate net A
  â†’ (hebb net A).toNet.graph.getEdgeWeight m n =
    net.graph.getEdgeWeight m n := by
--------------------------------------------------------------------
  intro hâ‚
  simp only [hebb, graph_update]
  rw [map_edges_apply _ _ _ _]
  
  -- Since m âˆ‰ Prop(A), this term that we're updating by reduces
  -- to weight + 0 = weight
  rw [if_neg hâ‚]
  rw [zero_mult]
  rw [commutative_mult_float]
  rw [zero_mult]
  rw [zero_plus]


/-â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  "Iterated"/Fixed-Point Naive Hebbian Update
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•-/

-- Takes a graph update function 'f' (e.g. graph_update for Hebb)
-- and iterates it 'no_times' times.
-- netáµ¢ and Sáµ¢ are the initial inputs.
def iterate (f : Graph â„• Float â†’ Set â„• â†’ Graph â„• Float) 
  (no_times : â„•) (gáµ¢ : Graph â„• Float) (Sáµ¢ : Set â„•) : Graph â„• Float :=
  match no_times with
  | Nat.zero => gáµ¢
  | Nat.succ k => f (iterate f k gáµ¢ Sáµ¢) Sáµ¢

-- We score neurons by the total sum of *negative* weights coming 
-- into them.  The neuron with the lowest score is the least likely
-- to activate (in the worst case where all of its negative signals
-- activate but none of its positive ones do).  If a neuron has
-- no negative incoming weights, we give it a score of 0.
def neg_weight_score (net : BFNN) (n : â„•) : Float :=
  sorry


-- This is the exact number of iterations of Hebbian learning
-- on 'net' and 'S' that we need to make the network unstable,
-- i.e. any activation involved within Prop(S) simply goes through.
-- 
-- This is the trickiest part to get right -- we actually need
-- to *construct* this number, based on the net's activation
-- function and the edge weights within Prop(S)!
-- 
-- We first pick the neuron with the lowest 'neg_weight_score' n_min.
-- The number of iterations is that number which would (in the worst
-- case) guarantee that n_min gets activated.
-- 
-- If n_score is n_min's score, and X is that point at which
-- our activation function is guranteed to be 1.0, and Î· is the
-- learning rate, then we return
-- 
-- (X - n_score) / Î·   *(I think!)*
def hebb_unstable_point (net : BFNN) (S : Set â„•) : â„• :=
  sorry
  -- let x := choose net.activ_pos
  -- have hâ‚ : net.activation x = 1.0 := sorry

  -- let n_min := @List.minimum (Vertex â„• Float) sorry sorry net.graph.vertices.toList
  -- let n_score := sorry
  -- sorry

-- Iterated hebbian update, up to a certain fixed point.
-- We implement this as a new net, whose graph is
-- 'graph_update' iterated 'hebb_unstable_point'
-- number of times.
-- FUTURE: Consider re-doing this using limits of graphs/categories
noncomputable
def hebb_star (net : BFNN) (S : Set â„•) : BFNN := 
{ net with
  graph := iterate (graph_update net) (hebb_unstable_point net S) net.graph S
}

/-â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Facts we will need about the unstable point
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•-/



/-â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Properties of "Iterated" Naive Hebbian Update
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•-/

-- This lemma allows us to "lift" equational properties of hebb 
-- to hebb_star.  (This holds *because* hebb_star is the fixed point
-- of hebb.)
--------------------------------------------------------------------
theorem hebb_lift (net : BFNN) (S : Set â„•) (P : BFNN â†’ Î±) : 
  (P (hebb net S) = P net)
  â†’ (P (hebb_star net S) = P net) := by 
--------------------------------------------------------------------
  intro hâ‚
  simp only [hebb_star]
  
  -- By induction on the unstable point of the net
  -- (we don't actually need to know what the unstable point *is*
  --  for this lemma to hold.)
  induction (hebb_unstable_point net S)
  case zero => simp only [iterate]
  case succ k IH =>
    simp only [iterate]
    simp only [hebb] at hâ‚

    -- Why won't hâ‚ work here?
    exact Eq.trans sorry IH 
    


-- Hebbian update hebb_star preserves the acyclicness of the graph.
-- (So the updated net is still acyclic.)
-- [LIFTED from hebb_once_acyclic]
--------------------------------------------------------------------
lemma hebb_acyclic (net : BFNN) (S : Set â„•) : 
  (hebb_star net S).graph.is_acyclic = net.graph.is_acyclic := by
--------------------------------------------------------------------
  exact hebb_lift _ _ (fun x => x.graph.is_acyclic) (hebb_once_acyclic _ _)

-- Hebbian update hebb_star does not affect the vertices of the graph.
--------------------------------------------------------------------
theorem hebb_vertices (net : BFNN) (S : Set â„•) : 
  (hebb_star net S).graph.get_vertices = net.graph.get_vertices := by
--------------------------------------------------------------------
  exact hebb_lift _ _ (fun x => x.graph.get_vertices) (hebb_once_vertices _ _)

-- Hebbian update hebb_star does not affect the predecessors
-- of any node.
-- [LIFTED from hebb_once_preds]
--------------------------------------------------------------------
theorem hebb_preds (net : BFNN) (S : Set â„•) : 
  preds (hebb_star net S) n = preds net n := by
--------------------------------------------------------------------
  exact hebb_lift _ _ (fun x => preds x n) (hebb_once_preds _ _ _)
  
-- Hebbian update hebb_star does not affect which neurons are
-- on which layer of the net.
-- [LIFTED from hebb_once_layers]
--------------------------------------------------------------------
theorem hebb_layers (net : BFNN) (S : Set â„•) : 
  layer (hebb_star net S) n = layer net n := by
--------------------------------------------------------------------
  exact hebb_lift _ _ (fun x => layer x n) (hebb_once_layers _ _)

-- Hebbian update hebb_star does not affect the activation function.
-- [LIFTED from hebb_once_activation]
--------------------------------------------------------------------
theorem hebb_activation (net : BFNN) (S : Set â„•) : 
  (hebb_star net S).activation = net.activation := by 
--------------------------------------------------------------------
  exact hebb_lift _ _ (fun x => x.activation) (hebb_once_activation _ _)

-- Hebbian update hebb_star does not affect graph reachability
-- (It only affects the edge weights)
-- -- [LIFTED from hebb_once_reach]
--------------------------------------------------------------------
theorem hebb_reach (net : BFNN) (A B : Set â„•) : 
  reachable (hebb_star net A) A B = 
    reachable net A B := by 
--------------------------------------------------------------------
  exact hebb_lift _ _ (fun x => reachable x A B) (hebb_once_reach _ _ _)


-- A lemma for 'activ' and 'hebb':
-- Essentially a simplified form of
-- activ net prev_activ n  âŸ¶  activ (hebb_star net A) prev_activ n
-- 
-- (the weights of the new net have *not decreased*, so if n
--  is activated by its predecessors in the original net, 
--  it's activated by its predecessors in the new net.)
/-
This depends on a lemma that says:
Graph.getEdgeWeight net.toNet.graph (List.get! (preds net n) i) n
      â‰¤ 
Graph.getEdgeWeight (hebb_star net A).toNet.graph (List.get! (preds net n) i) n

and the fact that the activation function is monotone nondecreasing
-/
--------------------------------------------------------------------
lemma hebb_activ (net : BFNN) (A B : Set â„•) (n : â„•) :
  activ net (List.bind (List.range (List.length (Graph.predecessors net.toNet.graph n))) fun i =>
    pure
      (if B (List.get! (Graph.predecessors net.toNet.graph n) i) 
      then 1.0 else 0.0)) n
  â†’ activ (hebb_star net A) (List.bind (List.range (List.length (Graph.predecessors net.toNet.graph n))) fun i =>
    pure
      (if B (List.get! (Graph.predecessors net.toNet.graph n) i) 
      then 1.0 else 0.0)) n := by
--------------------------------------------------------------------
  simp only [activ]
  rw [hebb_preds]
  
  apply activation_from_inequality _ _ _
  apply net.activ_nondecr _ _
  apply weighted_sum_le _ _ _ _
  
  intro i
  sorry

-- Every net N is a subnet of (hebb_star N)
-- (i.e. hebb_star includes the previous propagations)
-- (We had this property in The Logic of Hebbian Learning)
-- TODO: Can we lift this from single-iteration hebb???
--------------------------------------------------------------------
theorem hebb_extensive (net : BFNN) (A : Set â„•) : 
  net â‰¼ (hebb_star net A) := by 
--------------------------------------------------------------------
  intro (B : Set â„•)
  intro (n : â„•)
  intro (hâ‚ : n âˆˆ propagate net B)
  simp only [Membership.mem, Set.Mem, propagate]
  simp only [Membership.mem, Set.Mem, propagate] at hâ‚
  
  -- By induction on the layer of the 
  generalize hL : layer net n = L
  induction L

  --------------------------------
  -- Base Step
  --------------------------------
  case zero => 
    rw [hebb_layers]
    rw [hL] at hâ‚
    rw [hL]
    simp only [propagate_acc]
    simp only [propagate_acc] at hâ‚
    exact hâ‚

  --------------------------------
  -- Inductive Step
  --------------------------------
  case succ k IH => 
    -- By cases, to later simplify propagate_acc
    by_cases n âˆˆ B
    case pos => 
      rw [hebb_layers]
      rw [â† hebb_layers net A]
      exact propagate_acc_is_extens _ _ h  
    
    case neg => 
      -- Do simplifications and rewriting
      rw [hebb_layers]
      rw [hL]
      rw [hL] at hâ‚
      rw [simp_propagate_acc _ h]
      rw [simp_propagate_acc _ h] at hâ‚
      rw [hebb_preds]

      -- simp only [activ]
      -- rw [hebb_activation]
      
      simp
      simp at hâ‚
      -- We can *almost* apply 'hebb_activ'... what's missing???
      sorry -- need to argue here that 'activ' is *nondecreasing*
            -- i.e. never decreases a weight.


--------------------------------------------------------------------
lemma hebb_acc_is_extens (net : BFNN) (A B : Set â„•) (n : â„•) :
  propagate_acc net B n (layer net n) â†’ 
  propagate_acc (hebb_star net A) B n (layer net n) := by
-------------------------------------------------------------------- 
  intro hâ‚
  have hâ‚‚ : n âˆˆ propagate (hebb_star net A) B := hebb_extensive _ _ _ hâ‚
  simp only [Membership.mem, Set.Mem, propagate] at hâ‚‚
  rw [hebb_layers] at hâ‚‚
  exact hâ‚‚


/-â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  The more interesting/crucial properties
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•-/

-- If n âˆ‰ Prop(A), then the weights in the updated net are the same
-- as the weights in the original net.
-- IDEA: Lift this from single-iteration hebb!
-- Then just go into the definition of hebb.
--------------------------------------------------------------------
theorem hebb_weightsâ‚ (net : BFNN) : 
  n âˆ‰ propagate net A
  â†’ âˆ€ (i : â„•),
    ((hebb_star net A).toNet.graph.getEdgeWeight ((preds net n).get! i) n =
    net.graph.getEdgeWeight ((preds net n).get! i) n) := by
--------------------------------------------------------------------
  intro hâ‚
  intro i
  exact hebb_lift _ _ (fun x => x.toNet.graph.getEdgeWeight ((preds net n).get! i) n) 
    (hebb_once_weightsâ‚ _ hâ‚ _)

-- If m âˆ‰ Prop(A), then the weight of m âŸ¶ n in the updated net
-- is the same as the weight in the original net.
-- Lifted from hebb_once_weightsâ‚‚
--------------------------------------------------------------------
theorem hebb_weightsâ‚‚ (net : BFNN) :
  m âˆ‰ propagate net A
  â†’ (hebb_star net A).toNet.graph.getEdgeWeight m n =
    net.graph.getEdgeWeight m n := by
--------------------------------------------------------------------
  intro hâ‚
  exact hebb_lift _ _ (fun x => x.toNet.graph.getEdgeWeight m n) 
    (hebb_once_weightsâ‚‚ _ hâ‚)


-- If n âˆ‰ Prop(A), then activ (hebb_star net A) _ n = activ net _ n.
--------------------------------------------------------------------
theorem simp_hebb_activâ‚ (net : BFNN) (A : Set â„•) (prev_activ : List Float) :
  n âˆ‰ propagate net A
  â†’ activ (hebb_star net A) prev_activ n = activ net prev_activ n := by
--------------------------------------------------------------------
  intro hâ‚
  simp only [activ]
  rw [hebb_activation net A]
  rw [hebb_preds net A]
  conv =>
    lhs; enter [1, 2, 1, 2, i, 1]
    rw [hebb_weightsâ‚ _ hâ‚]


-- If every *active* predecessor of n âˆ‰ Prop(A), then
-- activ (hebb_star net A) _ n = activ net _ n.
-- Like activ_agree, we have to simplify the statement of this
-- lemma in order for Lean to be able to infer types efficiently.
--
-- Because this is ugly as hell, here's the statement of 
-- the lemma *before* simplification:
/-
theorem simp_hebb_activâ‚‚ (net : BFNN) (A B : Set â„•) :
  let prev_activ := do
    let i <- List.range (preds net n).length
    let m := (preds net n).get! i
    return if propagate_acc net (B âˆª reachable net (fun n => 
      propagate_acc net A n (layer net n)) fun n => 
        propagate_acc net B n (layer net n)) m (layer net m) 
      then 1.0 else 0.0

  (âˆ€ x, x âˆˆ (preds net n) â†’ x âˆ‰ (propagate net A) âˆ© (propagate net 
    (B âˆª reachable net (propagate net A) (propagate net B))))
  â†’ activ (hebb_star net A) prev_activ n = activ net prev_activ n
-/
--------------------------------------------------------------------
theorem simp_hebb_activâ‚‚ (net : BFNN) (A B : Set â„•) :
  (âˆ€ x, x âˆˆ (preds net n) â†’ x âˆ‰ (propagate net A) âˆ© (propagate net 
    (B âˆª reachable net (propagate net A) (propagate net B))))

  â†’ ((activ (hebb_star net A) (List.bind (List.range 
    (List.length (Graph.predecessors net.toNet.graph n))) fun i =>
    pure (if propagate_acc net (B âˆª reachable net (fun n => 
              propagate_acc net A n (layer net n)) fun n => 
              propagate_acc net B n (layer net n))
              (List.get! (Graph.predecessors net.toNet.graph n) i)
              (layer net (List.get! (Graph.predecessors net.toNet.graph n) i)) 
          then 1.0 else 0.0)) n)
  â†” (activ net (List.bind (List.range 
    (List.length (Graph.predecessors net.toNet.graph n))) fun i =>
    pure (if propagate_acc net (B âˆª reachable net (fun n => 
              propagate_acc net A n (layer net n)) fun n => 
              propagate_acc net B n (layer net n))
              (List.get! (Graph.predecessors net.toNet.graph n) i)
              (layer net (List.get! (Graph.predecessors net.toNet.graph n) i))
          then 1.0 else 0.0)) n)) := by
--------------------------------------------------------------------
  intro hâ‚
  apply Eq.to_iff
  
  -- Do simplifications to get to the weighted sum
  simp only [activ]
  rw [hebb_activation net A]
  rw [hebb_preds net A]
  apply congr_arg (fun x => Net.activation net.toNet x = 1.0)

  -- The weighted sums are equal, âˆ‘ wâ‚ xâ‚ = âˆ‘ wâ‚‚ xâ‚‚,
  -- if all of their entries are equal, wâ‚áµ¢ * xâ‚áµ¢ = wâ‚‚áµ¢ * xâ‚‚áµ¢
  apply weighted_sum_eq
  intro i
  generalize hm : List.get! (Graph.predecessors net.toNet.graph n) i = m

  -- We have two cases;
  by_cases m âˆˆ propagate net (B âˆª 
    reachable net (propagate net A) (propagate net B))
  
  -- m âˆˆ Prop(B âˆª Reach(Prop(A), Prop(B)))
  -- In this case, the RHS's reduce to 1.0, and we
  -- just need to argue that the weights are the same
  case pos => 
    -- First, notice that m âˆ‰ Prop(A).
    have hâ‚‚ : m âˆˆ preds net n := sorry
    have hâ‚ƒ : m âˆ‰ propagate net A := by
      sorry
      -- not quite right
      -- exact not_mem_subset _ (hâ‚ m hâ‚‚) 

    -- Now we simplify and show that the weights are the same
    simp only [propagate, Membership.mem, Set.Mem] at h
    rw [if_pos h]
    rw [one_mult]
    rw [one_mult]
    exact hebb_weightsâ‚‚ _ hâ‚ƒ

  -- Otherwise, the RHS's reduce to 0.0, and so the
  -- weighted sums are trivially equal
  case neg => 
    simp only [propagate, Membership.mem, Set.Mem] at h
    rw [if_neg h]
    rw [zero_mult]
    rw [zero_mult]


-- -- If *some* predecessor of n is âˆˆ Prop(A), and n âˆˆ Prop(A), then
-- -- if m is activated in (hebb_star net) then n is too
-- -- (the activation automatically goes through in (hebb_star net))
--------------------------------------------------------------------
theorem simp_hebb_activâ‚ƒ (net : BFNN) (A B : Set â„•) :
  let preds := preds net n
  let prev_activ := do
    let i <- List.range preds.length
    let m := preds.get! i
    return if propagate_acc (hebb_star net A) B m 
      (layer (hebb_star net A) m) then 1.0 else 0.0
  
  n âˆˆ propagate net A
  â†’ m âˆˆ preds âˆ§ m âˆˆ propagate net A  
  â†’ m âˆˆ propagate (hebb_star net A) B
  â†’ activ (hebb_star net A) prev_activ n := by
--------------------------------------------------------------------
  intro preds
  intro prev_activ
  intro hâ‚
  intro hâ‚‚
  intro hâ‚ƒ

  simp only [activ]
  rw [hebb_activation net A]
  rw [hebb_preds net A]
  
  sorry -- I have the proof written on paper, I should consult that.
        -- Depends on things like the monotonicity of 'activation', etc.


-- If there is a path within Prop(A) from B to n, then, since this
-- path is updated in hebb_star, n âˆˆ Prop(B).
--------------------------------------------------------------------
theorem hebb_updated_path (net : BFNN) (A B : Set â„•) :
  reachable net (propagate net A) (propagate net B)
  âŠ† propagate (hebb_star net A) B := by
--------------------------------------------------------------------
  intro (n : â„•)
  intro hâ‚
  
  -- By induction on the layer of the net containing n
  generalize hL : layer net n = L
  induction L using Nat.case_strong_induction_on generalizing n

  --------------------------------
  -- Base Step
  --------------------------------
  -- Easy; at layer zero, n âˆˆ propagate net B,
  --       and so by extensive n âˆˆ propagate (hebb_star net A) B.
  case hz => 
    have hâ‚‚ : n âˆˆ propagate net B :=
      reach_layer_zero _ _ _ _ hL hâ‚
    exact hebb_extensive _ _ _ hâ‚‚

  --------------------------------
  -- Inductive Step
  --------------------------------
  case hi L IHâ‚ => 
    -- We have a path from Prop(B) to n, going through Prop(A).
    match hâ‚ with
    | âŸ¨m, hmâŸ© => 
      -- By induction on the length of this path
      induction hm.2

      case trivial _ => exact hebb_extensive _ _ _ hm.1
      case from_path x y path_mx edge_xy hy _ => 
        -- Split by whether y âˆˆ B in order to simplify propagate_acc
        by_cases y âˆˆ B
        case pos => exact propagate_is_extens _ _ h
        case neg =>
          -- Simplifications and rewriting
          simp only [propagate, Membership.mem, Set.Mem]
          rw [hebb_layers]
          rw [hL]
          rw [simp_propagate_acc _ h]
          rw [hebb_preds]
          simp

          -- Apply our inductive hypothesis: x âˆˆ Prop(B) in (hebb_star net) 
          have hpreds : x âˆˆ preds net y := (edge_from_preds _ _ _).mpr edge_xy
          have hpred_dec : layer net x â‰¤ L := 
            (Nat.lt_succ).mp (lt_of_lt_of_eq (preds_decreasing _ _ _ hpreds) hL)
          have hx_reachable : x âˆˆ reachable net (propagate net A) (propagate net B) :=
            âŸ¨m, âŸ¨hm.1, path_mxâŸ©âŸ©
          have hx_propA : x âˆˆ propagate net A := 
            reach_subset _ _ _ hx_reachable
          have hx_propB : x âˆˆ propagate (hebb_star net A) B := 
            IHâ‚ (layer net x) hpred_dec hx_reachable rfl
          
          -- Apply simp_hebb_activâ‚ƒ, which says:
          --  x, y âˆˆ Prop(A)
          --  There's an edge from x âŸ¶ y
          --  x âˆˆ Prop(B) in (hebb_star net)
          --  -------------------------------
          --  y is activ in hebb_star net
          exact simp_hebb_activâ‚ƒ net A B hy âŸ¨hpreds, hx_propAâŸ© hx_propB


/-â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Reduction for Unstable Hebbian Update
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•-/

-- These two are the big theorems.
-- They explain what hebb_star learns in terms of what the net
-- believed *before* update -- it turns out that we can completely
-- reduce the dynamic behavior to the static behavior.
--------------------------------------------------------------------
theorem hebb_reduction_empty (net : BFNN) (A B : Set â„•) : 
  propagate net A âˆ© propagate net B = âˆ… â†’

  propagate (hebb_star net A) B = propagate net B := by 
--------------------------------------------------------------------
  intro h_empty
  apply ext
  intro (n : â„•)

  -- By induction on the layer of the net containing n
  generalize hL : layer net n = L
  induction L using Nat.case_strong_induction_on generalizing n

  --------------------------------
  -- Base Step
  --------------------------------
  -- Easy; after simplifying, show that B = B.
  case hz =>
    simp only [Membership.mem, Set.Mem, propagate]
    rw [hebb_layers net A]
    rw [hL]
    simp only [propagate_acc]

  --------------------------------
  -- Inductive Step
  --------------------------------
  case hi L IH => 
    -- Backwards direction is easy;
    -- As for forward direction, TODO
    apply Iff.intro
    case mpr => exact fun hâ‚ => hebb_extensive _ _ _ hâ‚
    case mp =>
      intro hâ‚

      -- Split by whether n âˆˆ B, in order to simplify propagate_acc
      by_cases n âˆˆ B
      case pos => exact propagate_is_extens _ _ h
      case neg =>
        -- Proof Idea:
        -- Since n âˆˆ Prop(B) in (hebb_star net),
        -- its active predecessors m are âˆˆ Prop(B) in (hebb_star net).
        -- But by IH these m âˆˆ Prop(B) in the original net.
        -- Since Prop(A) âˆ© Prop(B) is empty, these m âˆ‰ Prop(A).
        -- By [lemma activâ‚‚], Prop(B) is the same in both nets.
        
        -- First, we show that any *active* predecessor of n 
        -- cannot be in Prop(A). (If it were, then it would
        -- be in both Prop(B) and Prop(A) -- but the intersection
        -- is empty!)
        have hâ‚‚ : (âˆ€ x, (x âˆˆ preds net n âˆ§ x âˆˆ propagate net B) â†’ x âˆ‰ propagate net A) := by
          intro x
          intro hâ‚ƒ
          intro contr_assump
          exact absurd h_empty (Set.Nonempty.ne_empty 
            (Set.nonempty_of_mem (Set.mem_inter contr_assump hâ‚ƒ.2)))
        
        -- Simplifications and rewriting, to prepare for IH
        simp only [propagate, Membership.mem, Set.Mem]
        simp only [propagate, Membership.mem, Set.Mem] at hâ‚
        simp only [propagate, Membership.mem, Set.Mem] at IH
        rw [hebb_layers] at hâ‚
        rw [hL] at hâ‚
        rw [hL]
        rw [simp_propagate_acc _ h]
        rw [simp_propagate_acc _ h] at hâ‚

        -- Use simp_hebb_activâ‚‚, which says that if all of the *active* 
        -- preds âˆ‰ Prop(A), the activ's are the same.
        rw [hebb_preds net A] at hâ‚
        sorry
        /-
        rw [simp_hebb_activâ‚‚ _ _ _ hâ‚‚] at hâ‚
        conv at hâ‚ => -- rewrite 'layers'
          enter [2, 2, i, m, 1]
          rw [hebb_layers net A]
        simp
        simp at hâ‚
        convert hâ‚ using 5
        rename_i i
        generalize hm : List.get! (net.graph.predecessors n) i = m
        generalize hLm : layer net m = Lm
        conv at IH => -- rewrite 'layers' in IH
          enter [L, hL, m, hLm]
          rw [hebb_layers]
          rw [hLm]
        
        -- We are now ready to apply our inductive hypothesis!
        have hâ‚ƒ : m âˆˆ preds net n := by
          rw [symm hm]
          simp [preds]
          exact get!_mem (net.graph.predecessors n) i
        have hâ‚„ : Lm â‰¤ L := by
          rw [symm hLm]
          apply Nat.lt_succ.mp
          rw [symm hL]
          exact preds_decreasing net m n hâ‚ƒ
        exact (symm (IH Lm hâ‚„ m hLm).to_eq).to_iff
        -/

--------------------------------------------------------------------
theorem hebb_reduction_nonempty (net : BFNN) (A B : Set â„•) : 
  propagate net A âˆ© propagate net B â‰  âˆ… â†’

  propagate (hebb_star net A) B =
  propagate net (B âˆª reachable net (propagate net A) (propagate net B)) := by 
--------------------------------------------------------------------
  intro h_nonempty
  apply ext
  intro (n : â„•)
  simp only [Membership.mem, Set.Mem, propagate]
  
  -- By induction on the layer of the net containing n
  generalize hL : layer net n = L
  induction L using Nat.case_strong_induction_on generalizing n

  --------------------------------
  -- Base Step
  --------------------------------
  case hz => 
    -- First, do the base case simplifications
    rw [hebb_layers]
    rw [hL]
    simp only [propagate_acc]
    simp only [Union.union, Set.union, Membership.mem, Set.Mem, setOf]

    -- Forward direction is the easy part, so we do it first.
    -- Backwards direction relies on reach_layer_zero,
    -- a fact about paths when we know n is at layer 0.
    apply Iff.intro
    case mp => exact fun hâ‚ => Or.inl hâ‚
    case mpr => 
      intro hâ‚

      -- Either n âˆˆ B or n is reachable from Prop(B) using only
      -- paths within Prop(A).  At layer 0, this means n âˆˆ B.
      cases hâ‚
      case inl hâ‚‚ => exact hâ‚‚
      case inr hâ‚‚ => 
        have heq : layer net n = 0 := 
          Eq.trans (symm (hebb_layers net A)) (Eq.trans (hebb_layers _ _) hL)
        exact prop_layer_zero _ _ _ heq (reach_layer_zero _ _ _ _ heq hâ‚‚)

  --------------------------------
  -- Inductive Step
  --------------------------------
  case hi L IH => 
    apply Iff.intro
    
    ---------------------
    -- Backward Direction
    ---------------------
    case mpr =>
      intro hâ‚
      
      -- By cases; either n âˆˆ B âˆª Reach(Prop(A), Prop(B)), or not.
      by_cases n âˆˆ B âˆª reachable net (propagate net A) (propagate net B)
      case pos =>
        -- From here, we split again; either:
        --    1. n âˆˆ B, and by extens n âˆˆ Prop(B) in (hebb_star net)
        --    2. n âˆˆ Reach(Prop(A), Prop(B)).  In this case, this path
        --       has been updated in the (hebb_star net), so of course
        --       n âˆˆ Prop(B) in (hebb_star_net)
        --       i.e. apply [hebb_updated_path]! 
        cases h
        case inl hâ‚‚ => exact propagate_acc_is_extens _ _ hâ‚‚
        case inr hâ‚‚ =>
          have hâ‚ƒ : n âˆˆ propagate (hebb_star net A) B := 
            hebb_updated_path _ _ _ hâ‚‚
          simp only [propagate, Membership.mem, Set.Mem] at hâ‚ƒ
          exact hâ‚ƒ

      case neg =>
        -- We get ready to simplify propagate_acc
        have n_not_in_B : n âˆ‰ B :=
          fun n_in_B => absurd (Set.mem_union_left _ n_in_B) h

        -- Simplifications and rewriting, to prepare for IH
        -- We also rewrite the 'preds', 'layers', and 'activ'
        -- for (hebb_star net) in terms of the original 'net'.
        rw [hebb_layers]
        rw [hL]
        simp only [propagate] at h
        rw [simp_propagate_acc _ n_not_in_B]
        rw [simp_propagate_acc _ h] at hâ‚
        rw [hebb_preds net A] -- rewrite 'preds'
        
        -- The plan is to now apply our inductive hypothesis
        -- and use the 'activ_agree' lemmas.
        -- We write the two sets as Sâ‚ and Sâ‚‚ for convenience 
        let Sâ‚ := fun m => 
          propagate_acc net (B âˆª reachable net (fun n => 
            propagate_acc net A n (layer net n)) fun n => 
              propagate_acc net B n (layer net n))
            m (layer net m)
        let Sâ‚‚ := fun m => propagate_acc (hebb_star net A) B m (layer (hebb_star net A) m)

        -- Apply IH to the predecessors
        have hâ‚‚ : âˆ€ (m : â„•), m âˆˆ preds net n â†’ (Sâ‚ m â†” Sâ‚‚ m) := by
          simp only [Membership.mem] -- really just want to unfold the let
          intro m hm
          generalize hLm : layer net m = Lm
          have hâ‚ƒ : Lm â‰¤ L := by
            rw [symm hLm]
            apply Nat.lt_succ.mp
            rw [symm hL]
            exact preds_decreasing net m n hm
          exact (symm (IH Lm hâ‚ƒ m hLm).to_eq).to_iff

        -- Argument: 
        -- By activ_agree, the predecessors m âˆˆ Prop(B) (over hebb_star)
        --   activate n *in the original net*
        -- But the weights in the updated net are either the same or
        --   increasing, so by hebb_activ, these same predecessors
        --   activate n *in the updated net*.
        simp
        simp at hâ‚
        exact hebb_activ net A Sâ‚‚ _
          (activ_agree _ Sâ‚ Sâ‚‚ _ hâ‚‚ hâ‚)

    ---------------------
    -- Forward Direction
    -- (similar, but differs slightly in Case 1)
    ---------------------
    -- This direction is a bit more tricky. This
    -- is where we rely on the net being fully
    -- connected & transitively closed.
    case mp => 
      intro hâ‚
      
      -- By cases; either n âˆˆ B âˆª reachable net (propagate net A) B, 
      -- or not.
      by_cases n âˆˆ B âˆª reachable net (propagate net A) (propagate net B)
      case pos => 
        -- In this case, just apply propagate_is_extens
        rw [â† hL]
        simp only [propagate] at h
        exact propagate_acc_is_extens _ _ h

      case neg hâ‚‚ => 
        -- From here, we split *again*, depending on whether n âˆˆ Prop(A).
        by_cases n âˆˆ propagate net A

        ---------------------
        -- Case 1: n âˆˆ Prop(A)
        ---------------------
        case pos => 
          -- Since Prop(A) âˆ© Prop(B) is nonempty, and
          -- Prop(A) âˆ© Prop(B) âŠ† Prop(A) âˆ© Prop(B âˆª Reach(Prop(A), Prop(B))) 
          --                   = S   (we abbreviate the set for convenience)
          -- there is an m in the latter set.
          generalize hS : (propagate net A) âˆ© (propagate net (B âˆª reachable net (propagate net A) (propagate net B))) = S
          have h_nonemptyS : Set.Nonempty S := by 
            rw [â† hS]
            rw [â† Set.nonempty_iff_ne_empty] at h_nonempty 
            exact match h_nonempty with
            | âŸ¨m, hmâŸ© => âŸ¨m, âŸ¨hm.1, propagate_is_extens _ _ 
              (mem_union_right _ (reach_is_extens _ _ _ hm))âŸ©âŸ©

          -- By the well-ordering principle, let m be the node
          -- in this set with the *smallest layer*
          let m := WellFounded.min (layer_wellfounded net) S h_nonemptyS

          -- This m is both in the set, and is the smallest such m.
          have hm : m âˆˆ S := WellFounded.min_mem _ _ _
          have hâ‚ƒ : âˆ€ x, x âˆˆ S â†’ layer net m â‰¤ layer net x := 
            fun x hx => le_of_not_le 
              (WellFounded.not_lt_min (layer_wellfounded net) _ _ hx) 
          
          -- We don't know whether n âˆˆ S yet,
          -- but we do know that either:
          --    - layer(m) < layer(n), or
          --    - layer(m) â‰¥ layer(n).
          -- So we split into these two cases.
          cases lt_or_ge (layer net m) (layer net n)

          ---------------------
          -- Case 1.1: n âˆˆ Prop(A)
          -- and layer[m] < layer[n]
          ---------------------
          -- Since the net is transitively closed, we have an
          -- edge from m to n.  Since
          --     m âˆˆ S = Prop(A) âˆ© Prop(B âˆª Reach(Prop(A), Prop(B)))
          -- and n âˆˆ Prop(A),
          -- we get the unfortunate expression
          --     n âˆˆ Reach(Prop(A), Prop(B âˆª Reach(Prop(A), Prop(B)))).
          -- The trick from here is to show, by Reach-Prop algebra:
          --     n âˆˆ Reach(Prop(A), Prop(B)),
          -- and so
          --     n âˆˆ Prop(B âˆª Reach(Prop(A), Prop(B)))
          --
          -- This is where the real action of the proof happens. 
          case inl hâ‚„ => 
            -- We apply the fact that the net is fully connected to get an
            -- edge mâŸ¶n.
            have hâ‚… : Graph.hasEdge net.toNet.graph m n := by
              exact connected _ m n hâ‚„

            -- So we have n âˆˆ Reach(Prop(A), Prop(B âˆª Reach(Prop(A), Prop(B))))
            -- (the unfortunately ugly expression)
            have hâ‚† : n âˆˆ reachable net (propagate net A) 
              (propagate net (B âˆª reachable net (propagate net A) 
                (propagate net B))) := by
              rw [â† hS] at hm
              exact âŸ¨m, âŸ¨hm.2, 
                focusedPath.from_path (focusedPath.trivial hm.1) hâ‚… hâŸ©âŸ©

            -- By algebra, this reduces to n âˆˆ Reach(Prop(A), Prop(B))
            -- TODO: Can I clean up the congr_arg parts?  What extensionality
            -- lemma do I need to use here???        
            have hâ‚‡ : reachable net (propagate net A) B âŠ† 
                      reachable net (propagate net A) 
                        (reachable net (propagate net A) (propagate net B)) := by
              intro x hx
              rw [â† reach_is_idempotent _ _ _]
              exact reach_is_monotone _ _ _ _ (propagate_is_extens _ _) hx

            have hâ‚ˆ : n âˆˆ reachable net (propagate net A) (propagate net B) := by
              apply (congr_arg (fun X => n âˆˆ X) (reach_is_idempotent _ _ _)).mpr
              apply (congr_arg (fun X => n âˆˆ X) (Set.union_eq_right_iff_subset.mpr hâ‚‡)).mp
              apply (congr_arg (fun X => n âˆˆ X) (reach_union _ _ _ _)).mp
              exact reach_propagate _ _ _ hâ‚†

            -- Since                 n âˆˆ Reach(Prop(A), Prop(B)),
            -- We have      n âˆˆ Prop(B âˆª Reach(Prop(A), Prop(B)))
            simp only [propagate] at hâ‚ˆ
            rw [â† hL]
            exact propagate_acc_is_extens net _ (Set.mem_union_right _ hâ‚ˆ)

          ---------------------
          -- Case 1.2: n âˆˆ Prop(A)
          -- and layer[m] â‰¥ layer[n]
          ---------------------
          -- Since m âˆˆ S is the node with the *smallest layer*,
          -- this means *none* of n's predecessors are âˆˆ S.
          -- But S = Prop(A) âˆ© Prop(B âˆª Reach(Prop(A), Prop(B))),
          -- which means that none of n's active predecessors
          -- (i.e. âˆˆ Prop(B âˆª Reach(Prop(A), Prop(B))) by IH)
          -- are in Prop(A).
          -- 
          -- i.e. n's preceding edges were not updated,
          -- and so the activ's are the same.
          case inr hâ‚„ => 
            -- First, we show that any predecessor of n cannot be in S.
            -- (if it was in S, it's layer would be smaller than m's)
            have hâ‚… : âˆ€ x, x âˆˆ preds net n â†’ x âˆ‰ S := by
              exact fun x hx x_in_S =>
                have m_smaller : layer net m â‰¤ layer net x := (hâ‚ƒ x x_in_S)
                have m_bigger : layer net m > layer net x := 
                  (gt_of_ge_of_gt hâ‚„ (preds_decreasing _ _ _ hx)) 
                absurd m_smaller (not_le_of_gt m_bigger)

            -- We get ready to simplify propagate_acc
            rename_i n_not_in_reach
            have n_not_in_B : n âˆ‰ B :=
              fun n_in_B => absurd (Set.mem_union_left _ n_in_B) n_not_in_reach

            -- Simplifications and rewriting, to prepare for IH
            -- We also rewrite the 'preds', 'layers', and 'activ'
            -- for (hebb_star net) in terms of the original 'net'.
            rw [hebb_layers] at hâ‚
            rw [hL] at hâ‚
            simp only [propagate] at n_not_in_reach
            rw [simp_propagate_acc _ n_not_in_B] at hâ‚
            rw [simp_propagate_acc _ n_not_in_reach]
            rw [â† hS] at hâ‚…
            rw [hebb_preds net A] at hâ‚ -- rewrite 'preds'
            simp
            simp at hâ‚

            -- Get ready to apply IH
            -- We write down the usual lemmas for 'm', but we don't
            -- know what the index 'i' is we're grabbing yet.  So
            -- we write these for all i.
            generalize hm : List.get! (Graph.predecessors net.toNet.graph n) = m
            have hâ‚† : âˆ€ i, (m i) âˆˆ preds net n := by
              intro i
              rw [symm hm]
              simp [preds]
              exact get!_mem (net.graph.predecessors n) i

            have hâ‚‡ : âˆ€ i, (layer net (m i)) â‰¤ L := by
              intro i
              apply Nat.lt_succ.mp
              rw [symm hL]
              exact preds_decreasing net (m i) n (hâ‚† i)

            -- Go into hâ‚ and apply our inductive hypothesis
            conv at hâ‚ =>
              enter [2, 2, i, 1]
              rw [hm]
              rw [IH (layer net (m i)) (hâ‚‡ i) (m i) rfl]
            
            -- Unpack the (m i) term
            rw [â† hm] at hâ‚
            rw [â† hm]

            -- Use simp_hebb_activâ‚‚, which says that if all
            -- of the *active* preds âˆ‰ Prop(A), the activ's are the same.
            rw [simp_hebb_activâ‚‚ _ _ _ hâ‚…] at hâ‚
            exact hâ‚            

        ---------------------
        -- Case 2: n âˆ‰ Prop(A)
        ---------------------
        -- In this case, the activ's are the same, so we can
        -- straightforwardly apply our inductive hypothesis.
        case neg =>
          -- We get ready to simplify propagate_acc
          rename_i n_not_in_reach
          have n_not_in_B : n âˆ‰ B :=
            fun n_in_B => absurd (Set.mem_union_left _ n_in_B) n_not_in_reach

          -- Simplifications and rewriting, to prepare for IH
          -- We also rewrite the 'preds', 'layers', and 'activ'
          -- for (hebb_star net) in terms of the original 'net'.
          rw [hebb_layers] at hâ‚
          rw [hL] at hâ‚
          simp only [propagate] at n_not_in_reach
          rw [simp_propagate_acc _ n_not_in_B] at hâ‚
          rw [simp_propagate_acc _ n_not_in_reach]
          
          rw [simp_hebb_activâ‚ _ _ _ h] at hâ‚ -- rewrite 'activ'
          rw [hebb_preds net A] at hâ‚ -- rewrite 'preds'
          conv at hâ‚ => -- rewrite 'layers'
            enter [2, 2, i, m, 1]
            rw [hebb_layers net A]
          simp
          simp at hâ‚
          convert hâ‚ using 5
          rename_i i
          generalize hm : List.get! (net.graph.predecessors n) i = m
          generalize hLm : layer net m = Lm
          conv at IH => -- rewrite 'layers' in IH
            enter [L, hL, m, hLm, 1]
            rw [hebb_layers]
            rw [hLm]

          -- We are now ready to apply our inductive hypothesis!
          have hâ‚‚ : m âˆˆ preds net n := by
            rw [symm hm]
            simp [preds]
            exact get!_mem (net.graph.predecessors n) i
          have hâ‚ƒ : Lm â‰¤ L := by
            rw [symm hLm]
            apply Nat.lt_succ.mp
            rw [symm hL]
            exact preds_decreasing net m n hâ‚‚
          exact (symm (IH Lm hâ‚ƒ m hLm).to_eq).to_iff
          

-- TODO: Prove that we can unravel these nets into ordinary
-- feedforward nets
-- 
-- TODO: Email David Sprunger about follow-up papers to
-- "backprop as a functor"

/-â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  The Logic (Language and Semantics)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•-/



/-â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Inference Rules and Proof System
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•-/




/-â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Soundness
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•-/





-- propagate_N (S) = propagate_hebb(N, S) (S)
-- This essentially says that repeated hebbian update
-- is well-defined; *after* updating on S, we can update
-- on S again and increase weights within the same propagation.
-- (The propagation of S doesn't suddenly change, which is
--  something we might be worried about.)
-- TODO: Not sure if I need this anymore!
-- It's somewhat interesting, but might not help with the
-- reduction.
-- --------------------------------------------------------------------
-- theorem hebb_iteration_is_well_defined (net : BFNN) (S : Set â„•) : 
--   propagate (hebb net S) S = propagate net S := by
-- --------------------------------------------------------------------
--   apply ext
--   intro (n : â„•)
--   simp only [Membership.mem, Set.Mem, propagate]

--   -- By induction on the layer of the net containing n
--   generalize hL : layer net n = L
--   induction L using Nat.case_strong_induction_on generalizing n

--   -- Base Step
--   case hz =>
--     apply Iff.intro
--     case mp => 
--       simp only [propagate_acc]
--       exact fun x => x
--     case mpr => 
--       simp only [propagate_acc]
--       exact fun x => x

--   -- Inductive Step
--   case hi k IH => 
--     apply Iff.intro

--     -- Forward Direction
--     case mp => 
--       intro hâ‚
--       simp only [propagate_acc] at hâ‚
--       simp only [propagate_acc]

--       cases hâ‚
--       case inl hâ‚‚ => exact Or.inl hâ‚‚
--       case inr hâ‚‚ =>
--         apply Or.inr

--         -- TODO: This is the stuff that should go in the activ_agree lemma!        
--         simp
--         simp at hâ‚‚
--         sorry
--         -- I do not have the tools to show this at this point.
--         -- I need a lemma about activations in the hebbian updated net.

--         -- show_term convert hâ‚‚

--     -- Backwards Direction
--     case mpr => sorry

-- This says that 'hebb_star' is a fixed point of 'hebb'
-- (with respect to â‰¡).  i.e. in the following sense, f(X) = X:
--   hebb(X, S) â‰¡ X,
-- where X = hebb_star net S
-- 
-- I may need additional lemmas (e.g. an activation function
-- within Prop(S) in hebb_star will simply go through.)
-- 
-- One important lemma:  If an edge is not in the propagation of S,
-- its weight is unaffected.
-- 
-- NOTE: Not sure if I need this before.  It's interesting,
-- but I'm not sure if it helps with the proof for the reduction.
-- --------------------------------------------------------------------
-- theorem hebb_star_is_fixed_point (net : BFNN) (S : Set â„•) : 
--   hebb (hebb_star net S) S â‰¡ hebb_star net S := by 
-- --------------------------------------------------------------------
--   sorry