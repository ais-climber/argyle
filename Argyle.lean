import Mathlib.Tactic.LibrarySearch
import Mathlib.Tactic.NthRewrite
import Mathlib.Mathport.Syntax
import Std.Tactic.ShowTerm
import Lean.Meta.Tactic.Simp.Main
import Mathlib.Tactic.Basic
import Mathlib.Data.List.Sigma

import Lean.Parser.Tactic
-- import Graph.Graph
-- import Graph.TopologicalSort
import Mathlib.Init.Set
import Mathlib.Data.List.Defs
import Mathlib.Init.Propext
import Mathlib.Data.Set.Basic
import Mathlib.Logic.Basic
import Mathlib.Logic.Function.Basic

-- open Graph
open Set
open Tactic
open Classical

-- Doesn't detect inefficient code!
set_option maxHeartbeats 0

-------------------------------------------------
-- Goofing about with inductive types
-------------------------------------------------

inductive my_lte : â„• â†’ â„• â†’ Prop where
  | reflexive : my_lte n n
  | from_succ : my_lte m x â†’ (n = x + 1) â†’ my_lte m n

-- #eval my_lte 1 3





-------------------------------------------------
-- List comprehensions,
-- courtesy of lovettchris
-- See: 
--   https://github.com/leanprover/lean4-samples/blob/main/ListComprehension/ListComprehension.lean
-------------------------------------------------

declare_syntax_cat compClause
syntax "for " term " in " term : compClause
syntax "if " term : compClause

syntax "[" term " | " compClause,* "]" : term

def List.map' (xs : List Î±) (f : Î± â†’ Î²) : List Î² := List.map f xs

macro_rules
  | `([$t:term |]) => `([$t])
  | `([$t:term | for $x in $xs]) => `(List.map' $xs  (Î» $x => $t))
  | `([$t:term | if $x]) => `(if $x then [$t] else [])
  | `([$t:term | $c, $cs,*]) => `(List.join [[$t | $cs,*] | $c])

def prod_comprehens (xs : List Î±) (ys : List Î²) : List (Î± Ã— Î²) := 
  [(x, y) | for x in xs, for y in ys]

#eval [(x, y) | for x in [1, 2], for y in [3, 4]]


-------------------------------------------------
-- Weighted Directed Graphs
-- 
-- Since graphs are internally represented by lists,
-- we can just do induction on this inner list.
-- 
-- It's common for graph theory proofs to go by
-- induction on the graph:  "Suppose P holds for the
-- subgraph, and now add a new node."  In this case
-- what you probably want is to do induction on
-- *Graph.vertices.reverse*, so that each new node
-- can only "see" those nodes that come *before* it.
-- 
-- NOTE: For hygiene, we expect that the vertices
-- are natural numbers given in order, i.e. 0, 1, 2, ...
-- In principle, you can pick any other labels for your 
-- vertices, but I will be relying on the fact that they come
-- in this order.  My apologies.
-- 
-- These graphs are based on Peter Kementzey's 
-- implementation, but modified slightly.
-------------------------------------------------

-- Î± is the type of the nodes
-- Î² is the type of the weights
structure Vertex (Î± Î² : Type) where
  label : Î± 
  successors : List (â„• Ã— Î²)
deriving Repr

instance [Inhabited Î±] : Inhabited (Vertex Î± Î²) := 
  âŸ¨ { label := default, successors := default } âŸ©

structure Graph (Î± : Type) (Î² : Type) where
  vertices : List (Vertex Î± Î²) := []
deriving Repr

-- Notice that this graph is acyclic, since each predecessor
-- list only refers to nodes above the current node.  This
-- is foreshadowing.
def graphA : Graph â„• Float :=
  âŸ¨[âŸ¨0, [âŸ¨1, 0.5âŸ©, âŸ¨2, 0.6âŸ©, âŸ¨3, 0.7âŸ©]âŸ©, 
    âŸ¨1, [âŸ¨2, 0.8âŸ©, âŸ¨3, 0.9âŸ©]âŸ©, 
    âŸ¨2, [âŸ¨3, 1.0âŸ©, âŸ¨3, 3.0âŸ©]âŸ©, 
    âŸ¨3, []âŸ©]âŸ©

#check graphA
#eval graphA

-------------------------------------------------
-- Graph functions
-------------------------------------------------

-- TODO: for convenience, define 'map', 'foldl', 'filter',
-- 'find?', 'zip', 'some', 'any', and 'all' over graph vertices.

-- TODO: Make graph functions computable! (this shouldn't be
-- hard -- right now it just depends on 'Prop')
namespace Graph

def get_vertices (g : Graph â„• Float) : List â„• :=
  g.vertices.map (fun âŸ¨label, _âŸ© => label)

def successors (g : Graph â„• Float) (n : â„•) : List â„• :=
  g.vertices[n]!.successors.unzip.1

def predecessors (g : Graph â„• Float) (n : â„•) : List â„• :=
  List.filter (fun v => n âˆˆ (g.successors v)) g.get_vertices

-- Using 'predecessors' is slower than 'successors',
-- but we will tend to look backwards from a node rather
-- than forwards.
def hasEdge (g : Graph â„• Float) (m n : â„•) : Bool :=
  m âˆˆ (g.predecessors n)

-- Returns the weight of the edge m âŸ¶ n, if it exists.
-- If it does not exist, we say the weight is 0.0
-- TODO: In the future, it's better to use Option here
-- and return none if none!!!
def getEdgeWeight (g : Graph â„• Float) (m n : â„•) : Float :=
  match g.vertices[m]!.successors.find? (fun âŸ¨label, _âŸ© => label = n) with
  | some âŸ¨_, weightâŸ© => weight
  | none => 0.0

-- Some sanity checks
#eval get_vertices graphA
#eval successors graphA 0
#eval successors graphA 1
#eval successors graphA 2
#eval successors graphA 3
#eval predecessors graphA 0
#eval predecessors graphA 1
#eval predecessors graphA 2
#eval predecessors graphA 3
#eval hasEdge graphA 1 2
#eval hasEdge graphA 1 3
#eval hasEdge graphA 4 2
#eval getEdgeWeight graphA 1 2
#eval getEdgeWeight graphA 1 3
#eval getEdgeWeight graphA 4 2


inductive hasPath (g : Graph â„• Float) : â„• â†’ â„• â†’ Prop where
  | trivial {u : â„•} :
      hasPath g u u
  | from_path {u v w : â„•} : 
      hasPath g u v â†’ hasEdge g v w â†’ hasPath g u w

/-
TODO for later:  Make 'hasPath' computable so that we can execute
this code:
> #eval hasPath graphA 1 3

Some old code when I was trying to do this:

instance decPath : Decidable (hasPath g u v) :=
  sorry -- this should implement BFS!!!
  -- if h : u = v then
  --   isTrue (Eq.subst h hasPath.trivial)
  -- else if h : hasEdge g u v then
  --   isTrue (hasPath.from_path (hasPath.trivial) h)
  -- else
  --   sorry

instance decLte : Decidable (my_lte m n) :=
  if h : m = n then
    .isTrue (h â–¸ .trivial)
  else
    match n with
    | x + 1 =>
      have := @decLte m x
      decidable_of_iff (my_lte m x) âŸ¨(.from_path Â· rfl), fun h => by
        cases h with
        | trivial => cases h rfl
        | from_path h e => exact Nat.succ.inj e â–¸ hâŸ©
    | 0 => .isFalse fun h => by
      cases h with
      | trivial => exact h rfl
      | from_path h e => cases e
-/


--------------------------------------------------------------------
theorem hasPath_trans {u v w : â„•} (g : Graph â„• Float) :
  hasPath g u v â†’ hasPath g v w â†’ hasPath g u w := by
--------------------------------------------------------------------
  intro (hâ‚ : hasPath g u v)
  intro (hâ‚‚ : hasPath g v w)

  induction hâ‚‚
  case trivial => exact hâ‚
  case from_path x y _ edge_xy path_ux => 
    exact hasPath.from_path path_ux edge_xy


def is_refl (g : Graph â„• Float) : Prop := âˆ€ (u : â„•), 
  u âˆˆ g.get_vertices â†’ g.hasEdge u u

def is_symm (g : Graph â„• Float) : Prop := âˆ€ (u v : â„•), 
  g.hasEdge u v â†’ g.hasEdge v u

def is_trans (g : Graph â„• Float) : Prop := âˆ€ (u v w : â„•),
  g.hasEdge u v â†’ g.hasEdge v w â†’ g.hasEdge u w

def is_acyclic (g : Graph â„• Float) : Prop := âˆ€ (u v : â„•),
  g.hasPath u v â†’ g.hasPath v u â†’ u = v

end Graph

/-
TODO:  Define 'acyclic' as:  Recursively on graph.vertices,
every vertex can only "see" the vertices ahead of it.

TODO: We want to be able to check if a graph is acyclic by
just "computing" it -- i.e. we call Topological Sort on the
graph, and if successful we know it is acyclic.

So here is some old code I was using to try to do topological
sort.  I'll need to come back to this when I want to make
everything in this library computable.
namespace TopologicalSort

-- @[simp]
-- def topol_sort (g : Graph â„• Float) :=
--   (topSortUnsafe g).toList.reverse

-- holds iff u precedes v in array
-- note that we assume lst elements are all distinct
def list_precedes (lst : List â„•) (u v : â„•) : Bool :=
  match lst with
    | List.nil => false
    | List.cons x xs =>
      -- If we find 'u' first, and v is in the rest, true
      if x = u âˆ§ v âˆˆ xs then 
        true
      else 
        list_precedes xs u v

def listA : List â„• :=
  [2, 4, 9, 8, 5]

-- a couple of unit tests for good measure
#eval list_precedes listA 4 8 -- true
#eval list_precedes listA 2 8 -- true
#eval list_precedes listA 2 4 -- true
#eval list_precedes listA 2 9 -- true
#eval list_precedes listA 9 5 -- true

#eval list_precedes listA 8 2 -- should be false, is true
#eval list_precedes listA 5 9 -- should be false, is true

#eval list_precedes listA 1 7 -- undefined (false)
#eval list_precedes listA 9 9 -- false, makes sure an element
                              -- does not precede itself.

-- The ordering induced by Topological Sort
-- TODO: Rewrite as an inductive data type!
/-
def topOrder (g : Graph â„• Î²) (u v : â„•) : Prop :=
  match (topSort g) with
    | some sorted => list_precedes sorted.toList u v
    | none => sorry
-/

-- inductive TopologicalOrdering (g : Graph â„• Î²) (u : â„•) where
--   | constr1 : TopologicalOrdering g u
--   | constr2 (x : â„•) : TopologicalOrdering g u

-- inductive graph_â‰º (g : Graph â„• Î²) (u v : â„•) where
--   | constr1 : sorry
--   | constr2 : sorry

-- Says that Topological Sort is actually correct, i.e.
-- if there is an edge from x to y, then x â‰º y in the ordering.
-- theorem topSort_is_ordered (g : Graph â„• Î²) (u v : â„•) :
--   g.hasEdge u v â†’ topOrder g u v := by

--   intro (hâ‚ : hasEdge g u v)
--   rw [topOrder]
--   sorry

end TopologicalSort
-/

-------------------------------------------------
-- Example:  Our graphA is acyclic
-- (We should just be able to call 'Topological Sort'
-- on the graph and check if that is successful.)
-------------------------------------------------
-- Put in examples file! (and figure it out later, we don't need
-- it right now)
-- 
-- theorem graphA_is_acyclic : graphA.is_acyclic := by
--   intro (u : â„•) (v : â„•)
--         (path_uv : hasPath graphA u v)
--         (path_vu : hasPath graphA v u)

--   sorry

-------------------------------------------------
-- Activation functions
-------------------------------------------------
def binary_step (x : Float) : Float :=
  if x > 0.0 then
    1.0
  else
    0.0

/-
TODO: If I want to do this the *right* way, I should define
a type of Real numbers that wrap around Floats.  I can *compute*
things at the Float level, and *prove* things at the Reals level,
but I should refuse to let the user *prove* things at the Float
level. i.e. they cannot prove things by evaluating Floats; this
is where Lean's Floats run into contradictions.
-/
axiom le_refl_float : âˆ€ (x : Float), x â‰¤ x
axiom lt_or_ge_float : âˆ€ (x y : Float), x < y âˆ¨ x â‰¥ y
axiom le_not_lt_float : âˆ€ (x y : Float), x â‰¤ y â†’ Â¬ (y < x)
axiom lt_le_lt_float : âˆ€ (x y z : Float), x < y â†’ y â‰¤ z â†’ x < z
axiom zero_le_one_float : 0.0 â‰¤ 1.0

theorem binary_step_is_binary (x : Float) :
    (binary_step x = 0.0) âˆ¨ (binary_step x = 1.0) :=
    by
      -- simp [binary_step]

      cases (lt_or_ge_float 0.0 x) with

      -- Case 1: 0.0 < x
      | inl case1 =>
          have (h : binary_step x = 1.0) :=
            by
              simp only [binary_step]
              rw [(if_pos case1)]
          exact Or.inr h

      -- Case 2: Â¬ (0.0 < x)
      | inr case2 =>
          have (h : binary_step x = 0.0) := 
            by 
              simp only [binary_step]
              rw [(if_neg (le_not_lt_float x 0.0 case2))]
          exact Or.inl h

-- Proof that binary_step is nondecreasing
-- This is also a 'hello world' to see if I can
-- reason about a branching program.
theorem binary_step_nondecr (xâ‚ xâ‚‚ : Float) (hyp : xâ‚ â‰¤ xâ‚‚) :
  (binary_step xâ‚ â‰¤ binary_step xâ‚‚) := 
  by
    -- Simplify by applying the definition of binary_step.
    simp [binary_step]
    
    cases (lt_or_ge_float 0.0 xâ‚) with
    | inl case1 =>
      cases (lt_or_ge_float 0.0 xâ‚‚) with
      | inl case11 => 
          -- Both sides evaluate to 1.0,
          -- so we just prove that 1.0 â‰¤ 1.0.
          rw [(if_pos case1)]
          rw [(if_pos case11)]
          exact le_refl_float 1.0
      | inr case12 => 
          -- We have 0.0 < xâ‚ â‰¤ xâ‚‚ < 0.0,
          -- so this case is absurd. 
          exact absurd
            (lt_le_lt_float 0.0 xâ‚ xâ‚‚ case1 hyp) -- library_search!!! 
            (le_not_lt_float xâ‚‚ 0.0 case12)
    | inr case2 => 
      cases (lt_or_ge_float 0.0 xâ‚‚) with
      | inl case21 => 
          -- We are in the second and first cases.
          rw [(if_neg (le_not_lt_float xâ‚ 0.0 case2))]
          rw [(if_pos case21)]
          exact zero_le_one_float
      | inr case22 => 
          rw [(if_neg (le_not_lt_float xâ‚ 0.0 case2))]
          rw [(if_neg (le_not_lt_float xâ‚‚ 0.0 case22))]
          exact le_refl_float 0.0 -- library_search!!!

-------------------------------------------------
-- Feedforward neural nets
-------------------------------------------------
structure Net where
  graph : Graph â„• Float
  activation : Float â†’ Float
  rate : Float -- learning rate

structure BFNN extends Net where 
  -- The activation function is binary
  binary : âˆ€ (x : Float), 
    (activation x = 0.0) âˆ¨ (activation x = 1.0)

  -- Our graph is acyclic
  acyclic : graph.is_acyclic

  -- The activation function is nondecreasing
  activ_nondecr : âˆ€ (xâ‚ xâ‚‚ : Float),
    xâ‚ â‰¤ xâ‚‚ â†’ activation xâ‚ â‰¤ activation xâ‚‚

  -- There is *some* x for which the activation is 1.0
  activ_pos : âˆƒ (x : Float), activation x = 1.0

-- Put in examples file!  (We don't need to figure it out
-- right now!)
-- def myBFNN : BFNN :=
--   {
--     graph := graphA
--     activation := binary_step
--     rate := 1.0

--     binary := binary_step_is_binary
--     -- sort := (topSortUnsafe graphA).toList.reverse
--     acyclic := sorry -- graphA_is_acyclic
--     activ_nondecr := binary_step_nondecr
--     activ_pos := sorry
--   }

-- inductive Layer (net : BFNN) : List â„• â†’ Prop where
--   | initial_layer : Layer net Nâ‚€
--   | next_layer : âˆ€ (n : â„•), (n âˆˆ N â†’ 
--     âˆƒ (m : â„•), m âˆˆ Náµ¢ âˆ§ Layer net Náµ¢) â†’ Layer net N

variable (net : BFNN)

-------------------------------------------------
-- Playing around with Sets
-------------------------------------------------

def setA : Set â„• :=
  {n | n â‰¤ 10}

def setB : Set â„• :=
  {n âˆˆ setA | n > 5}

def setC : Set â„• :=
  {n | n â‰¤ 5}

#check setA

-- Example proof of a subset, just to make
-- sure I can do it.
example : setB âŠ† setA := by
  intro (n : â„•)
  intro (h : n âˆˆ setB)

  exact show n âˆˆ setA from h.left

-- Another example proof of a subset, this
-- time using the RHS of the set comprehension.
example : setC âŠ† setA := by
  intro (n : â„•)
  intro (hâ‚ : n âˆˆ setC)

  have (hâ‚‚ : n â‰¤ 5) := hâ‚
  have (hâ‚ƒ : 5 â‰¤ 10) := (by native_decide)
  exact show n âˆˆ setA from le_trans hâ‚‚ hâ‚ƒ

-- Prove that a set is contained in its powerset
example : âˆ€ (S : Set Î±), S âˆˆ ğ’« S := by
  intro (S : Set Î±)
  intro (a : Î±)
  intro (h : a âˆˆ S)
  exact h

-- TODO Next: Define graph reachability and propagate
-- Prove that the above BFNN is acyclic, just to make sure
-- we have the right tools for the job.


theorem setExample : 3 âˆˆ setC := by 
  have (hâ‚ : 3 â‰¤ 4) := by native_decide
  constructor
  exact hâ‚

/-â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Forward propagation in a net
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•-/

def weighted_sum (weights : List Float) (lst : List Float) : Float :=
  List.sum [w * x | for w in weights, for x in lst]

#eval weighted_sum [] []
#eval weighted_sum [1.0] [3.0]
#eval weighted_sum [1.0, 2.0, 3.0] [5.0, 5.0, 5.0]

-- Not well-defined behavior (we expect the weights and lst to be of equal size,
-- but this is left implicit.)
#eval weighted_sum [1.0, 2.0] [3.0]

-- WARNING:
-- This is actually FALSE!  For infinite sets, l[i] is not provably
-- in l (as far as I can figure.)
-- TODO: In the future, when making all this computable, I will
-- be using finite sets, and then I can use get instead of get!,
-- and get_mem in the standard library.
axiom get!_mem {Î± : Type} [Inhabited Î±] : 
  âˆ€ (l : List Î±) i, (l.get! i) âˆˆ l

@[simp]
def preds (net : BFNN) (n : â„•): List â„• :=
  net.graph.predecessors n

--------------------------------------------------------------------
theorem edge_from_preds (net : BFNN) (m n : â„•) :
  m âˆˆ preds net n â†” net.graph.hasEdge m n := by
--------------------------------------------------------------------
  simp only [preds, Graph.hasEdge]
  rw [Bool.decide_iff]


-- (Weightless) *minimal* graph distance from node m to n.  Just count
-- the number of edges, i.e. don't apply weights.
-- (just here in order to define layer -- but if there's
--  a better way, I should use it!)
-- def distance (graph : Graph â„• Float) (m n : â„•) : â„• :=
--   sorry

/-
def my_argmax (f : Î± â†’ Î²) (l : List Î±) : Option Î± :=
  l.foldl (argAux fun b c => f c < f b) none
-/

-- The layer of n.
-- We get all predecessors of n, and take the predecessor m
-- with the *maximum* layer.  Then layer(n) = layer(m) + 1.
-- If n has no predecessors, then layer(n) = 0.
-- 
-- TODO: Prove terminating if I can!  (What exactly is decreasing
--       here...)
-- def layer (net : BFNN) (n : â„•) : â„• :=
--   sorry
-- partial

-- TODO: I can do away with this axiom, and define 'layer'
-- more naturally if I define acyclic graphs recursively
-- in the first place!  Then I can do induction on 'net.graph'!
axiom graph_ascending_order : âˆ€ (g : Graph â„• Float) (m n : â„•), 
  m âˆˆ g.predecessors n â†’ m < n

-- Accumulator-style helper function for 'layer'
-- Defined recursively on the *reverse* of the vertex list
-- (this means we are looking at vertices backwards -- each
--  vertex can only "see" the vertices preceding it.)
def layer_acc (net : BFNN) (n : â„•) (ls : List (Vertex â„• Float)) : â„• :=
  match ls with -- net.graph.vertices.reverse
  | [] => 0
  | âŸ¨_, _âŸ© :: rest =>
      let layers := List.map (fun x => layer_acc net x rest) (preds net n)
      let max := layers.maximum
      
      match max with
      | some L => L + 1
      | none => 0

def layer (net : BFNN) (n : â„•) : â„• :=
  layer_acc net n (net.graph.vertices.reverse)

/- -- Put in test file!
-- 0 jumps to 2, 1 jumps to 3, short connection from 1 to 2
def layer_test_graph : Graph â„• Float :=
  âŸ¨[âŸ¨0, [âŸ¨2, 0.5âŸ©]âŸ©, -- âŸ¨4, 0.5âŸ©
    âŸ¨1, [âŸ¨2, 0.5âŸ©, âŸ¨3, 0.5âŸ©]âŸ©, -- âŸ¨4, 0.5âŸ©
    âŸ¨2, []âŸ©, -- âŸ¨4, 0.5âŸ©
    âŸ¨3, [âŸ¨4, 0.5âŸ©]âŸ©, -- remove âŸ¨4, 0.5âŸ©
    
    -- Add a new edge, and toggle which previous edge jumps to it.
    âŸ¨4, []âŸ©]âŸ©

def layer_test_net : BFNN :=
  { graph := layer_test_graph, activation := binary_step, rate := 1.0,
    binary := binary_step_is_binary, acyclic := sorry,
    activ_nondecr := binary_step_nondecr, activ_pos := sorry
  }

#eval layer layer_test_net 0
#eval layer layer_test_net 1
#eval layer layer_test_net 2
#eval layer layer_test_net 3
#eval layer layer_test_net 4
-/

-- AXIOM: We assume the net is fully connected!
-- This is essentially the statement we need, which might
-- follow from being fully connected.
-- TODO: Put this in the definition of BFNN!!!
axiom connected : âˆ€ (net : BFNN) (m n : â„•), 
  layer net m < layer net n â†’ net.graph.hasEdge m n

-- If m is a predecessor of n, then it must be in a previous layer.
-- Proof idea:
-- layer(m)  â‰¤  max({layer(v) | v âˆˆ preds(n)})  <  layer(n)
--------------------------------------------------------------------
lemma preds_decreasing (net : BFNN) (m n : â„•) :
  m âˆˆ preds net n 
  â†’ layer net m < layer net n := by
--------------------------------------------------------------------
  intro hâ‚
  simp only [layer]

  generalize hls : net.graph.vertices.reverse = ls
  induction ls
  case nil =>
    -- This case is impossible;
    -- m âˆˆ preds(n) means that there is *something* in the graph.
    -- This contradicts the fact that the graph is empty!
    have hâ‚‚ : net.graph.vertices = [] := sorry

    simp [preds, Graph.predecessors, Graph.get_vertices] at hâ‚
    rw [hâ‚‚] at hâ‚
    rw [List.map_nil] at hâ‚
    rw [List.filter_nil] at hâ‚
    exact False.elim ((List.mem_nil_iff _).mp hâ‚)

  case cons vertex rest IH =>
    generalize hmax : (List.map (fun x => layer_acc net x rest) (preds net n)).maximum = max
    -- generalize hmax_m : (List.map (fun x => layer_acc net x rest) (preds net m)).maximum = max_m
    -- let max := layers.maximum
    
    simp only [layer_acc]
    match max with
    | none =>
        -- This case is also impossible;
        -- m âˆˆ preds(n) means that there is *some* maximum in preds(n),
        -- which contradicts the fact that the max is empty. 
        rw [hmax]
        conv => rhs; simp

        sorry

    | some L =>
        -- layer(m)  â‰¤  max({layer(v) | v âˆˆ preds(n)})  <  layer(n)
        rw [hmax]
        conv => rhs; simp
        sorry
        
        -- I'm not sure how to proceed...
        

noncomputable
def activ (net : BFNN) (prev_activ : List Float) (n : â„•) : Prop :=
  let preds := preds net n
  let weights := do
    let i <- List.range preds.length
    let m := preds.get! i
    return net.graph.getEdgeWeight m n
  let weight_sum := weighted_sum weights prev_activ
  let curr_activ := net.activation weight_sum
  curr_activ = 1.0


-- FORWARD PROPAGATION IN A NET
-- By recursion on the layers of our feedforward network.
-- (_acc indicates that we are using the layer as an accumulator
--  to do recursion.)
-- 
-- n âˆˆ propagate_acc(S) iff either:
--   Base Case (L=0): n âˆˆ S
--   Rcrs Case (Lâ‰¥0): n âˆˆ S, or the nodes m preceding n activate n.
--     (We check their activation values via propagate_acc on m)
-- 
-- TODO: Make this computable!!!
-- change return type to 'Bool' instead of 'Prop'
-- and change 'Set' to be a finite set
-- and change net.graph to be finite as well!
-- 
-- Then unit-test all this with #eval!
@[simp]
def propagate_acc (net : BFNN) (S : Set â„•) (n : â„•) (L : â„•) : Prop :=
  match L with
  | Nat.zero => n âˆˆ S
  | Nat.succ _ =>
    let preds := preds net n
    let prev_activ := do
      let i <- List.range preds.length
      let m := preds.get! i
      return if propagate_acc net S m (layer net m) then 1.0 else 0.0
    n âˆˆ S âˆ¨ activ net prev_activ n
termination_by propagate_acc net S n L => layer net n
decreasing_by exact preds_decreasing net m n (get!_mem preds i)


-- Set variation of propagate
@[simp]
def propagate (net : BFNN) (S : Set â„•) : Set â„• :=
  fun n => propagate_acc net S n (layer net n)

-------------------------------------------------
-- Some helper lemmas
-- (just to clean up the monstrous proofs ahead!)
-- 
-- TODO: Clean these up with nicer @simp lemmas about
-- propagate and activ
-------------------------------------------------

--------------------------------------------------------------------
lemma simp_propagate_acc (net : BFNN) :
  n âˆ‰ S
  â†’ propagate_acc net S n (Nat.succ L) =
  let preds := preds net n
  let prev_activ := do
    let i <- List.range preds.length
    let m := preds.get! i
    return if propagate_acc net S m (layer net m) then 1.0 else 0.0
  activ net prev_activ n := by
--------------------------------------------------------------------
  intro (hâ‚ : n âˆ‰ S)
  apply Iff.to_eq
  apply Iff.intro

  case mp => 
    intro hâ‚‚
    simp only [propagate_acc]
    simp only [propagate_acc] at hâ‚‚
    
    cases hâ‚‚
    case inl hâ‚ƒ => contradiction
    case inr hâ‚ƒ => exact hâ‚ƒ 
  
  case mpr => 
    intro hâ‚‚
    simp only [propagate_acc]
    simp only [propagate_acc] at hâ‚‚
    exact Or.inr hâ‚‚

-- -- If A and B agree on all the predecessors of n, then they agree on n.
-- -- TODO: We don't seem to need this lemma anymore!
-- lemma activ_agree (net : BFNN) (A B : Set â„•) (n : â„•) :
--   let preds := preds net n
--   (âˆ€ (m : â„•), m âˆˆ preds â†’ (m âˆˆ A â†” m âˆˆ B))
--   â†’ activ net A n
--   â†’ activ net B n := by

-- If A and B agree on all the predecessors of n, then they agree on n.
--------------------------------------------------------------------
-- lemma activ_agree (net : BFNN) (A B : Set â„•) (n : â„•) :
--   let preds := preds net n
--   let prevâ‚ := do
--     let i <- List.range preds.length
--     let m := preds.get! i
--     return if m âˆˆ A then 1.0 else 0.0
--   let prevâ‚‚ := do
--     let i <- List.range preds.length
--     let m := preds.get! i
--     return if m âˆˆ B then 1.0 else 0.0

--   (âˆ€ (m : â„•), m âˆˆ preds â†’ (m âˆˆ A â†” m âˆˆ B))
--   â†’ activ net prevâ‚ n
--   â†’ activ net prevâ‚‚ n := by
-- --------------------------------------------------------------------
--   -- let preds := preds net n
--   intro preds
--   intro prevâ‚
--   intro prevâ‚‚
--   intro (hâ‚ : âˆ€ (m : â„•), m âˆˆ preds â†’ (m âˆˆ A â†” m âˆˆ B))
--   intro (hâ‚‚ : activ net prevâ‚ n)
  
--   simp only [activ]
--   simp only [activ] at hâ‚‚
--   convert â† hâ‚‚ using 7

--   rename_i i
--   let m := preds.get! i
--   have hâ‚ƒ : m âˆˆ preds := get!_mem preds i
--   exact hâ‚ m hâ‚ƒ

-- If A and B agree on all the predecessors of n, then they agree on n.
--------------------------------------------------------------------
-- lemma activ_agree (net : BFNN) (A B : Set â„•) (n : â„•) :
--   (âˆ€ (m : â„•), layer net m â‰¤ layer net n â†’ (m âˆˆ A â†” m âˆˆ B))
  
--   â†’ (let preds := preds net n
--   let prev_activ := do
--     let i <- List.range preds.length
--     let m := preds.get! i
--     return if m âˆˆ A then 1.0 else 0.0
--   activ net prev_activ n)
  
--   â†’ (let preds := preds net n
--   let prev_activ := do
--     let i <- List.range preds.length
--     let m := preds.get! i
--     return if m âˆˆ B then 1.0 else 0.0
--   activ net prev_activ n) := by
-- --------------------------------------------------------------------
--   -- Just go in and subsitute m âˆˆ A for m âˆˆ B.
--   intro (hâ‚ : âˆ€ (m : â„•), layer net m â‰¤ layer net n â†’ (m âˆˆ A â†” m âˆˆ B))
--   intro hâ‚‚
  
--   simp
--   simp at hâ‚‚
--   convert hâ‚‚ using 5
--   rename_i i
--   generalize hm : List.get! (predecessors net.graph n).data i = m
--   -- generalize hLm : layer net m = Lm

--   have hâ‚ƒ : m âˆˆ preds net n := by
--     rw [symm hm]
--     simp [preds]
--     exact get!_mem (predecessors net.graph n).data i
--   have hâ‚„ : layer net m â‰¤ layer net n := by
--     apply le_of_lt
--     exact preds_decreasing net m n hâ‚ƒ
--   exact (symm (hâ‚ m hâ‚„).to_eq).to_iff

-- If A and B agree on all the predecessors of n, 
-- then the corresponding activ's agree on n.
-- lemma activ_agree (net : BFNN) (A B : Set â„•) (n : â„•) :
--   (âˆ€ (m : â„•), layer net m â‰¤ layer net n â†’ (m âˆˆ A â†” m âˆˆ B))
  
--   â†’ (activ net
--       (List.bind (List.range (preds net n).length) fun i =>
--         pure (if propagate_acc net 
--               (fun n => propagate_acc net S n (layer net n)) ((preds net n).get! i)
--                     (layer net ((preds net n).get! i)) 
--               then 1.0 else 0.0)) n)
  
--   â†’ (activ net
--       (List.bind (List.range (List.length (preds net n))) fun i =>
--         pure (if propagate_acc net S ((preds net n).get! i)
--               (layer net ((preds net n).get! i)) 
--               then 1.0 else 0.0)) n) := by
-- --------------------------------------------------------------------
--   intro (hâ‚ : âˆ€ (m : â„•), layer net m â‰¤ layer net n â†’ (m âˆˆ A â†” m âˆˆ B))
--   intro hâ‚‚

--   convert hâ‚‚ using 5
--   rename_i i
--   generalize hm : List.get! (predecessors net.graph n).data i = m
--   sorry
  -- -- Just go in and subsitute m âˆˆ A for m âˆˆ B.
  -- intro (hâ‚ : âˆ€ (m : â„•), layer net m â‰¤ layer net n â†’ (m âˆˆ A â†” m âˆˆ B))
  -- intro hâ‚‚
  
  -- simp
  -- simp at hâ‚‚
  -- convert hâ‚‚ using 5
  -- rename_i i
  -- generalize hm : List.get! (predecessors net.graph n).data i = m
  -- -- generalize hLm : layer net m = Lm

  -- have hâ‚ƒ : m âˆˆ preds net n := by
  --   rw [symm hm]
  --   simp [preds]
  --   exact get!_mem (predecessors net.graph n).data i
  -- have hâ‚„ : layer net m â‰¤ layer net n := by
  --   apply le_of_lt
  --   exact preds_decreasing net m n hâ‚ƒ
  -- exact (symm (hâ‚ m hâ‚„).to_eq).to_iff

/-â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Properties of Propagation
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•-/

--------------------------------------------------------------------
lemma prop_layer_zero (net : BFNN) : âˆ€ (S : Set â„•) (n : â„•),
  layer net n = 0
  â†’ n âˆˆ propagate net S
  â†’ n âˆˆ S := by
--------------------------------------------------------------------
  intro (S : Set â„•) (n : â„•)
        (hL : layer net n = 0)
        (hâ‚ : n âˆˆ propagate net S)

  simp only [propagate, Membership.mem, Set.Mem] at hâ‚
  rw [hL] at hâ‚
  simp only [propagate_acc] at hâ‚
  exact hâ‚

--------------------------------------------------------------------
theorem propagate_is_extens : 
  âˆ€ (S : Set â„•),
  S âŠ† propagate net S := by
--------------------------------------------------------------------
  intro (S : Set â„•)
        (n : â„•) (hâ‚ : n âˆˆ S)
  simp only [Membership.mem, Set.Mem, propagate]

  -- By induction on the layer of the net containing n
  generalize hL : layer net n = L
  induction L

  -- Base Step
  case zero => 
    simp only [propagate_acc]
    exact hâ‚
  
  -- Inductive Step
  case succ k _ => 
    simp only [propagate_acc]
    exact Or.inl hâ‚

-- Corollary: The same statement, but for propagate_acc
-- (this is a helper lemma for the properties that follow.)
-------------------------------------------------
lemma propagate_acc_is_extens :
  âˆ€ (S : Set â„•),
  n âˆˆ S â†’ propagate_acc net S n (layer net n) := by
-------------------------------------------------
  intro (S : Set â„•)
  intro (hâ‚ : n âˆˆ S)
  have hâ‚‚ : n âˆˆ propagate net S := propagate_is_extens _ _ hâ‚
  simp only [Membership.mem, Set.Mem, propagate] at hâ‚‚
  exact hâ‚‚
  

--------------------------------------------------------------------
theorem propagate_is_idempotent : 
  âˆ€ (S : Set â„•),
  propagate net S = 
    propagate net (propagate net S) := by
--------------------------------------------------------------------
  intro (S : Set â„•)
  apply ext
  intro (n : â„•)
  simp only [Membership.mem, Set.Mem, propagate]

  -- By strong induction on the layer of the net containing n
  generalize hL : layer net n = L
  induction L using Nat.case_strong_induction_on generalizing n

  -- Base Step
  case hz =>
    simp only [Membership.mem, Set.Mem, propagate_acc]
    conv in (layer net n) => rw [hL]
    simp only [propagate_acc]
    exact âŸ¨fun x => x, fun x => xâŸ©
  
  -- Inductive Step
  case hi k IH =>
    apply Iff.intro
    
    -- Forward direction is easy, just apply extensive
    case mp =>
      intro hâ‚
      rw [symm hL]
      rw [symm hL] at hâ‚
      exact @propagate_acc_is_extens net _ _ hâ‚

    -- Backwards direction is trickier
    case mpr => 
      intro hâ‚
      
      -- By cases; either n âˆˆ S or n âˆ‰ S
      -- Again; either n âˆˆ propagate S or n âˆ‰ propagate S 
      by_cases n âˆˆ S
      case pos => 
        rw [symm hL]
        exact @propagate_acc_is_extens net _ _ h
      case neg => 
        by_cases propagate_acc net S n (layer net n)
        case pos => 
          rw [symm hL]
          exact h
        case neg =>
          -- Just some simplifications and rewriting definitions
          rename_i n_not_in_S
          rw [simp_propagate_acc net n_not_in_S]
          
          have hâ‚‚ : Â¬n âˆˆ propagate net S := h
          simp [propagate] at hâ‚‚
          rw [simp_propagate_acc net hâ‚‚] at hâ‚
          
          -- -- activ_agrees lemma goes here
          -- TODO: Having lots of trouble with the activ_agrees lemma atm...
          
          simp
          simp at hâ‚
          convert hâ‚ using 5
          rename_i i
          generalize hm : List.get! (net.graph.predecessors n) i = m
          generalize hLm : layer net m = Lm

          -- Apply the inductive hypothesis!
          have hâ‚ƒ : m âˆˆ preds net n := by
            rw [symm hm]
            simp [preds]
            exact get!_mem (net.graph.predecessors n) i
          have hâ‚„ : Lm â‰¤ k := by
            rw [symm hLm]
            apply Nat.lt_succ.mp
            rw [symm hL]
            exact preds_decreasing net m n hâ‚ƒ
          exact IH Lm hâ‚„ m hLm
          


-- This is essentially Hannes' proof.
--------------------------------------------------------------------
theorem propagate_is_cumulative :
  âˆ€ (A B : Set â„•), A âŠ† B
  â†’ B âŠ† propagate net A
  â†’ propagate net A = propagate net B := by
--------------------------------------------------------------------
  intro (A : Set â„•) (B : Set â„•)
        (hâ‚ : A âŠ† B)
        (hâ‚‚ : B âŠ† propagate net A)
  apply ext
  intro (n : â„•)
  simp only [Membership.mem, Set.Mem, propagate]
  simp only [Membership.mem, Set.Mem, propagate] at hâ‚‚

  -- By induction on the layer of the net containing n
  generalize hL : layer net n = L
  induction L using Nat.case_strong_induction_on generalizing n

  -- Base Step
  case hz =>
    simp only [propagate_acc]
    apply Iff.intro
    case mp => exact fun hâ‚ƒ => hâ‚ hâ‚ƒ
    case mpr =>
      intro hâ‚ƒ
      have hâ‚„ : propagate_acc net A n (layer net n) := hâ‚‚ hâ‚ƒ
      conv at hâ‚„ in (layer net n) => rw [hL]
      simp only [propagate_acc] at hâ‚„
      exact hâ‚„

  -- Inductive Step
  case hi k IH => 
    apply Iff.intro

    -- Forward Direction
    case mp => 
      intro hâ‚ƒ

      -- By cases; either n âˆˆ B or n âˆ‰ B.
      -- Similarly, either n âˆˆ A or n âˆ‰ A. 
      by_cases n âˆˆ B
      case pos =>
        rw [symm hL]
        exact @propagate_acc_is_extens net _ _ h -- TODO: replace acc variation
      case neg =>
        by_cases n âˆˆ A
        case pos => 
          rename_i n_not_in_B 
          exact absurd (hâ‚ h) n_not_in_B
        case neg => 
          -- Just some simplifications and rewriting definitions
          rename_i n_not_in_B
          rw [simp_propagate_acc net h] at hâ‚ƒ
          rw [simp_propagate_acc net n_not_in_B]

          -- Just try to prove it and turn it into an 'activ_agree'
          -- lemma later!
          -- Go into 'prev_activ' and substitute using our IH.
          -- Then try to prove what's left.
          simp
          simp at hâ‚ƒ
          convert hâ‚ƒ using 5
          rename_i i
          generalize hm : List.get! (net.graph.predecessors n) i = m
          generalize hLm : layer net m = Lm

          -- Apply the inductive hypothesis!
          have hâ‚ƒ : m âˆˆ preds net n := by
            rw [symm hm]
            simp [preds]
            exact get!_mem (net.graph.predecessors n) i
          have hâ‚„ : Lm â‰¤ k := by 
            rw [symm hLm]
            apply Nat.lt_succ.mp
            rw [symm hL]
            exact preds_decreasing net m n hâ‚ƒ
          exact (symm (IH Lm hâ‚„ m hLm).to_eq).to_iff

    -- Backwards Direction (should be very similar)
    case mpr => 
      intro hâ‚ƒ

      -- By cases; either n âˆˆ A or n âˆ‰ A
      by_cases n âˆˆ A
      case pos => 
        rw [symm hL]
        exact @propagate_acc_is_extens net _ _ h -- TODO: replace acc variation
      case neg => 
        by_cases n âˆˆ B
        case pos => 
          rename_i n_not_in_A
          rw [symm hL]
          exact hâ‚‚ h
        case neg => 
          -- Just some simplifications and rewriting definitions
          rename_i n_not_in_A
          rw [simp_propagate_acc net h] at hâ‚ƒ
          rw [simp_propagate_acc net n_not_in_A]

          -- Just try to prove it and turn it into an 'activ_agree'
          -- lemma later!
          -- Go into 'prev_activ' and substitute using our IH.
          -- Then try to prove what's left.
          simp
          simp at hâ‚ƒ
          convert hâ‚ƒ using 5
          rename_i i
          generalize hm : List.get! (net.graph.predecessors n) i = m
          generalize hLm : layer net m = Lm

          -- Apply the inductive hypothesis!
          have hâ‚ƒ : m âˆˆ preds net n := by
            rw [symm hm]
            simp [preds]
            exact get!_mem (net.graph.predecessors n) i
          have hâ‚„ : Lm â‰¤ k := by 
            rw [symm hLm]
            apply Nat.lt_succ.mp
            rw [symm hL]
            exact preds_decreasing net m n hâ‚ƒ
          exact IH Lm hâ‚„ m hLm


-- #check propagate myBFNN {n : â„• | n â‰¤ 4}
-- #eval propagate myBFNN {n : â„• | n â‰¤ 4}
-- need to make sets finite in order to evaluate???
-- 
-- It's important for everything to be evaluatable, since:
-- 1) I will want to verify that a *specific*
--    neural network has certain properties
-- 2) #eval helps me debug errors


/-â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Conditional Graph Reachability
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•-/
-- reachable(A, B) is the set of all nodes reachable from B
-- using a path where all nodes are inside A (i.e. there is a 
-- focusedPath from B to n).
-- 
-- This is *precisely* what Van Benthem refers to as "conditional
-- common knowledge" (although here we don't need the word "common"
-- because we don't have group dynamics.)  
-- Quote:
-- CG(A, B) "is true in all worlds reachable via some finite path
-- of accessibilities running entirely through worlds satisfying A"
-- [Van Benthem, Belief Revision and Dynamic Logic, page 6]
-- In this paper, he also talks about "pre-encoding" future
-- information in order to get a reduction, which is exactly
-- what we're doing here!
-- 
-- I don't know what the complete axioms are for this conditional
-- knowledge.  But this isn't the main focus here.  I'll just
-- prove a few sound things to give an idea about what it's like.

-- A focused path is a path where every node is contained
-- within the set S.
inductive focusedPath (g : Graph â„• Float) (S : Set â„•) : â„• â†’ â„• â†’ Prop where
  | trivial {u : â„•} :
      u âˆˆ S â†’ focusedPath g S u u
  | from_path {u v w : â„•} : 
      focusedPath g S u v â†’ g.hasEdge v w â†’ w âˆˆ S â†’ focusedPath g S u w

-- focusedPath is transitive
theorem focusedPath_trans {u v w : â„•} (g : Graph â„• Float) (A : Set â„•) :
  focusedPath g A u v â†’ focusedPath g A v w â†’ focusedPath g A u w := by

  intro (hâ‚ : focusedPath g A u v)
  intro (hâ‚‚ : focusedPath g A v w)

  induction hâ‚‚
  case trivial => exact hâ‚
  case from_path x y _ edge_xy hy path_ux => 
    exact focusedPath.from_path path_ux edge_xy hy


-- This is the set of all nodes reachable from B using
-- paths where *every* node in the path is within A
-- (including the initial and final nodes)
def reachable (net : BFNN) (A B : Set â„•) : Set â„• :=
  fun (n : â„•) =>
    âˆƒ (m : â„•), m âˆˆ B âˆ§ focusedPath net.graph A m n

-- Argument: If there is a path from B to n, but n is in
-- layer 0 -- there are *no* incoming nodes -- then the path
-- must be of length 0.  So n must be that n âˆˆ B with
-- a path to n, i.e. n âˆˆ B.
--------------------------------------------------------------------
lemma reach_layer_zero (net : BFNN) : âˆ€ (A B : Set â„•) (n : â„•),
  layer net n = 0
  â†’ n âˆˆ reachable net A B
  â†’ n âˆˆ B := by
--------------------------------------------------------------------
  intro (A : Set â„•)
        (B : Set â„•)
        (n : â„•) (hL : layer net n = 0)
        (hâ‚ : n âˆˆ reachable net A B)
  
  match hâ‚ with
  | âŸ¨m, hâ‚‚âŸ© => 
    -- By induction on the length of the path from B to n.
    --   path length = 0 => m âˆˆ B means n âˆˆ B
    --   path length â‰¥ 0 => this case should be impossible,
    --                      because at layer 0 n has *no predecessors*! 
    induction hâ‚‚.2
    case trivial _ => exact hâ‚‚.1
    case from_path x y _ edge_xy _ _ =>
      -- Contradiction; y's layer is 0, but there is an edge from x to y!
      -- (layer net x < layer net y, but that means layer net x < 0) 
      have hâ‚ƒ : layer net x < layer net y :=
        preds_decreasing net x y ((edge_from_preds _ _ _).mpr edge_xy)
      
      exact absurd hL (Nat.not_eq_zero_of_lt hâ‚ƒ)


--------------------------------------------------------------------
theorem reach_subset (net : BFNN) : âˆ€ (A B : Set â„•),
  reachable net A B âŠ† A := by
--------------------------------------------------------------------
  intro (A : Set â„•)
        (B : Set â„•)
        (n : â„•) (hâ‚ : n âˆˆ reachable net A B)
  
  simp only [Membership.mem, Set.Mem] at hâ‚
  match hâ‚ with
  | âŸ¨m, hmâŸ© => 
    induction hm.2
    case trivial hbase => exact hbase
    case from_path _ y _ _ hy _ => exact hy 


-- This is like propag_is_extens, except we also have to ensure
-- that n âˆˆ A.
--------------------------------------------------------------------
theorem reach_is_extens (net : BFNN) : âˆ€ (A B : Set â„•),
  (A âˆ© B) âŠ† reachable net A B := by
--------------------------------------------------------------------
  intro (A : Set â„•)
        (B : Set â„•)
        (n : â„•) (hâ‚ : n âˆˆ A âˆ© B)

  have (hâ‚‚ : focusedPath net.graph A n n) := 
    focusedPath.trivial hâ‚.1
  exact âŸ¨n, âŸ¨hâ‚.2, hâ‚‚âŸ©âŸ©


--------------------------------------------------------------------
theorem reach_is_idempotent (net : BFNN) : âˆ€ (A B : Set â„•),
  reachable net A B = reachable net A (reachable net A B) := by
--------------------------------------------------------------------
  intro (A : Set â„•)
        (B : Set â„•)
  
  exact Set.ext (fun (n : â„•) =>
    -- âŠ† direction; easy, just apply reach_subset and reach_is_extens
    âŸ¨(fun (hâ‚ : n âˆˆ reachable net A B) => 
      have hâ‚‚ : n âˆˆ A := reach_subset _ _ _ hâ‚
      reach_is_extens _ _ _ âŸ¨hâ‚‚, hâ‚âŸ©),

    -- âŠ‡ direction
    (fun (hâ‚ : n âˆˆ reachable net A (reachable net A B)) =>
      match hâ‚ with
      | âŸ¨x, hâ‚‚âŸ© => 
        match hâ‚‚.1 with
        | âŸ¨m, hâ‚ƒâŸ© =>
          âŸ¨m, âŸ¨hâ‚ƒ.1, focusedPath_trans _ A hâ‚ƒ.2 hâ‚‚.2âŸ©âŸ©)âŸ©)


--------------------------------------------------------------------
theorem reach_is_monotone (net : BFNN) : âˆ€ (S A B : Set â„•),
  A âŠ† B â†’ reachable net S A âŠ† reachable net S B := by
--------------------------------------------------------------------
  intro (S : Set â„•)
        (A : Set â„•)
        (B : Set â„•)
        (hâ‚ : A âŠ† B)
        (n : â„•) (hâ‚‚ : n âˆˆ reachable net S A)
  
  exact match hâ‚‚ with
    | âŸ¨m, hmâŸ© => âŸ¨m, âŸ¨hâ‚ hm.1, hm.2âŸ©âŸ©


/-â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Reach-Prop Interaction Properties
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•-/

-- --------------------------------------------------------------------
-- theorem propagate_reach_inclusion (net : BFNN) : âˆ€ (S : Set â„•),
--   propagate net S âŠ† reachable net S := by
-- --------------------------------------------------------------------
--   sorry

-- --------------------------------------------------------------------
-- lemma minimal_cause_helper (net : BFNN) : âˆ€ (A B : Set â„•), âˆ€ (n : â„•),
--   n âˆˆ reachedby net B
--   â†’ (n âˆˆ propagate net A
--   â†” n âˆˆ propagate net (A âˆ© reachedby net B)) := by
-- --------------------------------------------------------------------
--   intro (A : Set â„•) (B : Set â„•)
--   intro (n : â„•)
--   intro (hâ‚ : n âˆˆ reachedby net B)
--   simp only [Membership.mem, Set.Mem, propagate]

--   -- By induction on the layer of the net containing n
--   generalize hL : layer net n = L
--   induction L using Nat.case_strong_induction_on generalizing n
  
--   -- Base Step
--   case hz => 
--     apply Iff.intro
--     case mp => 
--       intro hâ‚‚
--       simp only [propagate_acc]
--       simp only [propagate_acc] at hâ‚‚
--       exact âŸ¨hâ‚‚, hâ‚âŸ©

--     case mpr => 
--       intro hâ‚‚
--       simp only [propagate_acc]
--       simp only [propagate_acc] at hâ‚‚
--       exact hâ‚‚.1

--   -- Inductive Step
--   case hi k IH => 
--     apply Iff.intro

--     -- Forward Direction
--     case mp => 
--       intro hâ‚‚

--       -- By cases; either n âˆˆ A or not.
--       by_cases n âˆˆ A
--       case pos => 
--         -- This case is trivial (just apply Extens)
--         rw [symm hL]
--         have hâ‚ƒ : n âˆˆ A âˆ© reachedby net B := âŸ¨h, hâ‚âŸ© 
--         exact @propagate_acc_is_extens net _ _ hâ‚ƒ
--       case neg => 
--         -- If n âˆ‰ A, then n âˆ‰ A âˆ© reachedby net B
--         have hâ‚ƒ : n âˆ‰ A âˆ© reachedby net B := 
--           fun n_in_A => absurd n_in_A.1 h
        
--         -- Just some simplifications and rewriting definitions
--         rw [simp_propagate_acc net h] at hâ‚‚
--         rw [simp_propagate_acc net hâ‚ƒ]

--         -- TODO: This is the stuff that should go in the activ_agree lemma!
--         simp
--         simp at hâ‚‚
--         convert hâ‚‚ using 5
--         rename_i i
--         generalize hm : List.get! (predecessors net.graph n).data i = m
--         generalize hLm : layer net m = Lm

--         -- Apply the inductive hypothesis!
--         have hâ‚„ : m âˆˆ preds net n := by
--           rw [symm hm]
--           simp [preds]
--           exact get!_mem (predecessors net.graph n).data i
--         have hâ‚… : Lm â‰¤ k := by
--           rw [symm hLm]
--           apply Nat.lt_succ.mp
--           rw [symm hL]
--           exact preds_decreasing net m n hâ‚„
--         have hâ‚† : m âˆˆ reachedby net B :=
--           match hâ‚ with
--           | âŸ¨x, hxâŸ© => âŸ¨x, âŸ¨hx.1, hasPath_trans _ (preds_path _ hâ‚„) hx.2âŸ©âŸ©
--         exact (symm (IH Lm hâ‚… m hâ‚† hLm).to_eq).to_iff

--     -- Backwards Direction (should be similar)
--     case mpr =>
--       intro hâ‚‚

--       -- By cases; either n âˆˆ A or not.
--       by_cases n âˆˆ A
--       case pos => 
--         -- This case is trivial (just apply Extens)
--         rw [symm hL]
--         exact @propagate_acc_is_extens net _ _ h
--       case neg => 
--         -- If n âˆ‰ A, then n âˆ‰ A âˆ© reachedby net B
--         have hâ‚ƒ : n âˆ‰ A âˆ© reachedby net B := 
--           fun n_in_A => absurd n_in_A.1 h
        
--         -- Just some simplifications and rewriting definitions
--         rw [simp_propagate_acc net hâ‚ƒ] at hâ‚‚
--         rw [simp_propagate_acc net h]

--         -- TODO: This is the stuff that should go in the activ_agree lemma!
--         simp
--         simp at hâ‚‚
--         convert hâ‚‚ using 5
--         rename_i i
--         generalize hm : List.get! (predecessors net.graph n).data i = m
--         generalize hLm : layer net m = Lm

--         -- Apply the inductive hypothesis!
--         have hâ‚„ : m âˆˆ preds net n := by
--           rw [symm hm]
--           simp [preds]
--           exact get!_mem (predecessors net.graph n).data i
--         have hâ‚… : Lm â‰¤ k := by
--           rw [symm hLm]
--           apply Nat.lt_succ.mp
--           rw [symm hL]
--           exact preds_decreasing net m n hâ‚„
--         have hâ‚† : m âˆˆ reachedby net B :=
--           match hâ‚ with
--           | âŸ¨x, hxâŸ© => âŸ¨x, âŸ¨hx.1, hasPath_trans _ (preds_path _ hâ‚„) hx.2âŸ©âŸ©
--         exact IH Lm hâ‚… m hâ‚† hLm


-- -- This is the actual property I want, re-written with conditionals
-- -- in mind
-- --------------------------------------------------------------------
-- theorem minimal_cause (net : BFNN) : âˆ€ (A B : Set â„•),
--   B âŠ† propagate net A
--   â†” B âŠ† propagate net (A âˆ© reachedby net B) := by
-- --------------------------------------------------------------------
--   intro (A : Set â„•) (B : Set â„•)
--   apply Iff.intro
--   case mp => exact fun hâ‚ n hâ‚‚ => (minimal_cause_helper net _ _ _ 
--     (reachedby_is_extens _ _ hâ‚‚)).mp (hâ‚ hâ‚‚)
--   case mpr => exact fun hâ‚ n hâ‚‚ => (minimal_cause_helper net _ _ _ 
--     (reachedby_is_extens _ _ hâ‚‚)).mpr (hâ‚ hâ‚‚)

/-â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Update Policy: "Make neurons fire together"
  (without regard for the edges of the net)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•-/

/-
*Notes*
The basic idea is that we take a subset A of the graph, and
increase our preference for nodes in A as much as possible.
We do this by: 1) completing the graph A, and 2) maximizing all
of the edges within this completed graph.  The overall effect
is that for all neurons m, n âˆˆ A, m fires exactly when n fires.

But this operation results in a graph with cycles -- so [A] Prop(B)
is not well-defined!  But we can do something equivalent:
Take all the nodes in A, and quotient them.  This results in a
net where all the nodes m, n âˆˆ A "fire as one".

So we need:
  [Def] Define 'complete_and_max' update
  [Def] Define 'fire_together' update (the quotient structure)
  [Thm] Prove that the two updates are equivalent.
        (We can't use Prop for this -- maybe I can prove
         that the 'activ' for both nets agrees?)
  [Thm] Find a reduction for 'fire_together' and prove it.
        (It should look like the dual of Lexicographic Upgrade)
  [Cor] 'fire_together' is the dual of Lexicographic Upgrade
    [Depends] 
      Preference Models
      Lexicographic Upgrade
      Lexicographic Upgrade reduction
      Model translation from pref models âŸ· nets
-/

-- def complete_and_max (net : BFNN) (A : Set â„•) : BFNN :=
--   sorry

-- def fire_together (net : BFNN) (A : Set â„•) : BFNN :=
-- { net with
--   graph := sorry
--   activation := sorry
--   rate := sorry

--   -- binary := sorry
--   binary := sorry
--   acyclic := sorry
--   activ_nondecr := sorry
--   activ_pos := sorry
-- }



/-â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Naive (Unstable) Hebbian Update
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•-/

-- A function to map edges in the graph.  
-- We update the edge weight xâ‚ âŸ¶ xâ‚‚, but we also allow information 
-- about the *nodes* xâ‚ and xâ‚‚.
-- Credit to Peter Kementzey for the original mapEdges function.
def map_edges (g : Graph â„• Float) (f : â„• â†’ â„• â†’ Float â†’ Float) : Graph â„• Float := âŸ¨
  g.vertices.map (fun vertex => 
    { vertex with successors := vertex.successors.map (fun 
      âŸ¨target, weightâŸ© => âŸ¨target, f vertex.label target weightâŸ©
  )})
âŸ©

-- For every m âŸ¶ n where m, n âˆˆ Prop(S), increase the weight
-- of that edge by Î· * act(m) * act(n).
noncomputable
def graph_update (net : BFNN) (g : Graph â„• Float) (S : Set â„•) : Graph â„• Float :=
  map_edges g (fun m n weight => 
    let activ_m := if m âˆˆ propagate net S then 1.0 else 0.0
    let activ_n := if n âˆˆ propagate net S then 1.0 else 0.0
    weight + (net.rate * activ_m * activ_n))

-- This graph update does not affect the vertices of the graph.
--------------------------------------------------------------------
lemma graph_update_vertices (net : BFNN) (g : Graph â„• Float) (S : Set â„•) :
  (graph_update net net.graph S).get_vertices = net.graph.get_vertices := by
--------------------------------------------------------------------
  simp only [graph_update, map_edges]
  simp only [Graph.get_vertices]

  -- Go in and compose the maps.  This seems to do the trick.
  conv => lhs; rw [List.map_map]


-- This graph update does not affect the *successor/edge* structure
-- of the graph (it only affects weights!!!)
--------------------------------------------------------------------
lemma graph_update_successors (net : BFNN) (g : Graph â„• Float) (S : Set â„•) (n : â„•) :
  (graph_update net net.graph S).successors n = net.graph.successors n := by
--------------------------------------------------------------------
  simp only [graph_update, map_edges] -- map_edges
  simp only [Graph.successors]

  -- Go in and rewrite the 'unzip' as a 'map'
  rw [List.unzip_eq_map]
  rw [List.unzip_eq_map]
  simp only [Prod.fst]

  -- Now go in and compose the maps.
  -- conv => lhs; rw [List.map_map]
  sorry -- Why the FUCK won't this rewrite work???


-- A single step of Hebbian update.
-- Propagate S through the net, and then increase the weights
-- of all the edges xâ‚ âŸ¶ xâ‚‚ involved in that propagation
-- by Î· * xâ‚ * xâ‚‚.
noncomputable
def hebb (net : BFNN) (S : Set â„•) : BFNN :=
{ net with
  graph := graph_update net net.graph S

  -- We have to ensure that the update doesn't create any cycles
  -- (In this case, we're only changing the weights.)
  acyclic := sorry
}


-- Takes a graph update function 'f' (e.g. graph_update for Hebb)
-- and iterates it 'no_times' times.
-- netáµ¢ and Sáµ¢ are the initial inputs.
def iterate (f : Graph â„• Float â†’ Set â„• â†’ Graph â„• Float) 
  (no_times : â„•) (gáµ¢ : Graph â„• Float) (Sáµ¢ : Set â„•) : Graph â„• Float :=
  match no_times with
  | Nat.zero => gáµ¢
  | Nat.succ k => f (iterate f k gáµ¢ Sáµ¢) Sáµ¢


-- We score neurons by the total sum of *negative* weights coming 
-- into them.  The neuron with the lowest score is the least likely
-- to activate (in the worst case where all of its negative signals
-- activate but none of its positive ones do).  If a neuron has
-- no negative incoming weights, we give it a score of 0.
def neg_weight_score (net : BFNN) (n : â„•) : Float :=
  sorry


-- This is the exact number of iterations of Hebbian learning
-- on 'net' and 'S' that we need to make the network unstable,
-- i.e. any activation involved within Prop(S) simply goes through.
-- 
-- This is the trickiest part to get right -- we actually need
-- to *construct* this number, based on the net's activation
-- function and the edge weights within Prop(S)!
-- 
-- We first pick the neuron with the lowest 'neg_weight_score' n_min.
-- The number of iterations is that number which would (in the worst
-- case) guarantee that n_min gets activated.
-- 
-- If n_score is n_min's score, and X is that point at which
-- our activation function is guranteed to be 1.0, and Î· is the
-- learning rate, then we return
-- 
-- (X - n_score) / Î·   *(I think!)*
def hebb_unstable_point (net : BFNN) (S : Set â„•) : â„• :=
  sorry
  -- let x := choose net.activ_pos
  -- have hâ‚ : net.activation x = 1.0 := sorry

  -- let n_min := @List.minimum (Vertex â„• Float) sorry sorry net.graph.vertices.toList
  -- let n_score := sorry
  -- sorry

-- Iterated hebbian update, up to a certain fixed point.
-- We implement this as a new net, whose graph is
-- 'graph_update' iterated 'hebb_unstable_point'
-- number of times.
-- FUTURE: Consider re-doing this using limits of graphs/categories
noncomputable
def hebb_star (net : BFNN) (S : Set â„•) : BFNN := 
{ net with
  graph := iterate (graph_update net) (hebb_unstable_point net S) net.graph S
  
  -- We have to ensure that the update doesn't create any cycles
  -- (In this case, we're only changing the weights.)
  acyclic := sorry
}



/-â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Properties of Unstable Hebbian Update
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•-/

-- A net netâ‚ is a subnet of netâ‚‚ (netâ‚ â‰¼ netâ‚‚) iff for all
-- sets S, every node activated in the propagation of S
-- in netâ‚ is activated in the propagation of S in netâ‚‚.
-- (They both have the same *propagation structure*)
def subnet (netâ‚ netâ‚‚ : BFNN) : Prop :=
  âˆ€ (S : Set â„•), propagate netâ‚ S âŠ† propagate netâ‚‚ S

infixl:65   " â‰¼ " => subnet


-- Two nets are equivalent if they have the same 
-- *propagation structure* (i.e. if their propagations agree 
-- for every set S)
def net_eq (netâ‚ netâ‚‚ : BFNN) : Prop :=
  netâ‚ â‰¼ netâ‚‚ âˆ§ netâ‚‚ â‰¼ netâ‚

infixl:65   " â‰¡ " => net_eq


-- A super easy example, just to briefly test â‰¼ and â‰¡
example : example_net â‰¡ example_net :=
  âŸ¨fun S n h => h, fun S n h => hâŸ©  


-- propagate_N (S) = propagate_hebb(N, S) (S)
-- This essentially says that repeated hebbian update
-- is well-defined; *after* updating on S, we can update
-- on S again and increase weights within the same propagation.
-- (The propagation of S doesn't suddenly change, which is
--  something we might be worried about.)
-- TODO: Not sure if I need this anymore!
-- It's somewhat interesting, but might not help with the
-- reduction.
-- --------------------------------------------------------------------
-- theorem hebb_iteration_is_well_defined (net : BFNN) (S : Set â„•) : 
--   propagate (hebb net S) S = propagate net S := by
-- --------------------------------------------------------------------
--   apply ext
--   intro (n : â„•)
--   simp only [Membership.mem, Set.Mem, propagate]

--   -- By induction on the layer of the net containing n
--   generalize hL : layer net n = L
--   induction L using Nat.case_strong_induction_on generalizing n

--   -- Base Step
--   case hz =>
--     apply Iff.intro
--     case mp => 
--       simp only [propagate_acc]
--       exact fun x => x
--     case mpr => 
--       simp only [propagate_acc]
--       exact fun x => x

--   -- Inductive Step
--   case hi k IH => 
--     apply Iff.intro

--     -- Forward Direction
--     case mp => 
--       intro hâ‚
--       simp only [propagate_acc] at hâ‚
--       simp only [propagate_acc]

--       cases hâ‚
--       case inl hâ‚‚ => exact Or.inl hâ‚‚
--       case inr hâ‚‚ =>
--         apply Or.inr

--         -- TODO: This is the stuff that should go in the activ_agree lemma!        
--         simp
--         simp at hâ‚‚
--         sorry
--         -- I do not have the tools to show this at this point.
--         -- I need a lemma about activations in the hebbian updated net.

--         -- show_term convert hâ‚‚

--     -- Backwards Direction
--     case mpr => sorry

-- This says that 'hebb_star' is a fixed point of 'hebb'
-- (with respect to â‰¡).  i.e. in the following sense, f(X) = X:
--   hebb(X, S) â‰¡ X,
-- where X = hebb_star net S
-- 
-- I may need additional lemmas (e.g. an activation function
-- within Prop(S) in hebb_star will simply go through.)
-- 
-- One important lemma:  If an edge is not in the propagation of S,
-- its weight is unaffected.
--------------------------------------------------------------------
theorem hebb_star_is_fixed_point (net : BFNN) (S : Set â„•) : 
  hebb (hebb_star net S) S â‰¡ hebb_star net S := by 
--------------------------------------------------------------------
  sorry

/-â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Properties of Single-Iteration Hebbian Update
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•-/

-- A single round of Hebbian update does not affect the predecessors
-- of any node.  To prove this, we just show that Hebbian update
-- does not affect the vertices of the graph, or the successor
-- structure of the graph.
--------------------------------------------------------------------
theorem hebb_once_preds (net : BFNN) (S : Set â„•) (n : â„•) : 
  preds (hebb net S) n = preds net n := by
--------------------------------------------------------------------
  simp only [preds, hebb]
  simp only [Graph.predecessors]
  congr 1

  -- graph_update doesn't affect successors of a node
  case _ => 
    funext v
    congr 2
    exact graph_update_successors _ (net.graph) _ v

  -- graph_update doesn't affect vertices of the graph
  case _ => exact graph_update_vertices _ (net.graph) _

  
-- A single round of Hebbian update hebb does not affect which 
-- neurons are on which layer of the net.
-- TODO: Try induction on the graph, the way I usually prove
-- things about the layers???
--------------------------------------------------------------------
theorem hebb_once_layers (net : BFNN) (S : Set â„•) : 
  layer (hebb net S) n = layer net n := by
--------------------------------------------------------------------
  sorry

-- A single round of Hebbian update hebb does not affect the 
-- activation function.
--------------------------------------------------------------------
theorem hebb_once_activation (net : BFNN) (S : Set â„•) : 
  (hebb net S).activation = net.activation := by 
--------------------------------------------------------------------
  exact rfl

-- A single round of Hebbian update hebb does not affect graph 
-- reachability (It only affects the edge weights)
--------------------------------------------------------------------
theorem hebb_once_reach (net : BFNN) (A B : Set â„•) : 
  reachable (hebb net A) A B = reachable net A B := by 
--------------------------------------------------------------------
  apply ext
  intro (n : â„•)
  -- simp only [reachable]
  
  apply Iff.intro
  case mp => 
    intro hâ‚
    exact match hâ‚ with
    | âŸ¨m, hmâŸ© => by -- âŸ¨m, âŸ¨hm.1, _âŸ©âŸ© 
      induction hm.2
      case trivial => exact âŸ¨m, âŸ¨hm.1, sorryâŸ©âŸ©  
      case from_path x y path_mx edge_xy hy IH =>
        -- Drop the path and edge from (hebb net) to the original net.
        -- Wait -- where do we apply the IH???
        -- have hâ‚‚ : focusedPath (hebb net A).toNet.graph A m x := sorry -- use path_mx
        -- have hâ‚ƒ : Graph.hasEdge net.graph x y = true := sorry -- use edge_xy
        
        have hâ‚‚ : x âˆˆ reachable net A B := sorry -- Apply IH!
        have hâ‚ƒ : Graph.hasEdge net.graph x y = true := sorry
        exact âŸ¨m, âŸ¨hm.1, (focusedPath.from_path sorry hâ‚ƒ hy)âŸ©âŸ© 
  case mpr => sorry


-- This lemma allows us to "lift" equational properties of hebb 
-- to hebb_star.  (This holds *because* hebb_star is the fixed point
-- of hebb.)
--------------------------------------------------------------------
theorem hebb_lift (net : BFNN) (S : Set â„•) (P : BFNN â†’ Î±) : 
  (P (hebb net S) = P net)
  â†’ (P (hebb_star net S) = P net) := by 
--------------------------------------------------------------------
  sorry


/-â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Properties of "Iterated" Naive Hebbian Update
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•-/

-- Hebbian update hebb_star does not affect the predecessors
-- of any node.
-- [LIFTED from hebb_once_preds]
--------------------------------------------------------------------
theorem hebb_preds (net : BFNN) (S : Set â„•) : 
  preds (hebb_star net S) n = preds net n := by
--------------------------------------------------------------------
  exact hebb_lift _ _ (fun x => preds x n) (hebb_once_preds _ _ _)
  
-- Hebbian update hebb_star does not affect which neurons are
-- on which layer of the net.
-- [LIFTED from hebb_once_layers]
--------------------------------------------------------------------
theorem hebb_layers (net : BFNN) (S : Set â„•) : 
  layer (hebb_star net S) n = layer net n := by
--------------------------------------------------------------------
  exact hebb_lift _ _ (fun x => layer x n) (hebb_once_layers _ _)


-- Hebbian update hebb_star does not affect the activation function.
-- [LIFTED from hebb_once_activation]
--------------------------------------------------------------------
theorem hebb_activation (net : BFNN) (S : Set â„•) : 
  (hebb_star net S).activation = net.activation := by 
--------------------------------------------------------------------
  exact hebb_lift _ _ (fun x => x.activation) (hebb_once_activation _ _)


-- Hebbian update hebb_star does not affect graph reachability
-- (It only affects the edge weights)
-- -- [LIFTED from hebb_once_reach]
--------------------------------------------------------------------
theorem hebb_reach (net : BFNN) (A B : Set â„•) : 
  reachable (hebb_star net A) A B = 
    reachable net A B := by 
--------------------------------------------------------------------
  exact hebb_lift _ _ (fun x => reachable x A B) (hebb_once_reach _ _ _)
  
-- Every net N is a subnet of (hebb_star N)
-- (i.e. hebb_star includes the previous propagations)
-- (We had this property in The Logic of Hebbian Learning)
-- TODO: Can we lift this from single-iteration hebb???
--------------------------------------------------------------------
theorem hebb_extensive (net : BFNN) (A : Set â„•) : 
  net â‰¼ (hebb_star net A) := by 
--------------------------------------------------------------------
  intro (B : Set â„•)
  intro (n : â„•)
  intro (hâ‚ : n âˆˆ propagate net B)
  simp only [Membership.mem, Set.Mem, propagate]
  simp only [Membership.mem, Set.Mem, propagate] at hâ‚
  
  -- By induction on the layer of the 
  generalize hL : layer net n = L
  induction L

  --------------------------------
  -- Base Step
  --------------------------------
  case zero => 
    rw [hebb_layers]
    rw [hL] at hâ‚
    rw [hL]
    simp only [propagate_acc]
    simp only [propagate_acc] at hâ‚
    exact hâ‚

  --------------------------------
  -- Inductive Step
  --------------------------------
  case succ k IH => 
    -- By cases, to later simplify propagate_acc
    by_cases n âˆˆ B
    case pos => 
      rw [hebb_layers]
      rw [â† hebb_layers net A]
      exact propagate_acc_is_extens _ _ h  
    
    case neg => 
      -- Do simplifications and rewriting
      rw [hebb_layers]
      rw [hL]
      rw [hL] at hâ‚
      rw [simp_propagate_acc _ h]
      rw [simp_propagate_acc _ h] at hâ‚

      sorry -- need to argue here that 'activ' is *nondecreasing*
            -- i.e. never decreases a weight.


--------------------------------------------------------------------
lemma hebb_acc_is_extens (net : BFNN) (A B : Set â„•) (n : â„•) :
  propagate_acc net B n (layer net n) â†’ 
  propagate_acc (hebb_star net A) B n (layer net n) := by
-------------------------------------------------------------------- 
  intro hâ‚
  have hâ‚‚ : n âˆˆ propagate (hebb_star net A) B := hebb_extensive _ _ _ hâ‚
  simp only [Membership.mem, Set.Mem, propagate] at hâ‚‚
  rw [hebb_layers] at hâ‚‚
  exact hâ‚‚



/-â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  The more interesting/crucial properties
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•-/

-- If n âˆ‰ Prop(A), then the weights in the updated net are the same
-- as the weights in the original net.
-- IDEA: Lift this from single-iteration hebb!
-- Then just go into the definition of hebb.
--------------------------------------------------------------------
theorem hebb_weightsâ‚ (net : BFNN) : 
  n âˆ‰ propagate net A
  â†’ âˆ€ (i : â„•),
    ((hebb_star net A).toNet.graph.getEdgeWeight ((preds net n).get! i) n =
    net.graph.getEdgeWeight ((preds net n).get! i) n) := by
--------------------------------------------------------------------
  intro hâ‚
  intro i
  apply hebb_lift _ _ (fun x => x.toNet.graph.getEdgeWeight ((preds net n).get! i) n)
  simp only [hebb, graph_update]

  sorry


-- If all predecessors of n âˆ‰ Prop(A), then the weights in the 
-- updated net are the same as the weights in the original net.
-- TODO: Can we lift this from single-iteration hebb?
--------------------------------------------------------------------
theorem hebb_weightsâ‚‚ (net : BFNN) : 
  (âˆ€ x, x âˆˆ preds net n â†’ x âˆ‰ propagate net A)
  â†’ âˆ€ (i : â„•),
    ((hebb_star net A).toNet.graph.getEdgeWeight ((preds net n).get! i) n =
    net.graph.getEdgeWeight ((preds net n).get! i) n) := by
--------------------------------------------------------------------
  intro hâ‚
  intro i
  apply hebb_lift _ _ (fun x => x.toNet.graph.getEdgeWeight ((preds net n).get! i) n)
  sorry


-- If n âˆ‰ Prop(A), then activ (hebb_star net A) _ n = activ net _ n.
--------------------------------------------------------------------
theorem hebb_activâ‚ (net : BFNN) (A : Set â„•) (prev_activ : List Float) :
  n âˆ‰ propagate net A
  â†’ activ (hebb_star net A) prev_activ n = activ net prev_activ n := by
--------------------------------------------------------------------
  intro hâ‚
  simp only [activ]
  rw [hebb_activation net A]
  rw [hebb_preds net A]
  conv =>
    lhs; enter [1, 2, 1, 2, i, 1]
    rw [hebb_weightsâ‚ _ hâ‚]


-- If every predecessor of n âˆ‰ Prop(A), then
-- activ (hebb_star net A) _ n = activ net _ n.
--------------------------------------------------------------------
theorem hebb_activâ‚‚ (net : BFNN) (A : Set â„•) (prev_activ : List Float) :
  (âˆ€ x, x âˆˆ preds net n â†’ x âˆ‰ propagate net A)
  â†’ activ (hebb_star net A) prev_activ n = activ net prev_activ n := by
--------------------------------------------------------------------
  intro hâ‚
  simp only [activ]
  rw [hebb_activation net A]
  rw [hebb_preds net A]
  conv =>
    lhs; enter [1, 2, 1, 2, i, 1]
    rw [hebb_weightsâ‚‚ _ hâ‚]


-- -- If *some* predecessor of n is âˆˆ Prop(A), and n âˆˆ Prop(A), then
-- -- if m is activated in (hebb_star net) then n is too
-- -- (the activation automatically goes through in (hebb_star net))
-- --------------------------------------------------------------------
-- theorem hebb_activâ‚ƒ (net : BFNN) :
--   n âˆˆ propagate net A
--   â†’ m âˆˆ preds net n âˆ§ m âˆˆ propagate net A
--   â†’ m âˆˆ propagate (hebb_star net A) B
--   â†’ activ (hebb_star net A) prev_activ m := by
-- --------------------------------------------------------------------
--   sorry



-- If there is a path within Prop(A) from B to n, then, since this
-- path is updated in hebb_star, n âˆˆ Prop(B).
--------------------------------------------------------------------
theorem hebb_updated_path (net : BFNN) (A B : Set â„•) :
  reachable net (propagate net A) (propagate net B)
  âŠ† propagate (hebb_star net A) B := by
--------------------------------------------------------------------
  intro (n : â„•)
  intro hâ‚
  
  -- We have a path from Prop(B) to n, going through Prop(A).
  match hâ‚ with
  | âŸ¨m, hmâŸ© => 

    -- By induction on the length of this path
    induction hm.2
    case trivial path_mm => exact hebb_extensive _ _ _ hm.1
    case from_path v w path_mv edge_vw w_in_propA IH => 
      -- This edge from v to w is key;
      -- Got to go inside hebb_star to see what it's updating.
      sorry


/-â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Reduction for Unstable Hebbian Update
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•-/

-- These two are the big theorems.
-- They explain what hebb_star learns in terms of what the net
-- believed *before* update -- it turns out that we can completely
-- reduce the dynamic behavior to the static behavior.
--------------------------------------------------------------------
theorem hebb_reduction_empty (net : BFNN) (A B : Set â„•) : 
  propagate net A âˆ© propagate net B = âˆ… â†’

  propagate (hebb_star net A) B = propagate net B := by 
--------------------------------------------------------------------
  intro h_empty
  apply ext
  intro (n : â„•)

  -- By induction on the layer of the net containing n
  generalize hL : layer net n = L
  induction L using Nat.case_strong_induction_on generalizing n

  --------------------------------
  -- Base Step
  --------------------------------
  -- Easy; after simplifying, show that B = B.
  case hz =>
    simp only [Membership.mem, Set.Mem, propagate]
    rw [hebb_layers net A]
    rw [hL]
    simp only [propagate_acc]

  --------------------------------
  -- Inductive Step
  --------------------------------
  case hi L IH => 
    -- Backwards direction is easy;
    -- As for forward direction, TODO
    apply Iff.intro
    case mpr => exact fun hâ‚ => hebb_extensive _ _ _ hâ‚
    case mp =>
      -- Split by whether n âˆˆ B, in order to simplify propagate_acc
      by_cases n âˆˆ B
      case pos => exact fun hâ‚ => propagate_is_extens _ _ h
      case neg => 
        intro hâ‚

        -- From here, we split *again*, depending on whether n âˆˆ Prop(A).
        by_cases n âˆˆ propagate net A

        ---------------------
        -- Case 1: n âˆˆ Prop(A)
        ---------------------
        case pos => sorry

        ---------------------
        -- Case 1: n âˆ‰ Prop(A)
        ---------------------
        case neg => sorry


  -- -- Backwards direction is easy;
  -- -- As for forward direction, TODO
  -- apply Iff.intro
  -- case mpr => exact fun hâ‚ => hebb_extensive _ _ _ hâ‚
  -- case mp => 
  --   intro hâ‚
  --   sorry -- need to do induction!!!  (still easier than the big reduction)


--------------------------------------------------------------------
theorem hebb_reduction_nonempty (net : BFNN) (A B : Set â„•) : 
  propagate net A âˆ© propagate net B â‰  âˆ… â†’

  propagate (hebb_star net A) B =
  propagate net (B âˆª reachable net (propagate net A) (propagate net B)) := by 
--------------------------------------------------------------------
  intro h_nonempty
  apply ext
  intro (n : â„•)
  simp only [Membership.mem, Set.Mem, propagate]
  
  -- By induction on the layer of the net containing n
  generalize hL : layer net n = L
  induction L using Nat.case_strong_induction_on generalizing n

  --------------------------------
  -- Base Step
  --------------------------------
  case hz => 
    -- First, do the base case simplifications
    rw [hebb_layers]
    rw [hL]
    simp only [propagate_acc]
    simp only [Union.union, Set.union, Membership.mem, Set.Mem, setOf]

    -- Forward direction is the easy part, so we do it first.
    -- Backwards direction relies on reach_layer_zero,
    -- a fact about paths when we know n is at layer 0.
    apply Iff.intro
    case mp => exact fun hâ‚ => Or.inl hâ‚
    case mpr => 
      intro hâ‚

      -- Either n âˆˆ B or n is reachable from Prop(B) using only
      -- paths within Prop(A).  At layer 0, this means n âˆˆ B.
      cases hâ‚
      case inl hâ‚‚ => exact hâ‚‚
      case inr hâ‚‚ => 
        have heq : layer net n = 0 := 
          Eq.trans (symm (hebb_layers net A)) (Eq.trans (hebb_layers _ _) hL)
        exact prop_layer_zero _ _ _ heq (reach_layer_zero _ _ _ _ heq hâ‚‚)

  --------------------------------
  -- Inductive Step
  --------------------------------
  case hi L IH => 
    apply Iff.intro
    
    ---------------------
    -- Backward Direction
    ---------------------
    case mpr =>
      intro hâ‚
      
      -- By cases; either n âˆˆ B âˆª Reach(Prop(A), Prop(B)), or not.
      by_cases n âˆˆ B âˆª reachable net (propagate net A) (propagate net B)
      case pos =>
        -- From here, we split again; either:
        --    1. n âˆˆ B, and by extens n âˆˆ Prop(B) in (hebb_star net)
        --    2. n âˆˆ Reach(Prop(A), Prop(B)).  In this case, this path
        --       has been updated in the (hebb_star net), so of course
        --       n âˆˆ Prop(B) in (hebb_star_net)
        --       i.e. apply [hebb_updated_path]! 
        cases h
        case inl hâ‚‚ => exact propagate_acc_is_extens _ _ hâ‚‚
        case inr hâ‚‚ =>
          have hâ‚ƒ : n âˆˆ propagate (hebb_star net A) B := 
            hebb_updated_path _ _ _ hâ‚‚
          simp only [propagate, Membership.mem, Set.Mem] at hâ‚ƒ
          exact hâ‚ƒ

      case neg hâ‚‚ =>
        -- From here, we split *again*, depending on whether n âˆˆ Prop(A).
        by_cases n âˆˆ propagate net A 

        ---------------------
        -- Case 1: n âˆˆ Prop(A)
        ---------------------
        case pos => 
          -- Since Prop(A) âˆ© Prop(B) is nonempty, there is an m
          -- in the intersection.
          have hâ‚ƒ : âˆƒ m, m âˆˆ propagate net A âˆ§ m âˆˆ propagate net B :=
            Set.inter_nonempty_iff_exists_left.mp
              (Set.nonempty_iff_ne_empty.mpr h_nonempty)

          match hâ‚ƒ with
          | âŸ¨m, hmâŸ© => 
            -- Moreover, we can assume this m is the *smallest* such
            -- m âˆˆ Prop(A)!  (Via the well-ordering principle)
            have hâ‚„ : âˆ€ x, x âˆˆ propagate net A â†’
              layer net m â‰¤ layer net x := by
              sorry
            
            cases eq_or_lt_of_le (hâ‚„ n h)
            
            ---------------------
            -- Case 1.1: n âˆˆ Prop(A)
            -- and layer[m] < layer[n]
            ---------------------
            -- In this case, since the net is transitively closed
            -- (fully connected), we have an edge from m âˆˆ Prop(A) âˆ© Prop(B)
            -- to n âˆˆ Prop(A).  This gives us n âˆˆ Reach(Prop(A), Prop(B)).
            case inr hâ‚… =>
              -- We just provide the path from mâŸ¶n.
              have hâ‚† : n âˆˆ reachable net (propagate net A) (propagate net B) := by
                exact âŸ¨m, âŸ¨hm.2, 
                  focusedPath.from_path (focusedPath.trivial hm.1) 
                    (connected _ m n hâ‚…) hâŸ©âŸ©

              -- In this case, we conclude that n âˆˆ Prop(B) in
              -- the updated net by 'hebb_updated_path'
              have hâ‚‡ : n âˆˆ propagate (hebb_star net A) B := by
                exact hebb_updated_path _ _ _ hâ‚†
              
              simp only [propagate, Membership.mem, Set.Mem] at hâ‚‡
              exact hâ‚‡

            ---------------------
            -- Case 1.2: n âˆˆ Prop(A)
            -- and layer[m] = layer[n]
            ---------------------
            -- In this case, the activ's are the same, so
            -- we can straightforwardly apply our inductive
            -- hypothesis.
            case inl hâ‚… =>
              -- First, we show that any predecessor of n
              -- cannot be in Prop(A).
              have hâ‚† : âˆ€ x, x âˆˆ preds net n â†’ x âˆ‰ propagate net A := by
                rw [hâ‚…] at hâ‚„
                exact fun x hx x_in_propA => 
                  absurd (hâ‚„ x x_in_propA) (not_le.mpr (preds_decreasing _ _ _ hx))

              -- We get ready to simplify propagate_acc
              rename_i n_not_in_reach
              have n_not_in_B : n âˆ‰ B :=
                fun n_in_B => absurd (Set.mem_union_left _ n_in_B) n_not_in_reach

              -- Simplifications and rewriting, to prepare for IH
              -- We also rewrite the 'preds', 'layers', and 'activ'
              -- for (hebb_star net) in terms of the original 'net'.
              rw [hebb_layers]
              rw [hL]
              simp only [propagate] at n_not_in_reach
              rw [simp_propagate_acc _ n_not_in_B]
              rw [simp_propagate_acc _ n_not_in_reach] at hâ‚
              
              -- TODO: Use hebb_activâ‚‚, which says that if all
              -- of the preds âˆ‰ Prop(A), the activ's are the same.
              rw [hebb_activâ‚‚ _ _ _ hâ‚†] -- rewrite 'activ'
              rw [hebb_preds net A] -- rewrite 'preds'
              conv => -- rewrite 'layers'
                enter [2, 2, i, m, 1]
                rw [hebb_layers net A]
              simp
              simp at hâ‚
              convert hâ‚ using 5
              rename_i i
              generalize hm : List.get! (net.graph.predecessors n) i = m
              generalize hLm : layer net m = Lm
              conv at IH => -- rewrite 'layers' in IH
                enter [L, hL, m, hLm, 1]
                rw [hebb_layers]
                rw [hLm]
              
              -- We are now ready to apply our inductive hypothesis!
              have hâ‚‡ : m âˆˆ preds net n := by
                rw [symm hm]
                simp [preds]
                exact get!_mem (net.graph.predecessors n) i
              have hâ‚ˆ : Lm â‰¤ L := by
                rw [symm hLm]
                apply Nat.lt_succ.mp
                rw [symm hL]
                exact preds_decreasing net m n hâ‚‡
              exact IH Lm hâ‚ˆ m hLm

        ---------------------
        -- Case 2: n âˆ‰ Prop(A)
        ---------------------
        -- In this case, the activ's are the same, so we can
        -- straightforwardly apply our inductive hypothesis.
        case neg =>
          -- We get ready to simplify propagate_acc
          rename_i n_not_in_reach
          have n_not_in_B : n âˆ‰ B :=
            fun n_in_B => absurd (Set.mem_union_left _ n_in_B) n_not_in_reach

          -- Simplifications and rewriting, to prepare for IH
          -- We also rewrite the 'preds', 'layers', and 'activ'
          -- for (hebb_star net) in terms of the original 'net'.
          rw [hebb_layers]
          rw [hL]
          simp only [propagate] at n_not_in_reach
          rw [simp_propagate_acc _ n_not_in_B]
          rw [simp_propagate_acc _ n_not_in_reach] at hâ‚
          
          rw [hebb_activâ‚ _ _ _ h] -- rewrite 'activ'
          rw [hebb_preds net A] -- rewrite 'preds'
          conv => -- rewrite 'layers'
            enter [2, 2, i, m, 1]
            rw [hebb_layers net A]
          simp
          simp at hâ‚
          convert hâ‚ using 5
          rename_i i
          generalize hm : List.get! (net.graph.predecessors n) i = m
          generalize hLm : layer net m = Lm
          conv at IH => -- rewrite 'layers' in IH
            enter [L, hL, m, hLm, 1]
            rw [hebb_layers]
            rw [hLm]
          
          -- We are now ready to apply our inductive hypothesis!
          have hâ‚‚ : m âˆˆ preds net n := by
            rw [symm hm]
            simp [preds]
            exact get!_mem (net.graph.predecessors n) i
          have hâ‚ƒ : Lm â‰¤ L := by
            rw [symm hLm]
            apply Nat.lt_succ.mp
            rw [symm hL]
            exact preds_decreasing net m n hâ‚‚
          exact IH Lm hâ‚ƒ m hLm

    ---------------------
    -- Forward Direction
    -- (similar, but differs slightly in Case 1)
    ---------------------
    -- This direction is a bit more tricky. This
    -- is where we rely on the net being fully
    -- connected & transitively closed.
    case mp => 
      intro hâ‚
      
      -- By cases; either n âˆˆ B âˆª reachable net (propagate net A) B, 
      -- or not.
      by_cases n âˆˆ B âˆª reachable net (propagate net A) (propagate net B)
      case pos => 
        -- In this case, just apply propagate_is_extens
        rw [â† hL]
        simp only [propagate] at h
        exact propagate_acc_is_extens _ _ h

      case neg hâ‚‚ => 
        -- From here, we split *again*, depending on whether n âˆˆ Prop(A).
        by_cases n âˆˆ propagate net A

        ---------------------
        -- Case 1: n âˆˆ Prop(A)
        ---------------------
        case pos => 
          -- Since Prop(A) âˆ© Prop(B) is nonempty, there is an m
          -- in the intersection.
          have hâ‚ƒ : âˆƒ m, m âˆˆ propagate net A âˆ§ m âˆˆ propagate net B :=
            Set.inter_nonempty_iff_exists_left.mp
              (Set.nonempty_iff_ne_empty.mpr h_nonempty)
          
          match hâ‚ƒ with
          | âŸ¨m, hmâŸ© => 
            -- Moreover, this m is the *smallest* such m âˆˆ Prop(A)
            have hâ‚„ : âˆ€ x, x âˆˆ propagate net A â†’
              layer net m â‰¤ layer net x := by
              sorry
            
            cases eq_or_lt_of_le (hâ‚„ n h)
            
            ---------------------
            -- Case 1.1: n âˆˆ Prop(A)
            -- and layer[m] < layer[n]
            ---------------------
            -- In this case, since the net is transitively closed
            -- (fully connected), we have an edge from m âˆˆ Prop(A) âˆ© Prop(B)
            -- to n âˆˆ Prop(A).  This gives us n âˆˆ Reach(Prop(A), Prop(B)).
            case inr hâ‚… =>
              -- Since the net is fully connected, there is an edge mâŸ¶n.
              -- So we just provide the path from mâŸ¶n.
              have hâ‚† : n âˆˆ reachable net (propagate net A) (propagate net B) := by
                exact âŸ¨m, âŸ¨hm.2, 
                  focusedPath.from_path (focusedPath.trivial hm.1) 
                    (connected _ m n hâ‚…) hâŸ©âŸ©

              -- Since n âˆˆ Reach(...),
              -- We conclude that n âˆˆ Prop(B âˆª Reach(...))
              simp only [propagate] at hâ‚†
              rw [â† hL]
              exact propagate_acc_is_extens net _ 
                (Set.mem_union_right _ hâ‚†)

            ---------------------
            -- Case 1.2: n âˆˆ Prop(A)
            -- and layer[m] = layer[n]
            ---------------------
            -- In this case, the activ's are the same, so
            -- we can straightforwardly apply our inductive
            -- hypothesis.
            case inl hâ‚… => 
              -- First, we show that any predecessor of n
              -- cannot be in Prop(A).
              have hâ‚† : âˆ€ x, x âˆˆ preds net n â†’ x âˆ‰ propagate net A := by
                rw [hâ‚…] at hâ‚„
                exact fun x hx x_in_propA => 
                  absurd (hâ‚„ x x_in_propA) (not_le.mpr (preds_decreasing _ _ _ hx))

              -- We get ready to simplify propagate_acc
              rename_i n_not_in_reach
              have n_not_in_B : n âˆ‰ B :=
                fun n_in_B => absurd (Set.mem_union_left _ n_in_B) n_not_in_reach

              -- Simplifications and rewriting, to prepare for IH
              -- We also rewrite the 'preds', 'layers', and 'activ'
              -- for (hebb_star net) in terms of the original 'net'.
              rw [hebb_layers] at hâ‚
              rw [hL] at hâ‚
              simp only [propagate] at n_not_in_reach
              rw [simp_propagate_acc _ n_not_in_B] at hâ‚
              rw [simp_propagate_acc _ n_not_in_reach]
              
              -- TODO: Use hebb_activâ‚‚, which says that if all
              -- of the preds âˆ‰ Prop(A), the activ's are the same.
              rw [hebb_activâ‚‚ _ _ _ hâ‚†] at hâ‚ -- rewrite 'activ'
              rw [hebb_preds net A] at hâ‚ -- rewrite 'preds'
              conv at hâ‚ => -- rewrite 'layers'
                enter [2, 2, i, m, 1]
                rw [hebb_layers net A]
              simp
              simp at hâ‚
              convert hâ‚ using 5
              rename_i i
              generalize hm : List.get! (net.graph.predecessors n) i = m
              generalize hLm : layer net m = Lm
              conv at IH => -- rewrite 'layers' in IH
                enter [L, hL, m, hLm, 1]
                rw [hebb_layers]
                rw [hLm]
              
              -- We are now ready to apply our inductive hypothesis!
              have hâ‚‡ : m âˆˆ preds net n := by
                rw [symm hm]
                simp [preds]
                exact get!_mem (net.graph.predecessors n) i
              have hâ‚ˆ : Lm â‰¤ L := by
                rw [symm hLm]
                apply Nat.lt_succ.mp
                rw [symm hL]
                exact preds_decreasing net m n hâ‚‡
              exact (symm (IH Lm hâ‚ˆ m hLm).to_eq).to_iff
        
        ---------------------
        -- Case 2: n âˆ‰ Prop(A)
        ---------------------
        -- In this case, the activ's are the same, so we can
        -- straightforwardly apply our inductive hypothesis.

        case neg =>
          -- We get ready to simplify propagate_acc
          rename_i n_not_in_reach
          have n_not_in_B : n âˆ‰ B :=
            fun n_in_B => absurd (Set.mem_union_left _ n_in_B) n_not_in_reach

          -- Simplifications and rewriting, to prepare for IH
          -- We also rewrite the 'preds', 'layers', and 'activ'
          -- for (hebb_star net) in terms of the original 'net'.
          rw [hebb_layers] at hâ‚
          rw [hL] at hâ‚
          simp only [propagate] at n_not_in_reach
          rw [simp_propagate_acc _ n_not_in_B] at hâ‚
          rw [simp_propagate_acc _ n_not_in_reach]
          
          rw [hebb_activâ‚ _ _ _ h] at hâ‚ -- rewrite 'activ'
          rw [hebb_preds net A] at hâ‚ -- rewrite 'preds'
          conv at hâ‚ => -- rewrite 'layers'
            enter [2, 2, i, m, 1]
            rw [hebb_layers net A]
          simp
          simp at hâ‚
          convert hâ‚ using 5
          rename_i i
          generalize hm : List.get! (net.graph.predecessors n) i = m
          generalize hLm : layer net m = Lm
          conv at IH => -- rewrite 'layers' in IH
            enter [L, hL, m, hLm, 1]
            rw [hebb_layers]
            rw [hLm]

          -- We are now ready to apply our inductive hypothesis!
          have hâ‚‚ : m âˆˆ preds net n := by
            rw [symm hm]
            simp [preds]
            exact get!_mem (net.graph.predecessors n) i
          have hâ‚ƒ : Lm â‰¤ L := by
            rw [symm hLm]
            apply Nat.lt_succ.mp
            rw [symm hL]
            exact preds_decreasing net m n hâ‚‚
          exact (symm (IH Lm hâ‚ƒ m hLm).to_eq).to_iff
        

-- TODO: Prove that we can unravel these nets into ordinary
-- feedforward nets
-- 
-- TODO: Email David Sprunger about follow-up papers to
-- "backprop as a functor"

/-â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  The Logic (Language and Semantics)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•-/



/-â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Inference Rules and Proof System
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•-/




/-â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Soundness
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•-/



