import Mathlib.Tactic.LibrarySearch
import Mathlib.Tactic.NthRewrite
import Mathlib.Mathport.Syntax
import Std.Tactic.ShowTerm
import Lean.Meta.Tactic.Simp.Main
import Mathlib.Tactic.Basic
import Mathlib.Data.List.Sigma

import Lean.Parser.Tactic
-- import Graph.Graph
-- import Graph.TopologicalSort
import Mathlib.Init.Set
import Mathlib.Data.List.Defs
import Mathlib.Init.Propext
import Mathlib.Data.Set.Basic
import Mathlib.Logic.Basic
import Mathlib.Logic.Function.Basic

-- open Graph
open Set
open Tactic
open Classical

-- Doesn't detect inefficient code!
set_option maxHeartbeats 0

-------------------------------------------------
-- Goofing about with inductive types
-------------------------------------------------

inductive my_lte : ‚Ñï ‚Üí ‚Ñï ‚Üí Prop where
  | reflexive : my_lte n n
  | from_succ : my_lte m x ‚Üí (n = x + 1) ‚Üí my_lte m n

-- #eval my_lte 1 3





-------------------------------------------------
-- List comprehensions,
-- courtesy of lovettchris
-- See: 
--   https://github.com/leanprover/lean4-samples/blob/main/ListComprehension/ListComprehension.lean
-------------------------------------------------

declare_syntax_cat compClause
syntax "for " term " in " term : compClause
syntax "if " term : compClause

syntax "[" term " | " compClause,* "]" : term

def List.map' (xs : List Œ±) (f : Œ± ‚Üí Œ≤) : List Œ≤ := List.map f xs

macro_rules
  | `([$t:term |]) => `([$t])
  | `([$t:term | for $x in $xs]) => `(List.map' $xs  (Œª $x => $t))
  | `([$t:term | if $x]) => `(if $x then [$t] else [])
  | `([$t:term | $c, $cs,*]) => `(List.join [[$t | $cs,*] | $c])

def prod_comprehens (xs : List Œ±) (ys : List Œ≤) : List (Œ± √ó Œ≤) := 
  [(x, y) | for x in xs, for y in ys]

#eval [(x, y) | for x in [1, 2], for y in [3, 4]]


-------------------------------------------------
-- Weighted Directed Graphs
-- 
-- Since graphs are internally represented by lists,
-- we can just do induction on this inner list.
-- 
-- It's common for graph theory proofs to go by
-- induction on the graph:  "Suppose P holds for the
-- subgraph, and now add a new node."  In this case
-- what you probably want is to do induction on
-- *Graph.vertices.reverse*, so that each new node
-- can only "see" those nodes that come *before* it.
-- 
-- NOTE: For hygiene, we expect that the vertices
-- are natural numbers given in order, i.e. 0, 1, 2, ...
-- In principle, you can pick any other labels for your 
-- vertices, but I will be relying on the fact that they come
-- in this order.  My apologies.
-- 
-- WARNING: We will also assume that graphs are acyclic.
-- But NOTHING in this file directly enforces that.
-- Whenever I construct a new graph, I will check that
-- it preserves acyclic-ness.  But when making a graph
-- from scratch, kindly refrain from creating cycles.
-- 
-- These graphs are based on Peter Kementzey's 
-- implementation, but modified slightly.
-------------------------------------------------

-- Œ± is the type of the nodes
-- Œ≤ is the type of the weights
structure Vertex (Œ± Œ≤ : Type) where
  label : Œ± 
  successors : List (‚Ñï √ó Œ≤)
deriving Repr

instance [Inhabited Œ±] : Inhabited (Vertex Œ± Œ≤) := 
  ‚ü® { label := default, successors := default } ‚ü©

structure Graph (Œ± : Type) (Œ≤ : Type) where
  vertices : List (Vertex Œ± Œ≤) := []
deriving Repr

-- Notice that this graph is acyclic, since each predecessor
-- list only refers to nodes above the current node.  This
-- is foreshadowing.
def graphA : Graph ‚Ñï Float :=
  ‚ü®[‚ü®0, [‚ü®1, 0.5‚ü©, ‚ü®2, 0.6‚ü©, ‚ü®3, 0.7‚ü©]‚ü©, 
    ‚ü®1, [‚ü®2, 0.8‚ü©, ‚ü®3, 0.9‚ü©]‚ü©, 
    ‚ü®2, [‚ü®3, 1.0‚ü©, ‚ü®3, 3.0‚ü©]‚ü©, 
    ‚ü®3, []‚ü©]‚ü©

#check graphA
#eval graphA

-------------------------------------------------
-- Graph functions
-------------------------------------------------

-- TODO: for convenience, define 'map', 'foldl', 'filter',
-- 'find?', 'zip', 'some', 'any', and 'all' over graph vertices.

-- TODO: Make graph functions computable! (this shouldn't be
-- hard -- right now it just depends on 'Prop')
namespace Graph

def get_vertices (g : Graph ‚Ñï Float) : List ‚Ñï :=
  List.map (fun ‚ü®label, _‚ü© => label) g.vertices

-- Helper function to collect the List of pairs of n's successors
def successor_pairs (vertices : List (Vertex ‚Ñï Float)) (n : ‚Ñï) : List (‚Ñï √ó Float) :=
  match vertices with
  | [] => []
  | ‚ü®vertex, succ‚ü© :: rest => 
    if vertex = n then succ 
    else successor_pairs rest n

-- We get all vertices that are in n's successor list
def successors (g : Graph ‚Ñï Float) (n : ‚Ñï) : List ‚Ñï :=
  List.filter 
    (fun m => m ‚àà (successor_pairs g.vertices n).unzip.1) 
    g.get_vertices

  -- List.get n g.vertices -- successors.unzip.1
  -- g.vertices[n]!.successors.unzip.1


def predecessors (g : Graph ‚Ñï Float) (n : ‚Ñï) : List ‚Ñï :=
  List.filter (fun v => n ‚àà (g.successors v)) g.get_vertices
  
  -- List.map (fun ‚ü®label, _‚ü© => label) 
  --   (List.filter (fun v => n ‚àà (g.successors v)) g.vertices)

  -- List.filter 
  --   (fun m => n ‚àà (g.successors m)) 
  --   g.get_vertices

-- Using 'predecessors' is slower than 'successors',
-- but we will tend to look backwards from a node rather
-- than forwards.
def hasEdge (g : Graph ‚Ñï Float) (m n : ‚Ñï) : Bool :=
  m ‚àà (g.predecessors n)

-- Returns the weight of the edge m ‚ü∂ n, if it exists.
-- If it does not exist, we say the weight is 0.0
-- TODO: In the future, it's better to use Option here
-- and return none if none!!!
def getEdgeWeight (g : Graph ‚Ñï Float) (m n : ‚Ñï) : Float :=
  match g.vertices[m]!.successors.find? (fun ‚ü®label, _‚ü© => label = n) with
  | some ‚ü®_, weight‚ü© => weight
  | none => 0.0

-- Some sanity checks
#eval get_vertices graphA
#eval successors graphA 0
#eval successors graphA 1
#eval successors graphA 2
#eval successors graphA 3
#eval predecessors graphA 0
#eval predecessors graphA 1
#eval predecessors graphA 2
#eval predecessors graphA 3
#eval hasEdge graphA 1 2
#eval hasEdge graphA 1 3
#eval hasEdge graphA 4 2
#eval getEdgeWeight graphA 1 2
#eval getEdgeWeight graphA 1 3
#eval getEdgeWeight graphA 4 2


inductive hasPath (g : Graph ‚Ñï Float) : ‚Ñï ‚Üí ‚Ñï ‚Üí Prop where
  | trivial {u : ‚Ñï} :
      hasPath g u u
  | from_path {u v w : ‚Ñï} : 
      hasPath g u v ‚Üí hasEdge g v w ‚Üí hasPath g u w

/-
TODO for later:  Make 'hasPath' computable so that we can execute
this code:
> #eval hasPath graphA 1 3

Some old code when I was trying to do this:

instance decPath : Decidable (hasPath g u v) :=
  sorry -- this should implement BFS!!!
  -- if h : u = v then
  --   isTrue (Eq.subst h hasPath.trivial)
  -- else if h : hasEdge g u v then
  --   isTrue (hasPath.from_path (hasPath.trivial) h)
  -- else
  --   sorry

instance decLte : Decidable (my_lte m n) :=
  if h : m = n then
    .isTrue (h ‚ñ∏ .trivial)
  else
    match n with
    | x + 1 =>
      have := @decLte m x
      decidable_of_iff (my_lte m x) ‚ü®(.from_path ¬∑ rfl), fun h => by
        cases h with
        | trivial => cases h rfl
        | from_path h e => exact Nat.succ.inj e ‚ñ∏ h‚ü©
    | 0 => .isFalse fun h => by
      cases h with
      | trivial => exact h rfl
      | from_path h e => cases e
-/


--------------------------------------------------------------------
theorem hasPath_trans {u v w : ‚Ñï} (g : Graph ‚Ñï Float) :
  hasPath g u v ‚Üí hasPath g v w ‚Üí hasPath g u w := by
--------------------------------------------------------------------
  intro (h‚ÇÅ : hasPath g u v)
  intro (h‚ÇÇ : hasPath g v w)

  induction h‚ÇÇ
  case trivial => exact h‚ÇÅ
  case from_path x y _ edge_xy path_ux => 
    exact hasPath.from_path path_ux edge_xy


def is_refl (g : Graph ‚Ñï Float) : Prop := ‚àÄ (u : ‚Ñï), 
  u ‚àà g.get_vertices ‚Üí g.hasEdge u u

def is_symm (g : Graph ‚Ñï Float) : Prop := ‚àÄ (u v : ‚Ñï), 
  g.hasEdge u v ‚Üí g.hasEdge v u

def is_trans (g : Graph ‚Ñï Float) : Prop := ‚àÄ (u v w : ‚Ñï),
  g.hasEdge u v ‚Üí g.hasEdge v w ‚Üí g.hasEdge u w

def is_acyclic (g : Graph ‚Ñï Float) : Prop := ‚àÄ (u v : ‚Ñï),
  g.hasPath u v ‚Üí g.hasPath v u ‚Üí u = v

end Graph

/-
TODO:  Define 'acyclic' as:  Recursively on graph.vertices,
every vertex can only "see" the vertices ahead of it.

TODO: We want to be able to check if a graph is acyclic by
just "computing" it -- i.e. we call Topological Sort on the
graph, and if successful we know it is acyclic.

So here is some old code I was using to try to do topological
sort.  I'll need to come back to this when I want to make
everything in this library computable.
namespace TopologicalSort

-- @[simp]
-- def topol_sort (g : Graph ‚Ñï Float) :=
--   (topSortUnsafe g).toList.reverse

-- holds iff u precedes v in array
-- note that we assume lst elements are all distinct
def list_precedes (lst : List ‚Ñï) (u v : ‚Ñï) : Bool :=
  match lst with
    | List.nil => false
    | List.cons x xs =>
      -- If we find 'u' first, and v is in the rest, true
      if x = u ‚àß v ‚àà xs then 
        true
      else 
        list_precedes xs u v

def listA : List ‚Ñï :=
  [2, 4, 9, 8, 5]

-- a couple of unit tests for good measure
#eval list_precedes listA 4 8 -- true
#eval list_precedes listA 2 8 -- true
#eval list_precedes listA 2 4 -- true
#eval list_precedes listA 2 9 -- true
#eval list_precedes listA 9 5 -- true

#eval list_precedes listA 8 2 -- should be false, is true
#eval list_precedes listA 5 9 -- should be false, is true

#eval list_precedes listA 1 7 -- undefined (false)
#eval list_precedes listA 9 9 -- false, makes sure an element
                              -- does not precede itself.

-- The ordering induced by Topological Sort
-- TODO: Rewrite as an inductive data type!
/-
def topOrder (g : Graph ‚Ñï Œ≤) (u v : ‚Ñï) : Prop :=
  match (topSort g) with
    | some sorted => list_precedes sorted.toList u v
    | none => sorry
-/

-- inductive TopologicalOrdering (g : Graph ‚Ñï Œ≤) (u : ‚Ñï) where
--   | constr1 : TopologicalOrdering g u
--   | constr2 (x : ‚Ñï) : TopologicalOrdering g u

-- inductive graph_‚â∫ (g : Graph ‚Ñï Œ≤) (u v : ‚Ñï) where
--   | constr1 : sorry
--   | constr2 : sorry

-- Says that Topological Sort is actually correct, i.e.
-- if there is an edge from x to y, then x ‚â∫ y in the ordering.
-- theorem topSort_is_ordered (g : Graph ‚Ñï Œ≤) (u v : ‚Ñï) :
--   g.hasEdge u v ‚Üí topOrder g u v := by

--   intro (h‚ÇÅ : hasEdge g u v)
--   rw [topOrder]
--   sorry

end TopologicalSort
-/

-------------------------------------------------
-- Example:  Our graphA is acyclic
-- (We should just be able to call 'Topological Sort'
-- on the graph and check if that is successful.)
-------------------------------------------------
-- Put in examples file! (and figure it out later, we don't need
-- it right now)
-- 
-- theorem graphA_is_acyclic : graphA.is_acyclic := by
--   intro (u : ‚Ñï) (v : ‚Ñï)
--         (path_uv : hasPath graphA u v)
--         (path_vu : hasPath graphA v u)

--   sorry

-------------------------------------------------
-- Activation functions
-------------------------------------------------
def binary_step (x : Float) : Float :=
  if x > 0.0 then
    1.0
  else
    0.0

/-
TODO: If I want to do this the *right* way, I should define
a type of Real numbers that wrap around Floats.  I can *compute*
things at the Float level, and *prove* things at the Reals level,
but I should refuse to let the user *prove* things at the Float
level. i.e. they cannot prove things by evaluating Floats; this
is where Lean's Floats run into contradictions.
-/
axiom le_refl_float : ‚àÄ (x : Float), x ‚â§ x
axiom lt_or_ge_float : ‚àÄ (x y : Float), x < y ‚à® x ‚â• y
axiom le_not_lt_float : ‚àÄ (x y : Float), x ‚â§ y ‚Üí ¬¨ (y < x)
axiom lt_le_lt_float : ‚àÄ (x y z : Float), x < y ‚Üí y ‚â§ z ‚Üí x < z
axiom le_eq_le_float : ‚àÄ (x y z : Float), x ‚â§ y ‚Üí y = z ‚Üí x ‚â§ z
axiom eq_le_le_float : ‚àÄ (x y z : Float), x = y ‚Üí y ‚â§ z ‚Üí x ‚â§ z
axiom zero_lt_one_float : 0.0 < 1.0
axiom zero_le_zero : 0.0 ‚â§ 0.0
axiom le_of_lt_float : ‚àÄ (x y : Float), x < y ‚Üí x ‚â§ y
axiom not_le_float : ‚àÄ (x y : Float), x < y ‚Üí ¬¨ (y ‚â§ x)
axiom zero_mult : ‚àÄ (x : Float), x * 0.0 = 0.0
axiom one_mult : ‚àÄ (x : Float), x * 1.0 = x
axiom zero_plus : ‚àÄ (x : Float), x + 0.0 = x
axiom commutative_mult_float : ‚àÄ (x y : Float), x * y = y * x

-- This is probably the worst offender on this list.
-- We assume that we have an algorithm for casting a
-- natural number as a Float.
-- (Lean is especially bad about casting to and from Floats).
-- TODO: When changing everything to 'Real's, I should actually
-- give the cast to 'Real' here instead.

-- Positive natural numbers
axiom cast_float (n : ‚Ñï) : Float
axiom cast_float_zero : cast_float 0 = 0.0 

--------------------------------------------------------------------
theorem binary_step_is_binary (x : Float) :
    (binary_step x = 0.0) ‚à® (binary_step x = 1.0) := by
--------------------------------------------------------------------
      -- simp [binary_step]

      cases (lt_or_ge_float 0.0 x) with

      -- Case 1: 0.0 < x
      | inl case1 =>
          have (h : binary_step x = 1.0) :=
            by
              simp only [binary_step]
              rw [(if_pos case1)]
          exact Or.inr h

      -- Case 2: ¬¨ (0.0 < x)
      | inr case2 =>
          have (h : binary_step x = 0.0) := 
            by 
              simp only [binary_step]
              rw [(if_neg (le_not_lt_float x 0.0 case2))]
          exact Or.inl h

-- Proof that binary_step is nondecreasing
-- This is also a 'hello world' to see if I can
-- reason about a branching program.
--------------------------------------------------------------------
theorem binary_step_nondecr (x‚ÇÅ x‚ÇÇ : Float) (hyp : x‚ÇÅ ‚â§ x‚ÇÇ) :
  (binary_step x‚ÇÅ ‚â§ binary_step x‚ÇÇ) := by
--------------------------------------------------------------------
    -- Simplify by applying the definition of binary_step.
    simp [binary_step]
    
    cases (lt_or_ge_float 0.0 x‚ÇÅ) with
    | inl case1 =>
      cases (lt_or_ge_float 0.0 x‚ÇÇ) with
      | inl case11 => 
          -- Both sides evaluate to 1.0,
          -- so we just prove that 1.0 ‚â§ 1.0.
          rw [(if_pos case1)]
          rw [(if_pos case11)]
          exact le_refl_float 1.0
      | inr case12 => 
          -- We have 0.0 < x‚ÇÅ ‚â§ x‚ÇÇ < 0.0,
          -- so this case is absurd. 
          exact absurd
            (lt_le_lt_float 0.0 x‚ÇÅ x‚ÇÇ case1 hyp) -- library_search!!! 
            (le_not_lt_float x‚ÇÇ 0.0 case12)
    | inr case2 => 
      cases (lt_or_ge_float 0.0 x‚ÇÇ) with
      | inl case21 => 
          -- We are in the second and first cases.
          rw [(if_neg (le_not_lt_float x‚ÇÅ 0.0 case2))]
          rw [(if_pos case21)]
          exact (le_of_lt_float _ _ zero_lt_one_float)
      | inr case22 => 
          rw [(if_neg (le_not_lt_float x‚ÇÅ 0.0 case2))]
          rw [(if_neg (le_not_lt_float x‚ÇÇ 0.0 case22))]
          exact le_refl_float 0.0 -- library_search!!!

-------------------------------------------------
-- Feedforward neural nets
-------------------------------------------------
structure Net where
  graph : Graph ‚Ñï Float
  activation : Float ‚Üí Float
  rate : Float -- learning rate

structure BFNN extends Net where 
  -- The activation function is binary
  binary : ‚àÄ (x : Float), 
    (activation x = 0.0) ‚à® (activation x = 1.0)

  -- The activation function is nondecreasing
  activ_nondecr : ‚àÄ (x‚ÇÅ x‚ÇÇ : Float),
    x‚ÇÅ ‚â§ x‚ÇÇ ‚Üí activation x‚ÇÅ ‚â§ activation x‚ÇÇ

  -- There is *some* x for which the activation is 1.0
  activ_pos : ‚àÉ (x : Float), activation x = 1.0


-- Because our activation function is bounded above by 1.0,
-- if act(x‚ÇÅ) = 1.0
-- then any act(x‚ÇÇ) greater than act(x‚ÇÅ) also = 1.0
--------------------------------------------------------------------
lemma activation_from_inequality (net : BFNN) (x‚ÇÅ x‚ÇÇ : Float) :
  net.activation x‚ÇÅ ‚â§ net.activation x‚ÇÇ
  ‚Üí net.activation x‚ÇÅ = 1.0 ‚Üí net.activation x‚ÇÇ = 1.0 := by
--------------------------------------------------------------------
  intro h‚ÇÅ h‚ÇÇ
  cases net.binary x‚ÇÇ
  case inr actx‚ÇÇ_is_one => exact actx‚ÇÇ_is_one
  case inl actx‚ÇÇ_is_zero => 
    -- This case is impossible! 1 is not ‚â§ 0!
    rw [h‚ÇÇ] at h‚ÇÅ
    rw [actx‚ÇÇ_is_zero] at h‚ÇÅ
    exact absurd h‚ÇÅ (not_le_float _ _ zero_lt_one_float)

-- Put in examples file!  (We don't need to figure it out
-- right now!)
-- def myBFNN : BFNN :=
--   {
--     graph := graphA
--     activation := binary_step
--     rate := 1.0

--     binary := binary_step_is_binary
--     -- sort := (topSortUnsafe graphA).toList.reverse
--     acyclic := sorry -- graphA_is_acyclic
--     activ_nondecr := binary_step_nondecr
--     activ_pos := sorry
--   }

-- inductive Layer (net : BFNN) : List ‚Ñï ‚Üí Prop where
--   | initial_layer : Layer net N‚ÇÄ
--   | next_layer : ‚àÄ (n : ‚Ñï), (n ‚àà N ‚Üí 
--     ‚àÉ (m : ‚Ñï), m ‚àà N·µ¢ ‚àß Layer net N·µ¢) ‚Üí Layer net N

variable (net : BFNN)

-------------------------------------------------
-- Playing around with Sets
-------------------------------------------------

def setA : Set ‚Ñï :=
  {n | n ‚â§ 10}

def setB : Set ‚Ñï :=
  {n ‚àà setA | n > 5}

def setC : Set ‚Ñï :=
  {n | n ‚â§ 5}

#check setA

-- Example proof of a subset, just to make
-- sure I can do it.
example : setB ‚äÜ setA := by
  intro (n : ‚Ñï)
  intro (h : n ‚àà setB)

  exact show n ‚àà setA from h.left

-- Another example proof of a subset, this
-- time using the RHS of the set comprehension.
example : setC ‚äÜ setA := by
  intro (n : ‚Ñï)
  intro (h‚ÇÅ : n ‚àà setC)

  have (h‚ÇÇ : n ‚â§ 5) := h‚ÇÅ
  have (h‚ÇÉ : 5 ‚â§ 10) := (by native_decide)
  exact show n ‚àà setA from le_trans h‚ÇÇ h‚ÇÉ

-- Prove that a set is contained in its powerset
example : ‚àÄ (S : Set Œ±), S ‚àà ùí´ S := by
  intro (S : Set Œ±)
  intro (a : Œ±)
  intro (h : a ‚àà S)
  exact h

-- TODO Next: Define graph reachability and propagate
-- Prove that the above BFNN is acyclic, just to make sure
-- we have the right tools for the job.


theorem setExample : 3 ‚àà setC := by 
  have (h‚ÇÅ : 3 ‚â§ 4) := by native_decide
  constructor
  exact h‚ÇÅ

/-‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  Forward propagation in a net
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê-/

-- I would like to write 
--     List.sum [w * x | for w in weights, for x in lst],
-- but list comprehensions are harder to reason about.
def weighted_sum (weights : List Float) (lst : List Float) : Float :=
  List.foldr (¬∑ + ¬∑) 0.0 (List.zipWith (¬∑ * ¬∑) weights lst)
  -- List.sum [w * x | for w in weights, for x in lst]
  -- 

#eval weighted_sum [] []
#eval weighted_sum [1.0] [3.0]
#eval weighted_sum [1.0, 2.0, 3.0] [5.0, 5.0, 5.0]

-- Not well-defined behavior (we expect the weights and lst to be of equal size,
-- but this is left implicit.)
#eval weighted_sum [1.0, 2.0] [3.0]

--------------------------------------------------------------------
lemma weighted_sum_eq (fw‚ÇÅ fw‚ÇÇ fx‚ÇÅ fx‚ÇÇ : ‚Ñï ‚Üí Float) (ls : List ‚Ñï) :
  let x‚ÇÅ : List Float := List.map (fun i => fx‚ÇÅ i) (List.range ls.length)
  let x‚ÇÇ : List Float := List.map (fun i => fx‚ÇÇ i) (List.range ls.length)
  let w‚ÇÅ : List Float := List.map (fun i => fw‚ÇÅ i) (List.range ls.length)
  let w‚ÇÇ : List Float := List.map (fun i => fw‚ÇÇ i) (List.range ls.length)
  
  (‚àÄ i, (fw‚ÇÅ i) * (fx‚ÇÅ i) = (fw‚ÇÇ i) * (fx‚ÇÇ i))
  ‚Üí weighted_sum w‚ÇÅ x‚ÇÅ = weighted_sum w‚ÇÇ x‚ÇÇ := by
--------------------------------------------------------------------
  -- Simplify the weighted sum down to 
  -- fw‚ÇÅ i * fx‚ÇÅ j = fw‚ÇÇ i * fx‚ÇÇ j
  intro x‚ÇÅ x‚ÇÇ w‚ÇÅ w‚ÇÇ h‚ÇÅ
  simp only [weighted_sum]
  rw [List.zipWith_map_left]
  rw [List.zipWith_map_left]
  rw [List.zipWith_map_right]
  rw [List.zipWith_map_right]
  simp
  congr 2
  funext i
  
  -- Now we just need to show 
  -- fw‚ÇÅ i * fx‚ÇÅ i = fw‚ÇÇ i * fx‚ÇÇ i,
  -- but this was exactly our assumption.
  exact h‚ÇÅ i
  
--------------------------------------------------------------------
lemma weighted_sum_le (fw‚ÇÅ fw‚ÇÇ fx‚ÇÅ fx‚ÇÇ : ‚Ñï ‚Üí Float) (ls : List ‚Ñï) :
  let x‚ÇÅ : List Float := List.map (fun i => fx‚ÇÅ i) (List.range ls.length)
  let x‚ÇÇ : List Float := List.map (fun i => fx‚ÇÇ i) (List.range ls.length)
  let w‚ÇÅ : List Float := List.map (fun i => fw‚ÇÅ i) (List.range ls.length)
  let w‚ÇÇ : List Float := List.map (fun i => fw‚ÇÇ i) (List.range ls.length)
  
  (‚àÄ i, (fw‚ÇÅ i) * (fx‚ÇÅ i) ‚â§ (fw‚ÇÇ i) * (fx‚ÇÇ i))
  ‚Üí weighted_sum w‚ÇÅ x‚ÇÅ ‚â§ weighted_sum w‚ÇÇ x‚ÇÇ := by
--------------------------------------------------------------------

  intro h‚ÇÅ
  simp only [weighted_sum]
  sorry
  

-- WARNING:
-- This is actually FALSE!  For infinite sets, l[i] is not provably
-- in l (as far as I can figure.)
-- TODO: In the future, when making all this computable, I will
-- be using finite sets, and then I can use get instead of get!,
-- and get_mem in the standard library.
axiom get!_mem {Œ± : Type} [Inhabited Œ±] : 
  ‚àÄ (l : List Œ±) i, (l.get! i) ‚àà l

@[simp]
def preds (net : BFNN) (n : ‚Ñï): List ‚Ñï :=
  net.graph.predecessors n

--------------------------------------------------------------------
theorem edge_from_preds (net : BFNN) (m n : ‚Ñï) :
  m ‚àà preds net n ‚Üî net.graph.hasEdge m n := by
--------------------------------------------------------------------
  simp only [preds, Graph.hasEdge]
  rw [Bool.decide_iff]


-- (Weightless) *minimal* graph distance from node m to n.  Just count
-- the number of edges, i.e. don't apply weights.
-- (just here in order to define layer -- but if there's
--  a better way, I should use it!)
-- def distance (graph : Graph ‚Ñï Float) (m n : ‚Ñï) : ‚Ñï :=
--   sorry

/-
def my_argmax (f : Œ± ‚Üí Œ≤) (l : List Œ±) : Option Œ± :=
  l.foldl (argAux fun b c => f c < f b) none
-/

-- The layer of n.
-- We get all predecessors of n, and take the predecessor m
-- with the *maximum* layer.  Then layer(n) = layer(m) + 1.
-- If n has no predecessors, then layer(n) = 0.
-- 
-- TODO: Prove terminating if I can!  (What exactly is decreasing
--       here...)
-- def layer (net : BFNN) (n : ‚Ñï) : ‚Ñï :=
--   sorry
-- partial

-- TODO: I can do away with this axiom, and define 'layer'
-- more naturally if I define acyclic graphs recursively
-- in the first place!  Then I can do induction on 'net.graph'!
axiom graph_ascending_order : ‚àÄ (g : Graph ‚Ñï Float) (m n : ‚Ñï), 
  m ‚àà g.predecessors n ‚Üí m < n

-- Accumulator-style helper function for 'layer'
-- Defined recursively on the *reverse* of the vertex list
-- (this means we are looking at vertices backwards -- each
--  vertex can only "see" the vertices preceding it.)
def layer_acc (net : BFNN) (n : ‚Ñï) (ls : List ‚Ñï) : ‚Ñï :=
  match ls with
  | [] => 0
  | v :: rest =>
    if v = n then
      let layers := List.map (fun x => layer_acc net x rest) (preds net n)
      let max := layers.maximum

      match max with
      | some L => L + 1
      | none => 0

    else layer_acc net n rest

def layer (net : BFNN) (n : ‚Ñï) : ‚Ñï :=
  layer_acc net n (net.graph.get_vertices.reverse)

-- The layer relation layer[m] ‚â§ layer[n] is well-founded
-- (i.e. it has a least element)
--------------------------------------------------------------------
lemma layer_wellfounded (net : BFNN) : 
  WellFounded (fun x y => layer net x ‚â§ layer net y) := by
--------------------------------------------------------------------
  exact WellFounded.wellFounded_iff_has_min.mpr 
    (fun S hS => sorry)


/-
-- Put in test file!
-- 0 jumps to 2, 1 jumps to 3, short connection from 1 to 2
def layer_test_graph : Graph ‚Ñï Float :=
  ‚ü®[‚ü®0, [‚ü®2, 0.5‚ü©]‚ü©, -- ‚ü®4, 0.5‚ü©
    ‚ü®1, [‚ü®2, 0.5‚ü©, ‚ü®3, 0.5‚ü©]‚ü©, -- ‚ü®4, 0.5‚ü©
    ‚ü®2, []‚ü©, -- ‚ü®4, 0.5‚ü©
    ‚ü®3, [‚ü®4, 0.5‚ü©]‚ü©, -- remove ‚ü®4, 0.5‚ü©
    
    -- Add a new edge, and toggle which previous edge jumps to it.
    ‚ü®4, []‚ü©]‚ü©

def layer_test_net : BFNN :=
  { graph := layer_test_graph, activation := binary_step, rate := 1.0,
    binary := binary_step_is_binary,
    activ_nondecr := binary_step_nondecr, activ_pos := sorry
  }

#eval layer layer_test_net 0
#eval layer layer_test_net 1
#eval layer layer_test_net 2
#eval layer layer_test_net 3
#eval layer layer_test_net 4
-/

-- AXIOM: We assume the net is fully connected!
-- This is essentially the statement we need, which might
-- follow from being fully connected.
-- TODO: Put this in the definition of BFNN!!!
axiom connected : ‚àÄ (net : BFNN) (m n : ‚Ñï), 
  layer net m < layer net n ‚Üí net.graph.hasEdge m n

-- If m is a predecessor of n, then it must be in a previous layer.
-- Proof idea:
-- layer(m)  ‚â§  max({layer(v) | v ‚àà preds(n)})  <  layer(n)
--------------------------------------------------------------------
lemma preds_decreasing (net : BFNN) (m n : ‚Ñï) :
  m ‚àà preds net n 
  ‚Üí layer net m < layer net n := by
--------------------------------------------------------------------
  intro h‚ÇÅ
  simp only [layer]

  generalize hls : (List.reverse (Graph.get_vertices net.toNet.graph)) = ls
  induction ls
  case nil =>
    -- This case is impossible;
    -- m ‚àà preds(n) means that there is *something* in the graph.
    -- This contradicts the fact that the graph is empty!
    simp [preds, Graph.predecessors, Graph.get_vertices] at h‚ÇÅ
    simp [Graph.get_vertices] at hls
    rw [hls] at h‚ÇÅ
    rw [List.map_nil] at h‚ÇÅ
    rw [List.filter_nil] at h‚ÇÅ
    exact False.elim ((List.mem_nil_iff _).mp h‚ÇÅ)
    

  case cons v rest IH =>
    simp only [layer_acc]
    generalize hmax_m : (List.map (fun x => layer_acc net x rest) (preds net m)).maximum = max_m
    generalize hmax_n : (List.map (fun x => layer_acc net x rest) (preds net n)).maximum = max_n

    -- We branch out all of the possible cases
    -- (we have 4 branches from the 'if-then-else's, 
    -- and more for the 'match'es)
    by_cases v = m
    case pos => 
      rw [if_pos h]
      by_cases v = n
      case pos => 
        rw [if_pos h]
        
        match max_n with
        | none => -- This case is also impossible;
          -- m ‚àà preds(n) means that there is *some* maximum in preds(n),
          -- which contradicts the fact that the max is empty.
          sorry

        | some L => 
          match max_m with
          | none => exact Nat.succ_pos L
          | some K => -- This is the tricky case!
            simp
            sorry

      case neg => 
        rw [if_neg h]
        match max_m with
        | none => 
          simp
          sorry
        | some K => sorry

    case neg =>
      rw [if_neg h]
      by_cases v = n
      case pos => 
        rw [if_pos h]
        match max_n with
        | none => sorry
        | some L => sorry

      case neg => 
        rw [if_neg h]
        exact IH sorry

-- NOTE: Although 'do' notation might be more readable here,
--       I avoid it because it's hard to reason about.
noncomputable
def activ (net : BFNN) (prev_activ : List Float) (n : ‚Ñï) : Prop :=
  let preds := preds net n
  let weights := List.map (fun i => 
      let m := preds.get! i
      net.graph.getEdgeWeight m n) 
    (List.range preds.length)
  let weight_sum := weighted_sum weights prev_activ
  let curr_activ := net.activation weight_sum
  curr_activ = 1.0


-- FORWARD PROPAGATION IN A NET
-- By recursion on the layers of our feedforward network.
-- (_acc indicates that we are using the layer as an accumulator
--  to do recursion.)
-- 
-- n ‚àà propagate_acc(S) iff either:
--   Base Case (L=0): n ‚àà S
--   Rcrs Case (L‚â•0): n ‚àà S, or the nodes m preceding n activate n.
--     (We check their activation values via propagate_acc on m)
-- 
-- TODO: Make this computable!!!
-- change return type to 'Bool' instead of 'Prop'
-- and change 'Set' to be a finite set
-- and change net.graph to be finite as well!
-- 
-- Then unit-test all this with #eval!
-- 
-- NOTE: Although 'do' notation might be more readable here,
--       I avoid it because it's hard to reason about.
@[simp]
def propagate_acc (net : BFNN) (S : Set ‚Ñï) (n : ‚Ñï) (L : ‚Ñï) : Prop :=
  match L with
  | Nat.zero => n ‚àà S
  | Nat.succ _ =>
    let preds := preds net n
    let prev_activ := List.map (fun i => 
      let m := preds.get! i
      if propagate_acc net S m (layer net m) then 1.0 else 0.0) 
        (List.range preds.length)
    n ‚àà S ‚à® activ net prev_activ n
termination_by propagate_acc net S n L => layer net n
decreasing_by exact preds_decreasing net m n (get!_mem preds i)

-- Set variation of propagate
@[simp]
def propagate (net : BFNN) (S : Set ‚Ñï) : Set ‚Ñï :=
  fun n => propagate_acc net S n (layer net n)

-------------------------------------------------
-- Some helper lemmas
-- (just to clean up the monstrous proofs ahead!)
-- 
-- TODO: Clean these up with nicer @simp lemmas about
-- propagate and activ
-------------------------------------------------

--------------------------------------------------------------------
lemma simp_propagate_acc (net : BFNN) :
  n ‚àâ S
  ‚Üí propagate_acc net S n (Nat.succ L) =
  let preds := preds net n
  let prev_activ := List.map (fun i => 
    let m := preds.get! i
    if propagate_acc net S m (layer net m) then 1.0 else 0.0) 
      (List.range preds.length)
  activ net prev_activ n := by
--------------------------------------------------------------------
  intro (h‚ÇÅ : n ‚àâ S)
  apply Iff.to_eq
  apply Iff.intro

  case mp => 
    intro h‚ÇÇ
    simp only [propagate_acc]
    simp only [propagate_acc] at h‚ÇÇ
    
    cases h‚ÇÇ
    case inl h‚ÇÉ => contradiction
    case inr h‚ÇÉ => exact h‚ÇÉ 
  
  case mpr => 
    intro h‚ÇÇ
    simp only [propagate_acc]
    simp only [propagate_acc] at h‚ÇÇ
    exact Or.inr h‚ÇÇ


-- If A and B agree on all the predecessors of n, then they agree on n.
-- This lemma is extremely inefficient to use.  In order to make it
-- remotely usable, we've simplified everything down to unreadable
-- garbage.  In order to make use of it, I recommend:
--   - Applying 'simp' to your 'activ' hypotheses (get rid of any let's)
--   - Use 'exact' instead of 'apply' (exit tactic mode)
-- 
--------------------------------------------------------------------
lemma activ_agree (net : BFNN) (A B : Set ‚Ñï) (n : ‚Ñï) :
  (‚àÄ (m : ‚Ñï), m ‚àà preds net n ‚Üí (m ‚àà A ‚Üî m ‚àà B))
  ‚Üí activ net (List.map (fun i => 
      let m := (preds net n).get! i
      if A m then 1.0 else 0.0) 
        (List.range (preds net n).length)) n
  ‚Üí activ net (List.map (fun i => 
      let m := (preds net n).get! i
      if B m then 1.0 else 0.0) 
        (List.range (preds net n).length)) n := by
--------------------------------------------------------------------
  intro h‚ÇÅ h‚ÇÇ
  simp only [activ]
  simp only [activ] at h‚ÇÇ
  convert ‚Üê h‚ÇÇ using 6

  rename_i i
  let m := (preds net n).get! i
  have h‚ÇÉ : m ‚àà (preds net n) := get!_mem (preds net n) i
  exact h‚ÇÅ m h‚ÇÉ

/-‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  Properties of Propagation
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê-/

--------------------------------------------------------------------
lemma prop_layer_zero (net : BFNN) : ‚àÄ (S : Set ‚Ñï) (n : ‚Ñï),
  layer net n = 0
  ‚Üí n ‚àà propagate net S
  ‚Üí n ‚àà S := by
--------------------------------------------------------------------
  intro (S : Set ‚Ñï) (n : ‚Ñï)
        (hL : layer net n = 0)
        (h‚ÇÅ : n ‚àà propagate net S)

  simp only [propagate, Membership.mem, Set.Mem] at h‚ÇÅ
  rw [hL] at h‚ÇÅ
  simp only [propagate_acc] at h‚ÇÅ
  exact h‚ÇÅ

--------------------------------------------------------------------
theorem propagate_is_extens : 
  ‚àÄ (S : Set ‚Ñï),
  S ‚äÜ propagate net S := by
--------------------------------------------------------------------
  intro (S : Set ‚Ñï)
        (n : ‚Ñï) (h‚ÇÅ : n ‚àà S)
  simp only [Membership.mem, Set.Mem, propagate]

  -- By induction on the layer of the net containing n
  generalize hL : layer net n = L
  induction L

  -- Base Step
  case zero => 
    simp only [propagate_acc]
    exact h‚ÇÅ
  
  -- Inductive Step
  case succ k _ => 
    simp only [propagate_acc]
    exact Or.inl h‚ÇÅ

-- Corollary: The same statement, but for propagate_acc
-- (this is a helper lemma for the properties that follow.)
-------------------------------------------------
lemma propagate_acc_is_extens :
  ‚àÄ (S : Set ‚Ñï),
  n ‚àà S ‚Üí propagate_acc net S n (layer net n) := by
-------------------------------------------------
  intro (S : Set ‚Ñï)
  intro (h‚ÇÅ : n ‚àà S)
  have h‚ÇÇ : n ‚àà propagate net S := propagate_is_extens _ _ h‚ÇÅ
  simp only [Membership.mem, Set.Mem, propagate] at h‚ÇÇ
  exact h‚ÇÇ
  

--------------------------------------------------------------------
theorem propagate_is_idempotent : 
  ‚àÄ (S : Set ‚Ñï),
  propagate net S = 
    propagate net (propagate net S) := by
--------------------------------------------------------------------
  intro (S : Set ‚Ñï)
  apply ext
  intro (n : ‚Ñï)
  simp only [Membership.mem, Set.Mem, propagate]

  -- By strong induction on the layer of the net containing n
  generalize hL : layer net n = L
  induction L using Nat.case_strong_induction_on generalizing n

  -- Base Step
  case hz =>
    simp only [Membership.mem, Set.Mem, propagate_acc]
    conv in (layer net n) => rw [hL]
    simp only [propagate_acc]
    exact ‚ü®fun x => x, fun x => x‚ü©
  
  -- Inductive Step
  case hi L IH =>
    apply Iff.intro
    
    -- Forward direction is easy, just apply extensive
    case mp =>
      intro h‚ÇÅ
      rw [symm hL]
      rw [symm hL] at h‚ÇÅ
      exact @propagate_acc_is_extens net _ _ h‚ÇÅ

    -- Backwards direction is trickier
    case mpr => 
      intro h‚ÇÅ
      
      -- By cases; either n ‚àà S or n ‚àâ S
      -- Again; either n ‚àà propagate S or n ‚àâ propagate S 
      by_cases n ‚àà S
      case pos => 
        rw [symm hL]
        exact @propagate_acc_is_extens net _ _ h
      case neg => 
        by_cases propagate_acc net S n (layer net n)
        case pos => 
          rw [symm hL]
          exact h
        case neg =>
          -- Just some simplifications and rewriting definitions
          rename_i n_not_in_S
          rw [simp_propagate_acc net n_not_in_S]
          
          have h‚ÇÇ : ¬¨n ‚àà propagate net S := h
          simp [propagate] at h‚ÇÇ
          rw [simp_propagate_acc net h‚ÇÇ] at h‚ÇÅ

          -- We abbreviate the two 'simp'ed sets for convenience.
          let S‚ÇÅ := fun m => propagate_acc net (fun n => 
            propagate_acc net S n (layer net n)) m (layer net m)
          let S‚ÇÇ := fun m => propagate_acc net S m (layer net m)
          
          -- Apply IH to the predecessors
          have h‚ÇÇ : ‚àÄ (m : ‚Ñï), m ‚àà preds net n ‚Üí (S‚ÇÅ m ‚Üî S‚ÇÇ m) := by
            simp only [Membership.mem] -- really just want to unfold the let
            intro m hm
            generalize hLm : layer net m = Lm
            have h‚ÇÉ : Lm ‚â§ L := by
              rw [symm hLm]
              apply Nat.lt_succ.mp
              rw [symm hL]
              exact preds_decreasing net m n hm
            exact (symm (IH Lm h‚ÇÉ m hLm).to_eq).to_iff

          -- Apply the activ_agree lemma
          simp
          simp at h‚ÇÅ
          exact activ_agree _ S‚ÇÅ S‚ÇÇ _ h‚ÇÇ h‚ÇÅ


-- This is essentially Hannes' proof.
-- TODO: For consistency's sake, replace inner proof with
--   application of 'activ_agree'
--------------------------------------------------------------------
theorem propagate_is_cumulative :
  ‚àÄ (A B : Set ‚Ñï), A ‚äÜ B
  ‚Üí B ‚äÜ propagate net A
  ‚Üí propagate net A = propagate net B := by
--------------------------------------------------------------------
  intro (A : Set ‚Ñï) (B : Set ‚Ñï)
        (h‚ÇÅ : A ‚äÜ B)
        (h‚ÇÇ : B ‚äÜ propagate net A)
  apply ext
  intro (n : ‚Ñï)
  simp only [Membership.mem, Set.Mem, propagate]
  simp only [Membership.mem, Set.Mem, propagate] at h‚ÇÇ

  -- By induction on the layer of the net containing n
  generalize hL : layer net n = L
  induction L using Nat.case_strong_induction_on generalizing n

  -- Base Step
  case hz =>
    simp only [propagate_acc]
    apply Iff.intro
    case mp => exact fun h‚ÇÉ => h‚ÇÅ h‚ÇÉ
    case mpr =>
      intro h‚ÇÉ
      have h‚ÇÑ : propagate_acc net A n (layer net n) := h‚ÇÇ h‚ÇÉ
      conv at h‚ÇÑ in (layer net n) => rw [hL]
      simp only [propagate_acc] at h‚ÇÑ
      exact h‚ÇÑ

  -- Inductive Step
  case hi k IH => 
    apply Iff.intro

    -- Forward Direction
    case mp => 
      intro h‚ÇÉ

      -- By cases; either n ‚àà B or n ‚àâ B.
      -- Similarly, either n ‚àà A or n ‚àâ A. 
      by_cases n ‚àà B
      case pos =>
        rw [symm hL]
        exact @propagate_acc_is_extens net _ _ h -- TODO: replace acc variation
      case neg =>
        by_cases n ‚àà A
        case pos => 
          rename_i n_not_in_B 
          exact absurd (h‚ÇÅ h) n_not_in_B
        case neg => 
          -- Just some simplifications and rewriting definitions
          rename_i n_not_in_B
          rw [simp_propagate_acc net h] at h‚ÇÉ
          rw [simp_propagate_acc net n_not_in_B]

          -- Just try to prove it and turn it into an 'activ_agree'
          -- lemma later!
          -- Go into 'prev_activ' and substitute using our IH.
          -- Then try to prove what's left.
          simp
          simp at h‚ÇÉ
          convert h‚ÇÉ using 4
          rename_i i
          generalize hm : List.get! (net.graph.predecessors n) i = m
          generalize hLm : layer net m = Lm

          -- Apply the inductive hypothesis!
          have h‚ÇÉ : m ‚àà preds net n := by
            rw [symm hm]
            simp [preds]
            exact get!_mem (net.graph.predecessors n) i
          have h‚ÇÑ : Lm ‚â§ k := by 
            rw [symm hLm]
            apply Nat.lt_succ.mp
            rw [symm hL]
            exact preds_decreasing net m n h‚ÇÉ
          exact (symm (IH Lm h‚ÇÑ m hLm).to_eq).to_iff

    -- Backwards Direction (should be very similar)
    case mpr => 
      intro h‚ÇÉ

      -- By cases; either n ‚àà A or n ‚àâ A
      by_cases n ‚àà A
      case pos => 
        rw [symm hL]
        exact @propagate_acc_is_extens net _ _ h -- TODO: replace acc variation
      case neg => 
        by_cases n ‚àà B
        case pos => 
          rename_i n_not_in_A
          rw [symm hL]
          exact h‚ÇÇ h
        case neg => 
          -- Just some simplifications and rewriting definitions
          rename_i n_not_in_A
          rw [simp_propagate_acc net h] at h‚ÇÉ
          rw [simp_propagate_acc net n_not_in_A]

          -- Just try to prove it and turn it into an 'activ_agree'
          -- lemma later!
          -- Go into 'prev_activ' and substitute using our IH.
          -- Then try to prove what's left.
          simp
          simp at h‚ÇÉ
          convert h‚ÇÉ using 4
          rename_i i
          generalize hm : List.get! (net.graph.predecessors n) i = m
          generalize hLm : layer net m = Lm

          -- Apply the inductive hypothesis!
          have h‚ÇÉ : m ‚àà preds net n := by
            rw [symm hm]
            simp [preds]
            exact get!_mem (net.graph.predecessors n) i
          have h‚ÇÑ : Lm ‚â§ k := by 
            rw [symm hLm]
            apply Nat.lt_succ.mp
            rw [symm hL]
            exact preds_decreasing net m n h‚ÇÉ
          exact IH Lm h‚ÇÑ m hLm


-- #check propagate myBFNN {n : ‚Ñï | n ‚â§ 4}
-- #eval propagate myBFNN {n : ‚Ñï | n ‚â§ 4}
-- need to make sets finite in order to evaluate???
-- 
-- It's important for everything to be evaluatable, since:
-- 1) I will want to verify that a *specific*
--    neural network has certain properties
-- 2) #eval helps me debug errors


/-‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  Conditional Graph Reachability
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê-/
-- reachable(A, B) is the set of all nodes reachable from B
-- using a path where all nodes are inside A (i.e. there is a 
-- focusedPath from B to n).
-- 
-- This is *precisely* what Van Benthem refers to as "conditional
-- common knowledge" (although here we don't need the word "common"
-- because we don't have group dynamics.)  
-- Quote:
-- CG(A, B) "is true in all worlds reachable via some finite path
-- of accessibilities running entirely through worlds satisfying A"
-- [Van Benthem, Belief Revision and Dynamic Logic, page 6]
-- In this paper, he also talks about "pre-encoding" future
-- information in order to get a reduction, which is exactly
-- what we're doing here!
-- 
-- I don't know what the complete axioms are for this conditional
-- knowledge.  But this isn't the main focus here.  I'll just
-- prove a few sound things to give an idea about what it's like.

-- A focused path is a path where every node is contained
-- within the set S.
inductive focusedPath (g : Graph ‚Ñï Float) (S : Set ‚Ñï) : ‚Ñï ‚Üí ‚Ñï ‚Üí Prop where
  | trivial {u : ‚Ñï} :
      u ‚àà S ‚Üí focusedPath g S u u
  | from_path {u v w : ‚Ñï} : 
      focusedPath g S u v ‚Üí g.hasEdge v w ‚Üí w ‚àà S ‚Üí focusedPath g S u w

-- focusedPath is transitive
--------------------------------------------------------------------
theorem focusedPath_trans {u v w : ‚Ñï} (g : Graph ‚Ñï Float) (A : Set ‚Ñï) :
  focusedPath g A u v ‚Üí focusedPath g A v w ‚Üí focusedPath g A u w := by
--------------------------------------------------------------------
  intro (h‚ÇÅ : focusedPath g A u v)
  intro (h‚ÇÇ : focusedPath g A v w)

  induction h‚ÇÇ
  case trivial _ => exact h‚ÇÅ
  case from_path x y _ edge_xy hy path_ux => 
    exact focusedPath.from_path path_ux edge_xy hy

-- focusedPath is contained in A
--------------------------------------------------------------------
theorem focusedPath_subset {u v : ‚Ñï} (g : Graph ‚Ñï Float) (A : Set ‚Ñï) :
  focusedPath g A u v ‚Üí u ‚àà A := by
--------------------------------------------------------------------
  intro h‚ÇÅ

  induction h‚ÇÅ
  case trivial hA => exact hA
  case from_path _ _ _ _ _ hA => exact hA

-- If there's a path from m to n,
-- then m's layer in the net cannot be bigger than n's layer.
--------------------------------------------------------------------
lemma focusedPath_layer {m n : ‚Ñï} (net : BFNN) (A : Set ‚Ñï) :
  focusedPath net.graph A m n
  ‚Üí layer net m ‚â§ layer net n := by
--------------------------------------------------------------------
  sorry

-- This is the set of all nodes reachable from B using
-- paths where *every* node in the path is within A
-- (including the initial and final nodes)
def reachable (net : BFNN) (A B : Set ‚Ñï) : Set ‚Ñï :=
  fun (n : ‚Ñï) =>
    ‚àÉ (m : ‚Ñï), m ‚àà B ‚àß focusedPath net.graph A m n

-- Argument: If there is a path from B to n, but n is in
-- layer 0 -- there are *no* incoming nodes -- then the path
-- must be of length 0.  So n must be that n ‚àà B with
-- a path to n, i.e. n ‚àà B.
--------------------------------------------------------------------
lemma reach_layer_zero (net : BFNN) : ‚àÄ (A B : Set ‚Ñï) (n : ‚Ñï),
  layer net n = 0
  ‚Üí n ‚àà reachable net A B
  ‚Üí n ‚àà B := by
--------------------------------------------------------------------
  intro (A : Set ‚Ñï)
        (B : Set ‚Ñï)
        (n : ‚Ñï) (hL : layer net n = 0)
        (h‚ÇÅ : n ‚àà reachable net A B)
  
  match h‚ÇÅ with
  | ‚ü®m, h‚ÇÇ‚ü© => 
    -- By induction on the length of the path from B to n.
    --   path length = 0 => m ‚àà B means n ‚àà B
    --   path length ‚â• 0 => this case should be impossible,
    --                      because at layer 0 n has *no predecessors*! 
    induction h‚ÇÇ.2
    case trivial _ => exact h‚ÇÇ.1
    case from_path x y _ edge_xy _ _ =>
      -- Contradiction; y's layer is 0, but there is an edge from x to y!
      -- (layer net x < layer net y, but that means layer net x < 0) 
      have h‚ÇÉ : layer net x < layer net y :=
        preds_decreasing net x y ((edge_from_preds _ _ _).mpr edge_xy)
      
      exact absurd hL (Nat.not_eq_zero_of_lt h‚ÇÉ)

--------------------------------------------------------------------
theorem reach_subset (net : BFNN) : ‚àÄ (A B : Set ‚Ñï),
  reachable net A B ‚äÜ A := by
--------------------------------------------------------------------
  intro (A : Set ‚Ñï)
        (B : Set ‚Ñï)
        (n : ‚Ñï) (h‚ÇÅ : n ‚àà reachable net A B)
  
  simp only [Membership.mem, Set.Mem] at h‚ÇÅ
  match h‚ÇÅ with
  | ‚ü®m, hm‚ü© => 
    induction hm.2
    case trivial hbase => exact hbase
    case from_path _ y _ _ hy _ => exact hy 


-- This is like propag_is_extens, except we also have to ensure
-- that n ‚àà A.
--------------------------------------------------------------------
theorem reach_is_extens (net : BFNN) : ‚àÄ (A B : Set ‚Ñï),
  (A ‚à© B) ‚äÜ reachable net A B := by
--------------------------------------------------------------------
  intro (A : Set ‚Ñï)
        (B : Set ‚Ñï)
        (n : ‚Ñï) (h‚ÇÅ : n ‚àà A ‚à© B)

  have (h‚ÇÇ : focusedPath net.graph A n n) := 
    focusedPath.trivial h‚ÇÅ.1
  exact ‚ü®n, ‚ü®h‚ÇÅ.2, h‚ÇÇ‚ü©‚ü©


--------------------------------------------------------------------
theorem reach_is_idempotent (net : BFNN) : ‚àÄ (A B : Set ‚Ñï),
  reachable net A B = reachable net A (reachable net A B) := by
--------------------------------------------------------------------
  intro (A : Set ‚Ñï)
        (B : Set ‚Ñï)
  
  exact Set.ext (fun (n : ‚Ñï) =>
    -- ‚äÜ direction; easy, just apply reach_subset and reach_is_extens
    ‚ü®(fun (h‚ÇÅ : n ‚àà reachable net A B) => 
      have h‚ÇÇ : n ‚àà A := reach_subset _ _ _ h‚ÇÅ
      reach_is_extens _ _ _ ‚ü®h‚ÇÇ, h‚ÇÅ‚ü©),

    -- ‚äá direction
    (fun (h‚ÇÅ : n ‚àà reachable net A (reachable net A B)) =>
      match h‚ÇÅ with
      | ‚ü®x, h‚ÇÇ‚ü© => 
        match h‚ÇÇ.1 with
        | ‚ü®m, h‚ÇÉ‚ü© =>
          ‚ü®m, ‚ü®h‚ÇÉ.1, focusedPath_trans _ A h‚ÇÉ.2 h‚ÇÇ.2‚ü©‚ü©)‚ü©)


--------------------------------------------------------------------
theorem reach_is_monotone (net : BFNN) : ‚àÄ (S A B : Set ‚Ñï),
  A ‚äÜ B ‚Üí reachable net S A ‚äÜ reachable net S B := by
--------------------------------------------------------------------
  intro (S : Set ‚Ñï)
        (A : Set ‚Ñï)
        (B : Set ‚Ñï)
        (h‚ÇÅ : A ‚äÜ B)
        (n : ‚Ñï) (h‚ÇÇ : n ‚àà reachable net S A)
  
  exact match h‚ÇÇ with
    | ‚ü®m, hm‚ü© => ‚ü®m, ‚ü®h‚ÇÅ hm.1, hm.2‚ü©‚ü©


-- Reach is closed under union
-- (This is really a consequence of monotonicity)
--------------------------------------------------------------------
theorem reach_union (net : BFNN) : ‚àÄ (S A B : Set ‚Ñï),
  reachable net S (A ‚à™ B) = (reachable net S A) ‚à™ (reachable net S B) := by
--------------------------------------------------------------------
  intro S A B
  apply ext
  intro n
  apply Iff.intro
  
  -- Backwards direction
  -- (easy; A, B ‚äÜ A ‚à™ B, so we just apply monotonicity)
  case mpr => 
    intro h‚ÇÅ
    cases h‚ÇÅ
    case inl h‚ÇÇ => exact reach_is_monotone _ _ _ _ (subset_union_left _ _) h‚ÇÇ
    case inr h‚ÇÇ => exact reach_is_monotone _ _ _ _ (subset_union_right _ _) h‚ÇÇ

  -- Forward direction
  case mp => 
    intro h‚ÇÅ

    -- We have a path from m ‚àà A ‚à™ B to n;
    -- from here we go by cases; either m ‚àà A or m ‚àà B.
    -- In either case, the path from m ‚ü∂ n gives us n ‚àà Reach(_).
    match h‚ÇÅ with
    | ‚ü®m, hm‚ü© => 
      cases hm.1
      case inl h‚ÇÇ => exact Or.inl ‚ü®m, ‚ü®h‚ÇÇ, hm.2‚ü©‚ü©
      case inr h‚ÇÇ => exact Or.inr ‚ü®m, ‚ü®h‚ÇÇ, hm.2‚ü©‚ü©


/-‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  Reach-Prop Interaction Properties
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê-/

-- A simple interaction between graph reachability and propagation
--------------------------------------------------------------------
theorem reach_propagate (net : BFNN) : ‚àÄ (A B : Set ‚Ñï),
  reachable net A (propagate net B) ‚äÜ reachable net A B := by
--------------------------------------------------------------------
  intro A B n h‚ÇÅ
  
  -- By induction on the layer of the net containing n
  generalize hL : layer net n = L
  induction L using Nat.case_strong_induction_on generalizing n

  --------------------------------
  -- Base Step
  --------------------------------
  case hz => 
    have h‚ÇÇ : n ‚àà propagate net B := reach_layer_zero _ _ _ _ hL h‚ÇÅ
    simp only [propagate, Membership.mem, Set.Mem] at h‚ÇÇ
    rw [hL] at h‚ÇÇ
    simp only [propagate_acc] at h‚ÇÇ

    -- Moreover, n ‚àà A (since n ‚àà Reach(A, B))
    have h‚ÇÉ : n ‚àà A := reach_subset _ _ _ h‚ÇÅ

    -- We have n ‚àà B, and so n ‚àà Reach(A, B). 
    exact ‚ü®n, ‚ü®h‚ÇÇ, focusedPath.trivial h‚ÇÉ‚ü©‚ü© 

  --------------------------------
  -- Inductive Step
  --------------------------------
  case hi L IH =>
    -- simp [propagate] at h‚ÇÅ

    match h‚ÇÅ with
    | ‚ü®m, hm‚ü© =>
      -- First, apply our IH to m
      have h‚ÇÇ : m ‚àà A := focusedPath_subset _ _ hm.2
      have h‚ÇÉ : (layer net m) ‚â§ L := sorry
      have h‚ÇÑ : m ‚àà reachable net A (propagate net B) := by
        exact ‚ü®m, ‚ü®hm.1, focusedPath.trivial h‚ÇÇ‚ü©‚ü©
      have h‚ÇÖ : m ‚àà reachable net A B := IH (layer net m) h‚ÇÉ h‚ÇÑ rfl

      -- Now we have a path from some x‚ü∂m.
      match h‚ÇÖ with
      | ‚ü®x, hx‚ü© => 
        -- We show n ‚àà Reach(A, B)
        -- by providing a path x ‚ü∂ m ‚ü∂ n
        exact ‚ü®x, ‚ü®hx.1, focusedPath_trans _ _ hx.2 hm.2‚ü©‚ü©

/-‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  Naive (Unstable) Hebbian Update
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê-/

-- A function to map edges in the graph.  
-- We update the edge weight x‚ÇÅ ‚ü∂ x‚ÇÇ, but we also allow information 
-- about the *nodes* x‚ÇÅ and x‚ÇÇ.
-- Credit to Peter Kementzey for the original mapEdges function.
def map_edges (g : Graph ‚Ñï Float) (f : ‚Ñï ‚Üí ‚Ñï ‚Üí Float ‚Üí Float) : Graph ‚Ñï Float := ‚ü®
  g.vertices.map (fun vertex => 
    { vertex with successors := vertex.successors.map (fun 
      ‚ü®target, weight‚ü© => ‚ü®target, f vertex.label target weight‚ü©
  )})
‚ü©

--------------------------------------------------------------------
lemma map_edges_apply (g : Graph ‚Ñï Float) (f : ‚Ñï ‚Üí ‚Ñï ‚Üí Float ‚Üí Float) (m n : ‚Ñï) :
  (map_edges g f).getEdgeWeight m n = (f m n (g.getEdgeWeight m n)) := by
--------------------------------------------------------------------
  -- I have no idea... this one's tough, and it's hard to see
  -- what's going on.
  -- TODO: Reasoning about weights is hard!  Redefine getEdgeWeight!
  simp only [Graph.getEdgeWeight]

  match List.find? (fun x => decide (x.fst = n)) g.vertices[m]!.successors with
  | none => sorry
  | some ‚ü®_, weight‚ü© => sorry
  
  /-
  split
  case _ op‚ÇÅ target‚ÇÅ weight‚ÇÅ hop‚ÇÅ => 
    split
    case _ op‚ÇÇ target‚ÇÇ weight‚ÇÇ hop‚ÇÇ => sorry
    case _ op‚ÇÇ hop‚ÇÇ => sorry
  case _ op‚ÇÅ hop‚ÇÅ => 
    split
    case _ op‚ÇÇ target‚ÇÇ weight‚ÇÇ hop‚ÇÇ => sorry
    case _ op‚ÇÇ hop‚ÇÇ => sorry
  -/

-- For every m ‚ü∂ n where m, n ‚àà Prop(S), increase the weight
-- of that edge by Œ∑ * act(m) * act(n).
noncomputable
def graph_update (net : BFNN) (g : Graph ‚Ñï Float) (S : Set ‚Ñï) : Graph ‚Ñï Float :=
  map_edges g (fun m n weight => 
    let activ_m := if m ‚àà propagate net S then 1.0 else 0.0
    let activ_n := if n ‚àà propagate net S then 1.0 else 0.0
    weight + (net.rate * activ_m * activ_n))



-- This graph update does not affect the vertices of the graph.
--------------------------------------------------------------------
lemma graph_update_vertices (net : BFNN) (g : Graph ‚Ñï Float) (S : Set ‚Ñï) :
  (graph_update net g S).get_vertices = g.get_vertices := by
--------------------------------------------------------------------
  simp only [graph_update, map_edges]
  simp only [Graph.get_vertices]

  -- Go in and compose the maps.  This seems to do the trick.
  conv => lhs; rw [List.map_map]


-- This graph update does not affect the *successor/edge* structure
-- of the graph (it only affects weights!!!)
--------------------------------------------------------------------
lemma graph_update_successors (net : BFNN) (g : Graph ‚Ñï Float) (S : Set ‚Ñï) (n : ‚Ñï) :
  (graph_update net g S).successors n = g.successors n := by
--------------------------------------------------------------------
  -- Simplify definitions
  simp only [Graph.successors]
  rw [graph_update_vertices]
  simp only [graph_update, map_edges]
  
  -- Rewrite 'unzip's as 'map's
  rw [List.unzip_eq_map]
  rw [List.unzip_eq_map]
  simp only [Prod.fst]

  -- Get to the heart of the expression
  congr
  funext m
  apply Bool.decide_congr

  -- Our goal is to now show that when we *only* consider the target
  -- vertices, the graph_update is essentially the identity function.
  -- By induction on the graph's list of vertices:
  induction g.vertices
  case nil => 
    simp only [Graph.successor_pairs]

  case cons v rest IH => 
    simp only [Graph.successor_pairs]

    -- By cases; either v.label = n or not.  If so, we go in
    -- and compose the maps.  Otherwise we just apply our IH.
    by_cases (v.label = n)
    case pos => 
      rw [if_pos h]
      rw [if_pos h]
      rw [List.map_map]
      simp

    case neg =>
      rw [if_neg h]
      rw [if_neg h]
      exact IH

-- This graph update preserves whether the graph is acyclic.
--------------------------------------------------------------------
lemma graph_update_acyclic (net : BFNN) (g : Graph ‚Ñï Float) (S : Set ‚Ñï) :
  (graph_update net g S).is_acyclic ‚Üî g.is_acyclic := by
--------------------------------------------------------------------
  simp only [Graph.is_acyclic]
  apply Iff.intro
  
  case mp => sorry
  case mpr =>
    intro g_acyclic
    intro u v
    intro path_uv
    intro path_vu

    -- We have a path in the updated graph from u to v
    -- and another path from v to u.
    -- We now need to show that u = v.
    -- By induction on the length of this first path.
    induction path_uv
    case trivial => exact rfl
    case from_path x y path_ux edge_xy IH => 
      sorry

-- A single step of Hebbian update.
-- Propagate S through the net, and then increase the weights
-- of all the edges x‚ÇÅ ‚ü∂ x‚ÇÇ involved in that propagation
-- by Œ∑ * x‚ÇÅ * x‚ÇÇ.
noncomputable
def hebb (net : BFNN) (S : Set ‚Ñï) : BFNN :=
{ net with
  graph := graph_update net net.graph S
}


/-‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  Properties of Single-Iteration Hebbian Update
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê-/

-- First, we check that a single round of hebb preserves whether
-- the graph is acyclic. (This is a rehash of graph_update_acyclic,
-- but it helps to write it out so we can lift it later to hebb_star.)
--------------------------------------------------------------------
lemma hebb_once_acyclic (net : BFNN) (S : Set ‚Ñï) : 
  (hebb net S).graph.is_acyclic = net.graph.is_acyclic := by
--------------------------------------------------------------------
  simp only [hebb]
  rw [graph_update_acyclic]

-- A single round of Hebbian update does not affect the vertices
-- of the graph.
--------------------------------------------------------------------
theorem hebb_once_vertices (net : BFNN) (S : Set ‚Ñï) : 
  (hebb net S).graph.get_vertices = net.graph.get_vertices := by
--------------------------------------------------------------------
  simp only [hebb]
  rw [graph_update_vertices]

-- A single round of Hebbian update does not affect the predecessors
-- of any node.  To prove this, we just show that Hebbian update
-- does not affect the vertices of the graph, or the successor
-- structure of the graph.
--------------------------------------------------------------------
theorem hebb_once_preds (net : BFNN) (S : Set ‚Ñï) (n : ‚Ñï) : 
  preds (hebb net S) n = preds net n := by
--------------------------------------------------------------------
  simp only [preds, hebb]
  simp only [Graph.predecessors]
  congr 1

  -- graph_update doesn't affect successors of a node
  case _ => 
    funext v
    congr 2
    exact graph_update_successors _ (net.graph) _ v

  -- graph_update doesn't affect vertices of the graph
  case _ => exact graph_update_vertices _ (net.graph) _

  
-- A single round of Hebbian update hebb does not affect which 
-- neurons are on which layer of the net.
--------------------------------------------------------------------
theorem hebb_once_layers (net : BFNN) (S : Set ‚Ñï) : 
  layer (hebb net S) n = layer net n := by
--------------------------------------------------------------------
  simp only [layer]
  rw [hebb_once_vertices net S] -- vertices are the same

  -- By induction on the reverse of the graph's vertex list
  -- (We are looking at vertices backwards -- each
  --  vertex can only "see" the vertices preceding it.)
  induction List.reverse (Graph.get_vertices net.toNet.graph) generalizing n
  case nil => 
    simp only [layer_acc]
  case cons v rest IH => 
    simp only [layer_acc]

    -- By cases; either vertex.label = n or not.
    -- If so, this follows from our IH and the fact that the predecessors
    -- are the same in both nets.
    by_cases v = n
    case pos => 
      rw [if_pos h]
      rw [if_pos h]
      rw [hebb_once_preds]
      congr
      funext m
      exact @IH m

    -- If not, just apply our IH
    case neg => 
      rw [if_neg h]
      rw [if_neg h]
      exact IH

-- A single round of Hebbian update hebb does not affect the 
-- activation function.
--------------------------------------------------------------------
theorem hebb_once_activation (net : BFNN) (S : Set ‚Ñï) : 
  (hebb net S).activation = net.activation := by 
--------------------------------------------------------------------
  exact rfl

-- A single round of Hebbian update hebb does not affect graph 
-- reachability (It only affects the edge weights)
--------------------------------------------------------------------
theorem hebb_once_reach (net : BFNN) (A B : Set ‚Ñï) : 
  reachable (hebb net A) A B = reachable net A B := by 
--------------------------------------------------------------------
  apply ext
  intro (n : ‚Ñï)
  -- simp only [reachable]
  
  apply Iff.intro
  case mp => 
    intro h‚ÇÅ

    -- There is some m with focused path from m to n in the *updated* net
    match h‚ÇÅ with
    | ‚ü®m, hm‚ü© =>
      induction hm.2
      case trivial hma => exact reach_is_extens _ _ _ ‚ü®hma, hm.1‚ü©
      case from_path x y path_mx edge_xy hy IH =>
        -- First, apply our IH to get x ‚àà Reach(A, B)
        have h‚ÇÇ : x ‚àà reachable (hebb net A) A B := ‚ü®m, ‚ü®hm.1, path_mx‚ü©‚ü©
        have h‚ÇÉ : x ‚àà reachable net A B := IH h‚ÇÇ ‚ü®hm.1, path_mx‚ü©

        -- So there is some u ‚àà B with focused path from u to x
        -- (in the *original* net)
        -- We extend this path with the edge from x to y.
        match h‚ÇÉ with
        | ‚ü®u, hu‚ü© =>

          -- We have an edge from x to y in the *updated* net,
          -- but we have to bring it down to the *original* net.
          have h‚ÇÑ : Graph.hasEdge net.graph x y = true := by
            rw [‚Üê edge_from_preds]
            rw [‚Üê edge_from_preds] at edge_xy
            convert edge_xy using 1
            exact symm (hebb_once_preds _ _ _)

          exact ‚ü®u, ‚ü®hu.1, (focusedPath.from_path hu.2 h‚ÇÑ hy)‚ü©‚ü©

  -- This direction is very similar.
  case mpr => 
    intro h‚ÇÅ

    -- There is some m with focused path from m to n in the *original* net
    match h‚ÇÅ with
    | ‚ü®m, hm‚ü© =>
      induction hm.2
      case trivial hma => exact reach_is_extens _ _ _ ‚ü®hma, hm.1‚ü©
      case from_path x y path_mx edge_xy hy IH =>
        -- First, apply our IH to get x ‚àà Reach(A, B)
        have h‚ÇÇ : x ‚àà reachable net A B := ‚ü®m, ‚ü®hm.1, path_mx‚ü©‚ü©
        have h‚ÇÉ : x ‚àà reachable (hebb net A) A B := IH h‚ÇÇ ‚ü®hm.1, path_mx‚ü©
        
        -- So there is some u ‚àà B with focused path from u to x
        -- (in the *updated* net)
        -- We extend this path with the edge from x to y.
        match h‚ÇÉ with
        | ‚ü®u, hu‚ü© =>

          -- We have an edge from x to y in the *original* net,
          -- but we have to bring it down to the *updated* net.
          have h‚ÇÑ : Graph.hasEdge (hebb net A).graph x y = true := by
            rw [‚Üê edge_from_preds]
            rw [‚Üê edge_from_preds] at edge_xy
            convert edge_xy using 1
            exact hebb_once_preds _ _ _
            
          exact ‚ü®u, ‚ü®hu.1, (focusedPath.from_path hu.2 h‚ÇÑ hy)‚ü©‚ü©

-- If m ‚àâ Prop(A) or n ‚àâ Prop(A), then the weight of m ‚ü∂ n in 
-- the *once* updated net is the same as in the original net.
--------------------------------------------------------------------
theorem hebb_once_weights (net : BFNN) :
  (m ‚àâ propagate net A ‚à® n ‚àâ propagate net A)
  ‚Üí (hebb net A).toNet.graph.getEdgeWeight m n =
    net.graph.getEdgeWeight m n := by
--------------------------------------------------------------------
  intro h‚ÇÅ
  simp only [hebb, graph_update]
  rw [map_edges_apply _ _ _ _]

  -- We have two cases; either m ‚àâ Prop(A) or n ‚àâ Prop(A).
  -- In either case, the term that we're updating by reduces 
  -- to weight + 0 = weight.
  cases h‚ÇÅ
  case inl h‚ÇÇ => 
    rw [if_neg h‚ÇÇ]
    rw [zero_mult]
    rw [commutative_mult_float]
    rw [zero_mult]
    rw [zero_plus]

  case inr h‚ÇÇ => 
    rw [if_neg h‚ÇÇ]
    rw [zero_mult]
    rw [zero_plus]

-- The weights of the new net are nondecreasing
-- (One round of Hebbian update can only increase the weights)
--------------------------------------------------------------------
theorem hebb_once_weights_le (net : BFNN) :
  net.graph.getEdgeWeight m n ‚â§ 
  (hebb net A).toNet.graph.getEdgeWeight m n := by
--------------------------------------------------------------------
  simp only [hebb, graph_update]
  sorry -- Weights are hard to reason about,
        -- because of the 'match'!  Maybe redefine getEdgeWeights!


/-‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  "Iterated"/Fixed-Point Naive Hebbian Update
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê-/

-- Takes a graph update function 'f' (e.g. graph_update for Hebb)
-- and iterates it 'no_times' times.
-- net·µ¢ and S·µ¢ are the initial inputs.
-- def iterate (f : Graph ‚Ñï Float ‚Üí Set ‚Ñï ‚Üí Graph ‚Ñï Float) 
--   (no_times : ‚Ñï) (g·µ¢ : Graph ‚Ñï Float) (S·µ¢ : Set ‚Ñï) : Graph ‚Ñï Float :=
--   match no_times with
--   | Nat.zero => g·µ¢
--   | Nat.succ k => f (iterate f k g·µ¢ S·µ¢) S·µ¢

-- We score neurons by the total sum of *negative* weights coming 
-- into them.  The neuron with the lowest score is the least likely
-- to activate (in the worst case where all of its negative signals
-- activate but none of its positive ones do).  If a neuron has
-- no negative incoming weights, we give it a score of 0.
def neg_weight_score (net : BFNN) (n : ‚Ñï) : Float :=
  sorry


-- This is the exact number of iterations of Hebbian learning
-- on 'net' and 'S' that we need to make the network unstable,
-- i.e. any activation involved within Prop(S) simply goes through.
-- 
-- This is the trickiest part to get right -- we actually need
-- to *construct* this number, based on the net's activation
-- function and the edge weights within Prop(S)!
-- 
-- We first pick the neuron with the lowest 'neg_weight_score' n_min.
-- The number of iterations is that number which would (in the worst
-- case) guarantee that n_min gets activated.
-- 
-- If n_score is n_min's score, and X is that point at which
-- our activation function is guranteed to be 1.0, and Œ∑ is the
-- learning rate, then we return
-- 
-- (X - n_score) / Œ∑   *(I think!)*
def hebb_unstable_point (net : BFNN) (S : Set ‚Ñï) : ‚Ñï :=
  sorry
  -- let x := choose net.activ_pos
  -- have h‚ÇÅ : net.activation x = 1.0 := sorry

  -- let n_min := @List.minimum (Vertex ‚Ñï Float) sorry sorry net.graph.vertices.toList
  -- let n_score := sorry
  -- sorry

lemma unstable_point_pos (net : BFNN) (S : Set ‚Ñï) :
  0 < hebb_unstable_point net S := by
  sorry


-- For every m ‚ü∂ n where m, n ‚àà Prop(S), increase the weight
-- of that edge by (no_times) * Œ∑ * act(m) * act(n).
noncomputable
def graph_update_star (net : BFNN) (g : Graph ‚Ñï Float) (S : Set ‚Ñï) (no_times : ‚Ñï) : Graph ‚Ñï Float :=
  map_edges g (fun m n weight => 
    let activ_m := if m ‚àà propagate net S then 1.0 else 0.0
    let activ_n := if n ‚àà propagate net S then 1.0 else 0.0
    weight + ((cast_float no_times) * net.rate * activ_m * activ_n))

-- Iterated Hebbian Update
-- 
-- First, note that if we iterate 'hebb' or 'graph_update' in the
-- usual way, all Prop(S) will change with each iteration.  
-- So instead, we bake the iteration into the weight update:
-- We keep adding to the weights relative to the *original* net's
-- Prop(S) values. 
-- 
-- Second, rather than iterating up to a fixed point, for unstable
-- Hebbian learning it's enough to take a specific finite number
-- of iterations.  In this case, that number is 'hebb_unstable_point'.
-- For more complicated learning algorithms, we would need to build
-- up lemmas about the fixed point of a graph.
-- 
-- FUTURE: Consider re-doing this using limits of graphs/categories
noncomputable
def hebb_star (net : BFNN) (S : Set ‚Ñï) : BFNN :=
{ net with
  graph := graph_update_star net net.graph S (hebb_unstable_point net S)
}


/-‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  Facts we will need about the unstable point
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê-/



/-‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  Properties of "Iterated" Naive Hebbian Update
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê-/

-- This lemma allows us to "lift" equational properties of hebb 
-- to hebb_star.  (This holds *because* hebb_star is the fixed point
-- of hebb.)
--------------------------------------------------------------------
theorem hebb_lift (net : BFNN) (S : Set ‚Ñï) (P : BFNN ‚Üí Œ±) : 
  (P (hebb net S) = P net)
  ‚Üí (P (hebb_star net S) = P net) := by 
--------------------------------------------------------------------
  intro h‚ÇÅ
  
  -- By induction on the unstable point of the net
  -- (we don't actually need to know what the unstable point *is*
  --  for this lemma to hold.)
  generalize hpt : (hebb_unstable_point net S) = pt
  induction pt generalizing net
  case zero => 
    -- This case is impossible; hebb_unstable_point cannot be zero!
    exact absurd hpt (ne_of_gt (unstable_point_pos _ _))

  case succ k IH =>
    -- simp only [hebb] at h‚ÇÅ
    simp only [hebb_star]
    rw [hpt]
    -- simp only [graph_update] at h‚ÇÅ
    simp only [graph_update_star]

    -- convert h‚ÇÅ using 9
    -- convert h‚ÇÅ using 3
    -- rename_i x‚ÇÅ x‚ÇÇ w

    sorry
    


-- Hebbian update hebb_star preserves the acyclicness of the graph.
-- (So the updated net is still acyclic.)
-- [LIFTED from hebb_once_acyclic]
--------------------------------------------------------------------
lemma hebb_acyclic (net : BFNN) (S : Set ‚Ñï) : 
  (hebb_star net S).graph.is_acyclic = net.graph.is_acyclic := by
--------------------------------------------------------------------
  exact hebb_lift _ _ (fun x => x.graph.is_acyclic) (hebb_once_acyclic _ _)

-- Hebbian update hebb_star does not affect the vertices of the graph.
--------------------------------------------------------------------
theorem hebb_vertices (net : BFNN) (S : Set ‚Ñï) : 
  (hebb_star net S).graph.get_vertices = net.graph.get_vertices := by
--------------------------------------------------------------------
  exact hebb_lift _ _ (fun x => x.graph.get_vertices) (hebb_once_vertices _ _)

-- Hebbian update hebb_star does not affect the predecessors
-- of any node.
-- [LIFTED from hebb_once_preds]
--------------------------------------------------------------------
theorem hebb_preds (net : BFNN) (S : Set ‚Ñï) : 
  preds (hebb_star net S) n = preds net n := by
--------------------------------------------------------------------
  exact hebb_lift _ _ (fun x => preds x n) (hebb_once_preds _ _ _)
  
-- Hebbian update hebb_star does not affect which neurons are
-- on which layer of the net.
-- [LIFTED from hebb_once_layers]
--------------------------------------------------------------------
theorem hebb_layers (net : BFNN) (S : Set ‚Ñï) : 
  layer (hebb_star net S) n = layer net n := by
--------------------------------------------------------------------
  exact hebb_lift _ _ (fun x => layer x n) (hebb_once_layers _ _)

-- Hebbian update hebb_star does not affect the activation function.
-- [LIFTED from hebb_once_activation]
--------------------------------------------------------------------
theorem hebb_activation (net : BFNN) (S : Set ‚Ñï) : 
  (hebb_star net S).activation = net.activation := by 
--------------------------------------------------------------------
  exact hebb_lift _ _ (fun x => x.activation) (hebb_once_activation _ _)

-- Hebbian update hebb_star does not affect graph reachability
-- (It only affects the edge weights)
-- -- [LIFTED from hebb_once_reach]
--------------------------------------------------------------------
theorem hebb_reach (net : BFNN) (A B : Set ‚Ñï) : 
  reachable (hebb_star net A) A B = 
    reachable net A B := by 
--------------------------------------------------------------------
  exact hebb_lift _ _ (fun x => reachable x A B) (hebb_once_reach _ _ _)


-- If m ‚àâ Prop(A) or n ‚àâ Prop(A), then the edge m ‚ü∂ n was not
-- increased by Hebbian update.  So its weight in the updated
-- net is the same as its weight in the original net.
-- Lifted from hebb_once_weights
--------------------------------------------------------------------
theorem hebb_weights (net : BFNN) : 
  (m ‚àâ propagate net A ‚à® n ‚àâ propagate net A)
  ‚Üí ((hebb_star net A).toNet.graph.getEdgeWeight m n =
    net.graph.getEdgeWeight m n) := by
--------------------------------------------------------------------
  intro h‚ÇÅ
  exact hebb_lift _ _ (fun x => x.toNet.graph.getEdgeWeight m n) 
    (hebb_once_weights _ h‚ÇÅ)

 
-- The weights of the new net are nondecreasing
-- (Hebbian update can only increase the weights)
-- Note that we *cannot* lift this property straightforwardly,
-- since it's an inequality.
--------------------------------------------------------------------
theorem hebb_weights_le (net : BFNN) :
  net.graph.getEdgeWeight m n ‚â§ 
  (hebb_star net A).toNet.graph.getEdgeWeight m n := by
--------------------------------------------------------------------
  simp only [hebb_star]
  generalize hpt : (hebb_unstable_point net A) = pt
  
  -- By induction on the unstable point of the net
  induction pt
  case zero => 
    -- This case is impossible; hebb_unstable_point cannot be zero!
    exact absurd hpt (ne_of_gt (unstable_point_pos _ _))
  case succ k IH => 
    simp only [graph_update_star]
    sorry -- graph weights are hard to reason about
          -- because of the 'match'!
          -- there's got to be an easier way!


/-‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  The more interesting/crucial properties
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê-/

-- The lemma for 'activ' and 'hebb', essentially
-- activ net prev_activ n  ‚ü∂  activ (hebb_star net A) prev_activ n
--------------------------------------------------------------------
lemma hebb_activ (net : BFNN) (A B : Set ‚Ñï) (n : ‚Ñï) :
  activ net (List.map (fun i => 
      let m := (preds net n).get! i
      if B m then 1.0 else 0.0) 
        (List.range (preds net n).length)) n
  ‚Üí activ (hebb_star net A) (List.map (fun i => 
      let m := (preds net n).get! i
      if B m then 1.0 else 0.0) 
        (List.range (preds net n).length)) n := by
--------------------------------------------------------------------
  simp only [activ]
  rw [hebb_preds]
  
  apply activation_from_inequality _ _ _
  apply net.activ_nondecr _ _
  apply weighted_sum_le _ _ _ _
  intro i
  
  -- We split by cases; either m ‚àâ B in the original net,
  -- and both sides reduce to 0.0;
  -- or m ‚àà B, in which case we check that the weight for
  -- m ‚ü∂ n in the original net is ‚â§ the updated net weight.  
  generalize hm : (List.get! (preds net n) i) = m
  by_cases (B m)
  case neg => 
    rw [if_neg h]
    rw [zero_mult]
    rw [zero_mult]
    exact zero_le_zero
  case pos => 
    rw [if_pos h]
    rw [one_mult]
    rw [one_mult]
    exact hebb_weights_le _

-- If n ‚àâ Prop(A), then activ (hebb_star net A) _ n = activ net _ n.
--------------------------------------------------------------------
theorem simp_hebb_activ‚ÇÅ (net : BFNN) (A : Set ‚Ñï) (prev_activ : List Float) :
  n ‚àâ propagate net A
  ‚Üí activ (hebb_star net A) prev_activ n = activ net prev_activ n := by
--------------------------------------------------------------------
  intro h‚ÇÅ
  simp only [activ]
  rw [hebb_activation net A]
  rw [hebb_preds net A]
  conv =>
    lhs; enter [1, 2, 1, 1, i]
    rw [hebb_weights _ (Or.inr h‚ÇÅ)]


-- If every *active* predecessor of n ‚àâ Prop(A), then
-- activ (hebb_star net A) _ n = activ net _ n.
-- Like activ_agree, we have to simplify the statement of this
-- lemma in order for Lean to be able to infer types efficiently.
--------------------------------------------------------------------
theorem simp_hebb_activ‚ÇÇ (net : BFNN) (A B : Set ‚Ñï) :
  (‚àÄ x, x ‚àà (preds net n) ‚Üí x ‚àâ (propagate net A) ‚à© (propagate net B))

  ‚Üí (activ (hebb_star net A) (List.map (fun i => 
      if propagate_acc net B ((Graph.predecessors net.toNet.graph n).get! i) 
        (layer net ((Graph.predecessors net.toNet.graph n).get! i)) 
      then 1.0 else 0.0) 
        (List.range (Graph.predecessors net.toNet.graph n).length)) n
  ‚Üî activ net (List.map (fun i =>
      if propagate_acc net B ((Graph.predecessors net.toNet.graph n).get! i) 
        (layer net ((Graph.predecessors net.toNet.graph n).get! i)) 
      then 1.0 else 0.0) 
        (List.range (Graph.predecessors net.toNet.graph n).length)) n) := by
--------------------------------------------------------------------
  intro h‚ÇÅ
  apply Eq.to_iff
  
  -- Do simplifications to get to the weighted sum
  simp only [activ]
  rw [hebb_activation net A]
  rw [hebb_preds net A]
  apply congr_arg (fun x => Net.activation net.toNet x = 1.0)

  -- The weighted sums are equal, ‚àë w‚ÇÅ x‚ÇÅ = ‚àë w‚ÇÇ x‚ÇÇ,
  -- if all of their entries are equal, w‚ÇÅ·µ¢ * x‚ÇÅ·µ¢ = w‚ÇÇ·µ¢ * x‚ÇÇ·µ¢
  apply weighted_sum_eq
  intro i
  generalize hm : List.get! (Graph.predecessors net.toNet.graph n) i = m

  -- We have two cases;
  by_cases m ‚àà propagate net B
  
  -- m ‚àà Prop(B ‚à™ Reach(Prop(A), Prop(B)))
  -- In this case, the RHS's reduce to 1.0, and we
  -- just need to argue that the weights are the same
  case pos => 
    -- First, notice that m ‚àâ Prop(A).
    have h‚ÇÇ : m ‚àà preds net n := by
      rw [‚Üê hm]
      exact get!_mem _ _
    have h‚ÇÉ : m ‚àâ propagate net A := by
      simp only [Inter.inter, Set.inter, Membership.mem, Set.Mem, setOf] at h‚ÇÅ
      conv at h‚ÇÅ => enter [x, hx]; rw [not_and']
      exact h‚ÇÅ m h‚ÇÇ h
      
    -- Now we simplify and show that the weights are the same
    simp only [propagate, Membership.mem, Set.Mem] at h
    rw [if_pos h]
    rw [one_mult]
    rw [one_mult]
    exact hebb_weights _ (Or.inl h‚ÇÉ)

  -- Otherwise, the RHS's reduce to 0.0, and so the
  -- weighted sums are trivially equal
  case neg => 
    simp only [propagate, Membership.mem, Set.Mem] at h
    rw [if_neg h]
    rw [zero_mult]
    rw [zero_mult]


-- -- If *some* predecessor of n is ‚àà Prop(A), and n ‚àà Prop(A), then
-- -- if m is activated in (hebb_star net) then n is too
-- -- (the activation automatically goes through in (hebb_star net))
--------------------------------------------------------------------
theorem simp_hebb_activ‚ÇÉ (net : BFNN) (A B : Set ‚Ñï) :
  let preds := preds net n
  let prev_activ := List.map (fun i => 
    let m := preds.get! i
    if propagate_acc (hebb_star net A) B m (layer (hebb_star net A) m) 
    then 1.0 else 0.0) 
      (List.range preds.length)

  n ‚àà propagate net A
  ‚Üí m ‚àà preds ‚àß m ‚àà propagate net A  
  ‚Üí m ‚àà propagate (hebb_star net A) B
  ‚Üí activ (hebb_star net A) prev_activ n := by
--------------------------------------------------------------------
  intro preds
  intro prev_activ
  intro h‚ÇÅ
  intro h‚ÇÇ
  intro h‚ÇÉ

  simp only [activ]
  rw [hebb_activation net A]
  rw [hebb_preds net A]
  
  sorry -- I have the proof written on paper, I should consult that.
        -- Depends on things like the monotonicity of 'activation', etc.


-- If there is a path within Prop(A) from B to n, then, since this
-- path is updated in hebb_star, n ‚àà Prop(B).
--------------------------------------------------------------------
theorem hebb_updated_path (net : BFNN) (A B : Set ‚Ñï) :
  reachable net (propagate net A) B
  ‚äÜ propagate (hebb_star net A) B := by
--------------------------------------------------------------------
  intro (n : ‚Ñï)
  intro h‚ÇÅ
  
  -- By induction on the layer of the net containing n
  generalize hL : layer net n = L
  induction L using Nat.case_strong_induction_on generalizing n

  --------------------------------
  -- Base Step
  --------------------------------
  -- Easy; at layer zero, show that B = B.
  case hz =>
    have h‚ÇÇ : n ‚àà B :=
      reach_layer_zero _ _ _ _ hL h‚ÇÅ

    simp only [Membership.mem, Set.Mem, propagate]
    -- simp only [Membership.mem, Set.Mem, propagate] at h‚ÇÇ
    rw [hebb_layers net A]
    rw [hL]
    -- rw [hL] at h‚ÇÇ
    simp only [propagate_acc]
    simp only [propagate_acc] at h‚ÇÇ
    exact h‚ÇÇ
    
  --------------------------------
  -- Inductive Step
  --------------------------------
  case hi L IH‚ÇÅ => 
    -- We have a path from Prop(B) to n, going through Prop(A).
    match h‚ÇÅ with
    | ‚ü®m, hm‚ü© => 
      -- By induction on the length of this path
      induction hm.2

      case trivial _ => exact propagate_is_extens _ _ hm.1
      case from_path x y path_mx edge_xy hy _ => 
        -- Split by whether y ‚àà B in order to simplify propagate_acc
        by_cases y ‚àà B
        case pos => exact propagate_is_extens _ _ h
        case neg =>
          -- Simplifications and rewriting
          simp only [propagate, Membership.mem, Set.Mem]
          rw [hebb_layers]
          rw [hL]
          rw [simp_propagate_acc _ h]
          rw [hebb_preds]
          simp

          -- Apply our inductive hypothesis: x ‚àà Prop(B) in (hebb_star net) 
          have hpreds : x ‚àà preds net y := (edge_from_preds _ _ _).mpr edge_xy
          have hpred_dec : layer net x ‚â§ L := 
            (Nat.lt_succ).mp (lt_of_lt_of_eq (preds_decreasing _ _ _ hpreds) hL)
          have hx_reachable : x ‚àà reachable net (propagate net A) B :=
            ‚ü®m, ‚ü®hm.1, path_mx‚ü©‚ü©
          have hx_propA : x ‚àà propagate net A := 
            reach_subset _ _ _ hx_reachable
          have hx_propB : x ‚àà propagate (hebb_star net A) B := 
            IH‚ÇÅ (layer net x) hpred_dec hx_reachable rfl
          
          -- Apply simp_hebb_activ‚ÇÉ, which says:
          --  x, y ‚àà Prop(A)
          --  There's an edge from x ‚ü∂ y
          --  x ‚àà Prop(B) in (hebb_star net)
          --  -------------------------------
          --  y is activ in hebb_star net
          exact simp_hebb_activ‚ÇÉ net A B hy ‚ü®hpreds, hx_propA‚ü© hx_propB


/-‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  Reduction for Unstable Hebbian Update
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê-/

-- These two are the big theorems.
-- They explain what hebb_star learns in terms of what the net
-- believed *before* update -- it turns out that we can completely
-- reduce the dynamic behavior to the static behavior.
--------------------------------------------------------------------
theorem hebb_reduction_empty (net : BFNN) (A B : Set ‚Ñï) : 
  propagate net A ‚à© propagate net B = ‚àÖ ‚Üí

  propagate (hebb_star net A) B = propagate net B := by 
--------------------------------------------------------------------
  intro h_empty
  apply ext
  intro (n : ‚Ñï)

  -- By induction on the layer of the net containing n
  generalize hL : layer net n = L
  induction L using Nat.case_strong_induction_on generalizing n

  --------------------------------
  -- Base Step
  --------------------------------
  -- Easy; after simplifying, show that B = B.
  case hz =>
    simp only [Membership.mem, Set.Mem, propagate]
    rw [hebb_layers net A]
    rw [hL]
    simp only [propagate_acc]

  --------------------------------
  -- Inductive Step
  --------------------------------
  case hi L IH => 
    -- Both directions are similar
    apply Iff.intro
    case mp =>
      intro h‚ÇÅ

      -- Split by whether n ‚àà B, in order to simplify propagate_acc
      by_cases n ‚àà B
      case pos => exact propagate_is_extens _ _ h
      case neg =>
        -- Proof Idea:
        -- Since n ‚àà Prop(B) in (hebb_star net),
        -- its active predecessors m are ‚àà Prop(B) in (hebb_star net).
        -- But by IH these m ‚àà Prop(B) in the original net.
        -- Since Prop(A) ‚à© Prop(B) is empty, these m ‚àâ Prop(A).
        -- By [simp_hebb_activ‚ÇÇ], Prop(B) is the same in both nets.
        
        -- First, we show that any *active* predecessor of n 
        -- cannot be in Prop(A). (If it were, then it would
        -- be in both Prop(B) and Prop(A) -- but the intersection
        -- is empty!)
        have h‚ÇÇ : (‚àÄ x, x ‚àà preds net n ‚Üí x ‚àâ propagate net A ‚à© propagate net B) := by
          intro x _ contr_assump
          exact absurd h_empty (Set.Nonempty.ne_empty 
            (Set.nonempty_of_mem contr_assump))
          
        -- Simplifications and rewriting, to prepare for IH
        simp only [propagate, Membership.mem, Set.Mem]
        simp only [propagate, Membership.mem, Set.Mem] at h‚ÇÅ
        simp only [propagate, Membership.mem, Set.Mem] at IH
        rw [hebb_layers] at h‚ÇÅ
        rw [hL] at h‚ÇÅ
        rw [hL]
        rw [simp_propagate_acc _ h]
        rw [simp_propagate_acc _ h] at h‚ÇÅ
        rw [hebb_preds net A] at h‚ÇÅ
        simp
        simp at h‚ÇÅ
        
        -- Get ready to apply IH
        -- We write down the usual lemmas for 'm', but we don't
        -- know what the index 'i' is we're grabbing yet.  So
        -- we write these for all i.
        generalize hm : List.get! (Graph.predecessors net.toNet.graph n) = m
        have h‚ÇÉ : ‚àÄ i, (m i) ‚àà preds net n := by
          intro i
          rw [symm hm]
          simp [preds]
          exact get!_mem (net.graph.predecessors n) i

        have h‚ÇÑ : ‚àÄ i, (layer net (m i)) ‚â§ L := by
          intro i
          apply Nat.lt_succ.mp
          rw [symm hL]
          exact preds_decreasing net (m i) n (h‚ÇÉ i)

        -- Go into h‚ÇÅ and apply our inductive hypothesis
        conv at h‚ÇÅ =>
          enter [2, 1, i]
          -- enter [2, 2, i, 1]
          rw [hm]
          rw [IH (layer net (m i)) (h‚ÇÑ i) (m i) rfl]
        
        -- Unpack the (m i) term
        rw [‚Üê hm] at h‚ÇÅ
        rw [‚Üê hm]

        -- Use simp_hebb_activ‚ÇÇ, which says that if all
        -- of the *active* preds ‚àâ Prop(A), the activ's are the same.
        rw [simp_hebb_activ‚ÇÇ _ _ B h‚ÇÇ] at h‚ÇÅ
        exact h‚ÇÅ

    -- Do exactly what we did for the other direction, except
    -- we apply hebb_activ rather than simp_hebb_activ‚ÇÇ at the end.
    case mpr =>
      intro h‚ÇÅ

      -- Split by whether n ‚àà B, in order to simplify propagate_acc
      by_cases n ‚àà B
      case pos => exact propagate_is_extens _ _ h
      case neg =>
        -- Proof Idea:
        -- Since n ‚àà Prop(B) in (hebb_star net),
        -- its active predecessors m are ‚àà Prop(B) in (hebb_star net).
        -- But by IH these m ‚àà Prop(B) in the original net.
        -- Since Prop(A) ‚à© Prop(B) is empty, these m ‚àâ Prop(A).
        -- By [simp_hebb_activ‚ÇÇ], Prop(B) is the same in both nets.
        
        -- First, we show that any *active* predecessor of n 
        -- cannot be in Prop(A). (If it were, then it would
        -- be in both Prop(B) and Prop(A) -- but the intersection
        -- is empty!)
        have h‚ÇÇ : (‚àÄ x, x ‚àà preds net n ‚Üí x ‚àâ propagate net A ‚à© propagate net B) := by
          intro x _ contr_assump
          exact absurd h_empty (Set.Nonempty.ne_empty 
            (Set.nonempty_of_mem contr_assump))
          
        -- Simplifications and rewriting, to prepare for IH
        simp only [propagate, Membership.mem, Set.Mem]
        simp only [propagate, Membership.mem, Set.Mem] at h‚ÇÅ
        simp only [propagate, Membership.mem, Set.Mem] at IH
        rw [hebb_layers]
        rw [hL]
        rw [hL] at h‚ÇÅ
        rw [simp_propagate_acc _ h]
        rw [simp_propagate_acc _ h] at h‚ÇÅ
        rw [hebb_preds net A]
        simp
        simp at h‚ÇÅ
        
        -- Get ready to apply IH
        -- We write down the usual lemmas for 'm', but we don't
        -- know what the index 'i' is we're grabbing yet.  So
        -- we write these for all i.
        generalize hm : List.get! (Graph.predecessors net.toNet.graph n) = m
        have h‚ÇÉ : ‚àÄ i, (m i) ‚àà preds net n := by
          intro i
          rw [symm hm]
          simp [preds]
          exact get!_mem (net.graph.predecessors n) i

        have h‚ÇÑ : ‚àÄ i, (layer net (m i)) ‚â§ L := by
          intro i
          apply Nat.lt_succ.mp
          rw [symm hL]
          exact preds_decreasing net (m i) n (h‚ÇÉ i)

        -- Go into h‚ÇÅ and apply our inductive hypothesis
        conv at h‚ÇÅ =>
          enter [2, 1, i]
          -- enter [2, 2, i, 1]
          rw [hm]
          rw [‚Üê IH (layer net (m i)) (h‚ÇÑ i) (m i) rfl]
        
        -- Unpack the (m i) term
        rw [‚Üê hm] at h‚ÇÅ
        rw [‚Üê hm]

        -- Use simp_hebb_activ‚ÇÇ, which says that if all
        -- of the *active* preds ‚àâ Prop(A), the activ's are the same.
        let S := fun m => propagate_acc (hebb_star net A) B m (layer (hebb_star net A) m)
        exact hebb_activ net A S n h‚ÇÅ

--------------------------------------------------------------------
theorem hebb_reduction_nonempty (net : BFNN) (A B : Set ‚Ñï) : 
  propagate net A ‚à© propagate net B ‚â† ‚àÖ ‚Üí

  propagate (hebb_star net A) B =
  propagate net (B ‚à™ reachable net (propagate net A) B) := by 
--------------------------------------------------------------------
  intro h_nonempty
  apply ext
  intro (n : ‚Ñï)
  simp only [Membership.mem, Set.Mem, propagate]
  
  -- By induction on the layer of the net containing n
  generalize hL : layer net n = L
  induction L using Nat.case_strong_induction_on generalizing n

  --------------------------------
  -- Base Step
  --------------------------------
  case hz => 
    -- First, do the base case simplifications
    rw [hebb_layers]
    rw [hL]
    simp only [propagate_acc]
    simp only [Union.union, Set.union, Membership.mem, Set.Mem, setOf]

    -- Forward direction is the easy part, so we do it first.
    -- Backwards direction relies on reach_layer_zero,
    -- a fact about paths when we know n is at layer 0.
    apply Iff.intro
    case mp => exact fun h‚ÇÅ => Or.inl h‚ÇÅ
    case mpr => 
      intro h‚ÇÅ

      -- Either n ‚àà B or n is reachable from Prop(B) using only
      -- paths within Prop(A).  At layer 0, this means n ‚àà B.
      cases h‚ÇÅ
      case inl h‚ÇÇ => exact h‚ÇÇ
      case inr h‚ÇÇ => 
        have heq : layer net n = 0 := 
          Eq.trans (symm (hebb_layers net A)) (Eq.trans (hebb_layers _ _) hL)
        exact reach_layer_zero _ _ _ _ heq h‚ÇÇ

  --------------------------------
  -- Inductive Step
  --------------------------------
  case hi L IH => 
    apply Iff.intro
    
    ---------------------
    -- Backward Direction
    ---------------------
    case mpr =>
      intro h‚ÇÅ
      
      -- By cases; either n ‚àà B ‚à™ Reach(Prop(A), Prop(B)), or not.
      by_cases n ‚àà B ‚à™ reachable net (propagate net A) B
      case pos =>
        -- From here, we split again; either:
        --    1. n ‚àà B, and by extens n ‚àà Prop(B) in (hebb_star net)
        --    2. n ‚àà Reach(Prop(A), Prop(B)).  In this case, this path
        --       has been updated in the (hebb_star net), so of course
        --       n ‚àà Prop(B) in (hebb_star_net)
        --       i.e. apply [hebb_updated_path]! 
        cases h
        case inl h‚ÇÇ => exact propagate_acc_is_extens _ _ h‚ÇÇ
        case inr h‚ÇÇ =>
          have h‚ÇÉ : n ‚àà propagate (hebb_star net A) B := 
            hebb_updated_path _ _ _ h‚ÇÇ
          simp only [propagate, Membership.mem, Set.Mem] at h‚ÇÉ
          exact h‚ÇÉ

      case neg =>
        -- We get ready to simplify propagate_acc
        have n_not_in_B : n ‚àâ B :=
          fun n_in_B => absurd (Set.mem_union_left _ n_in_B) h

        -- Simplifications and rewriting, to prepare for IH
        -- We also rewrite the 'preds', 'layers', and 'activ'
        -- for (hebb_star net) in terms of the original 'net'.
        rw [hebb_layers]
        rw [hL]
        simp only [propagate] at h
        rw [simp_propagate_acc _ n_not_in_B]
        rw [simp_propagate_acc _ h] at h‚ÇÅ
        rw [hebb_preds net A] -- rewrite 'preds'
        
        -- The plan is to now apply our inductive hypothesis
        -- and use the 'activ_agree' lemmas.
        -- We write the two sets as S‚ÇÅ and S‚ÇÇ for convenience 
        let S‚ÇÅ := fun m => 
          propagate_acc net (B ‚à™ reachable net (fun n => 
            propagate_acc net A n (layer net n)) B)
            m (layer net m)
        let S‚ÇÇ := fun m => propagate_acc (hebb_star net A) B m (layer (hebb_star net A) m)

        -- Apply IH to the predecessors
        have h‚ÇÇ : ‚àÄ (m : ‚Ñï), m ‚àà preds net n ‚Üí (S‚ÇÅ m ‚Üî S‚ÇÇ m) := by
          simp only [Membership.mem] -- really just want to unfold the let
          intro m hm
          generalize hLm : layer net m = Lm
          have h‚ÇÉ : Lm ‚â§ L := by
            rw [symm hLm]
            apply Nat.lt_succ.mp
            rw [symm hL]
            exact preds_decreasing net m n hm
          exact (symm (IH Lm h‚ÇÉ m hLm).to_eq).to_iff

        -- Argument: 
        -- By activ_agree, the predecessors m ‚àà Prop(B) (over hebb_star)
        --   activate n *in the original net*
        -- But the weights in the updated net are either the same or
        --   increasing, so by hebb_activ, these same predecessors
        --   activate n *in the updated net*.
        simp
        simp at h‚ÇÅ
        exact hebb_activ net A S‚ÇÇ _
          (activ_agree _ S‚ÇÅ S‚ÇÇ _ h‚ÇÇ h‚ÇÅ)

    ---------------------
    -- Forward Direction
    -- (similar, but differs slightly in Case 1)
    ---------------------
    -- This direction is a bit more tricky. This
    -- is where we rely on the net being fully
    -- connected & transitively closed.
    case mp => 
      intro h‚ÇÅ
      
      -- By cases; either n ‚àà B ‚à™ reachable net (propagate net A) B, 
      -- or not.
      by_cases n ‚àà B ‚à™ reachable net (propagate net A) B
      case pos => 
        -- In this case, just apply propagate_is_extens
        rw [‚Üê hL]
        simp only [propagate] at h
        exact propagate_acc_is_extens _ _ h

      case neg h‚ÇÇ => 
        -- From here, we split *again*, depending on whether n ‚àà Prop(A).
        by_cases n ‚àà propagate net A

        ---------------------
        -- Case 1: n ‚àà Prop(A)
        ---------------------
        case pos => 
          -- Since Prop(A) ‚à© B is nonempty, and
          -- Prop(A) ‚à© B ‚äÜ Prop(A) ‚à© Prop(B ‚à™ Reach(Prop(A), B)) 
          --                   = S   (we abbreviate the set for convenience)
          -- there is an m in the latter set.
          -- 
          -- NOTE: This is the heart of the proof!  We have to use the
          -- Reach-Prop interaction property here (that's very interesting)!
          generalize hS : (propagate net A) ‚à© (propagate net (B ‚à™ reachable net (propagate net A) B)) = S
          have h_nonemptyS : Set.Nonempty S := by 
            rw [‚Üê hS]
            rw [‚Üê Set.nonempty_iff_ne_empty] at h_nonempty 
            exact match h_nonempty with
            | ‚ü®m, hm‚ü© => ‚ü®m, ‚ü®hm.1, propagate_is_extens _ _ 
              (Or.inr (reach_propagate _ _ _ (reach_is_extens _ _ _ hm)))‚ü©‚ü©

          -- By the well-ordering principle, let m be the node
          -- in this set with the *smallest layer*
          let m := WellFounded.min (layer_wellfounded net) S h_nonemptyS

          -- This m is both in the set, and is the smallest such m.
          have hm : m ‚àà S := WellFounded.min_mem _ _ _
          have h‚ÇÉ : ‚àÄ x, x ‚àà S ‚Üí layer net m ‚â§ layer net x := 
            fun x hx => le_of_not_le 
              (WellFounded.not_lt_min (layer_wellfounded net) _ _ hx) 
          
          -- We don't know whether n ‚àà S yet,
          -- but we do know that either:
          --    - layer(m) < layer(n), or
          --    - layer(m) ‚â• layer(n).
          -- So we split into these two cases.
          cases lt_or_ge (layer net m) (layer net n)

          ---------------------
          -- Case 1.1: n ‚àà Prop(A)
          -- and layer[m] < layer[n]
          ---------------------
          -- Since the net is transitively closed, we have an
          -- edge from m to n.  Since
          --     m ‚àà S = Prop(A) ‚à© Prop(B ‚à™ Reach(Prop(A), B))
          -- and n ‚àà Prop(A),
          -- we get the unfortunate expression
          --     n ‚àà Reach(Prop(A), Prop(B ‚à™ Reach(Prop(A), B))).
          -- The trick from here is to show, by Reach-Prop algebra:
          --     n ‚àà Reach(Prop(A), B),
          -- and so
          --     n ‚àà Prop(B ‚à™ Reach(Prop(A), B))
          --
          -- This is where the real action of the proof happens. 
          case inl h‚ÇÑ => 
            -- We apply the fact that the net is fully connected to get an
            -- edge m‚ü∂n.
            have h‚ÇÖ : Graph.hasEdge net.toNet.graph m n := by
              exact connected _ m n h‚ÇÑ

            -- So we have n ‚àà Reach(Prop(A), Prop(B ‚à™ Reach(Prop(A), B)))
            -- (the unfortunately ugly expression)
            have h‚ÇÜ : n ‚àà reachable net (propagate net A) 
              (propagate net (B ‚à™ reachable net (propagate net A) B)) := by
              rw [‚Üê hS] at hm
              exact ‚ü®m, ‚ü®hm.2, 
                focusedPath.from_path (focusedPath.trivial hm.1) h‚ÇÖ h‚ü©‚ü©

            -- By algebra, this reduces to n ‚àà Reach(Prop(A), B)
            -- TODO: Can I clean up the congr_arg parts?  What extensionality
            -- lemma do I need to use here???        
            have h‚Çá : reachable net (propagate net A) B ‚äÜ 
                      reachable net (propagate net A) 
                        (reachable net (propagate net A) B) := by
              intro x hx
              rw [‚Üê reach_is_idempotent _ _ _]
              exact hx

            have h‚Çà : n ‚àà reachable net (propagate net A) B := by
              apply (congr_arg (fun X => n ‚àà X) (reach_is_idempotent _ _ _)).mpr
              apply (congr_arg (fun X => n ‚àà X) (Set.union_eq_right_iff_subset.mpr h‚Çá)).mp
              apply (congr_arg (fun X => n ‚àà X) (reach_union _ _ _ _)).mp
              exact reach_propagate _ _ _ h‚ÇÜ

            -- Since                 n ‚àà Reach(Prop(A), B),
            -- We have      n ‚àà Prop(B ‚à™ Reach(Prop(A), B))
            simp only [propagate] at h‚Çà
            rw [‚Üê hL]
            exact propagate_acc_is_extens net _ (Set.mem_union_right _ h‚Çà)

          ---------------------
          -- Case 1.2: n ‚àà Prop(A)
          -- and layer[m] ‚â• layer[n]
          ---------------------
          -- Since m ‚àà S is the node with the *smallest layer*,
          -- this means *none* of n's predecessors are ‚àà S.
          -- But S = Prop(A) ‚à© Prop(B ‚à™ Reach(Prop(A), B)),
          -- which means that none of n's active predecessors
          -- (i.e. ‚àà Prop(B ‚à™ Reach(Prop(A), B)) by IH)
          -- are in Prop(A).
          -- 
          -- i.e. n's preceding edges were not updated,
          -- and so the activ's are the same.
          case inr h‚ÇÑ => 
            -- First, we show that any predecessor of n cannot be in S.
            -- (if it was in S, it's layer would be smaller than m's)
            have h‚ÇÖ : ‚àÄ x, x ‚àà preds net n ‚Üí x ‚àâ S := by
              exact fun x hx x_in_S =>
                have m_smaller : layer net m ‚â§ layer net x := (h‚ÇÉ x x_in_S)
                have m_bigger : layer net m > layer net x := 
                  (gt_of_ge_of_gt h‚ÇÑ (preds_decreasing _ _ _ hx)) 
                absurd m_smaller (not_le_of_gt m_bigger)

            -- We get ready to simplify propagate_acc
            rename_i n_not_in_reach
            have n_not_in_B : n ‚àâ B :=
              fun n_in_B => absurd (Set.mem_union_left _ n_in_B) n_not_in_reach

            -- Simplifications and rewriting, to prepare for IH
            -- We also rewrite the 'preds', 'layers', and 'activ'
            -- for (hebb_star net) in terms of the original 'net'.
            rw [hebb_layers] at h‚ÇÅ
            rw [hL] at h‚ÇÅ
            simp only [propagate] at n_not_in_reach
            rw [simp_propagate_acc _ n_not_in_B] at h‚ÇÅ
            rw [simp_propagate_acc _ n_not_in_reach]
            rw [‚Üê hS] at h‚ÇÖ
            rw [hebb_preds net A] at h‚ÇÅ -- rewrite 'preds'
            simp
            simp at h‚ÇÅ

            -- Get ready to apply IH
            -- We write down the usual lemmas for 'm', but we don't
            -- know what the index 'i' is we're grabbing yet.  So
            -- we write these for all i.
            generalize hm : List.get! (Graph.predecessors net.toNet.graph n) = m
            have h‚ÇÜ : ‚àÄ i, (m i) ‚àà preds net n := by
              intro i
              rw [symm hm]
              simp [preds]
              exact get!_mem (net.graph.predecessors n) i

            have h‚Çá : ‚àÄ i, (layer net (m i)) ‚â§ L := by
              intro i
              apply Nat.lt_succ.mp
              rw [symm hL]
              exact preds_decreasing net (m i) n (h‚ÇÜ i)

            -- Go into h‚ÇÅ and apply our inductive hypothesis
            conv at h‚ÇÅ =>
              enter [2, 1, i]
              rw [hm]
              rw [IH (layer net (m i)) (h‚Çá i) (m i) rfl]
            
            -- Unpack the (m i) term
            rw [‚Üê hm] at h‚ÇÅ
            rw [‚Üê hm]

            -- Use simp_hebb_activ‚ÇÇ, which says that if all
            -- of the *active* preds ‚àâ Prop(A), the activ's are the same.
            let B‚ÇÇ := (B ‚à™ reachable net (fun n => 
              propagate_acc net A n (layer net n)) B)
            rw [simp_hebb_activ‚ÇÇ _ _ B‚ÇÇ h‚ÇÖ] at h‚ÇÅ
            exact h‚ÇÅ
            
        ---------------------
        -- Case 2: n ‚àâ Prop(A)
        ---------------------
        -- In this case, the activ's are the same, so we can
        -- straightforwardly apply our inductive hypothesis.
        case neg =>
          -- We get ready to simplify propagate_acc
          rename_i n_not_in_reach
          have n_not_in_B : n ‚àâ B :=
            fun n_in_B => absurd (Set.mem_union_left _ n_in_B) n_not_in_reach

          -- Simplifications and rewriting, to prepare for IH
          -- We also rewrite the 'preds', 'layers', and 'activ'
          -- for (hebb_star net) in terms of the original 'net'.
          rw [hebb_layers] at h‚ÇÅ
          rw [hL] at h‚ÇÅ
          simp only [propagate] at n_not_in_reach
          rw [simp_propagate_acc _ n_not_in_B] at h‚ÇÅ
          rw [simp_propagate_acc _ n_not_in_reach]
          
          rw [simp_hebb_activ‚ÇÅ _ _ _ h] at h‚ÇÅ -- rewrite 'activ'
          rw [hebb_preds net A] at h‚ÇÅ -- rewrite 'preds'
          conv at h‚ÇÅ => -- rewrite 'layers'
            enter [2, 1, i, m]
            rw [hebb_layers net A]
          simp
          simp at h‚ÇÅ
          convert h‚ÇÅ using 4
          rename_i i
          generalize hm : List.get! (net.graph.predecessors n) i = m
          generalize hLm : layer net m = Lm
          conv at IH => -- rewrite 'layers' in IH
            enter [L, hL, m, hLm, 1]
            rw [hebb_layers]
            rw [hLm]

          -- We are now ready to apply our inductive hypothesis!
          have h‚ÇÇ : m ‚àà preds net n := by
            rw [symm hm]
            simp [preds]
            exact get!_mem (net.graph.predecessors n) i
          have h‚ÇÉ : Lm ‚â§ L := by
            rw [symm hLm]
            apply Nat.lt_succ.mp
            rw [symm hL]
            exact preds_decreasing net m n h‚ÇÇ
          exact (symm (IH Lm h‚ÇÉ m hLm).to_eq).to_iff
          

-- TODO: Prove that we can unravel these nets into ordinary
-- feedforward nets
-- 
-- TODO: Email David Sprunger about follow-up papers to
-- "backprop as a functor"

/-‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  The Logic (Language and Semantics)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê-/



/-‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  Inference Rules and Proof System
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê-/




/-‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  Soundness
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê-/





-- propagate_N (S) = propagate_hebb(N, S) (S)
-- This essentially says that repeated hebbian update
-- is well-defined; *after* updating on S, we can update
-- on S again and increase weights within the same propagation.
-- (The propagation of S doesn't suddenly change, which is
--  something we might be worried about.)
-- TODO: Not sure if I need this anymore!
-- It's somewhat interesting, but might not help with the
-- reduction.
-- --------------------------------------------------------------------
-- theorem hebb_iteration_is_well_defined (net : BFNN) (S : Set ‚Ñï) : 
--   propagate (hebb net S) S = propagate net S := by
-- --------------------------------------------------------------------
--   apply ext
--   intro (n : ‚Ñï)
--   simp only [Membership.mem, Set.Mem, propagate]

--   -- By induction on the layer of the net containing n
--   generalize hL : layer net n = L
--   induction L using Nat.case_strong_induction_on generalizing n

--   -- Base Step
--   case hz =>
--     apply Iff.intro
--     case mp => 
--       simp only [propagate_acc]
--       exact fun x => x
--     case mpr => 
--       simp only [propagate_acc]
--       exact fun x => x

--   -- Inductive Step
--   case hi k IH => 
--     apply Iff.intro

--     -- Forward Direction
--     case mp => 
--       intro h‚ÇÅ
--       simp only [propagate_acc] at h‚ÇÅ
--       simp only [propagate_acc]

--       cases h‚ÇÅ
--       case inl h‚ÇÇ => exact Or.inl h‚ÇÇ
--       case inr h‚ÇÇ =>
--         apply Or.inr

--         -- TODO: This is the stuff that should go in the activ_agree lemma!        
--         simp
--         simp at h‚ÇÇ
--         sorry
--         -- I do not have the tools to show this at this point.
--         -- I need a lemma about activations in the hebbian updated net.

--         -- show_term convert h‚ÇÇ

--     -- Backwards Direction
--     case mpr => sorry

-- This says that 'hebb_star' is a fixed point of 'hebb'
-- (with respect to ‚â°).  i.e. in the following sense, f(X) = X:
--   hebb(X, S) ‚â° X,
-- where X = hebb_star net S
-- 
-- I may need additional lemmas (e.g. an activation function
-- within Prop(S) in hebb_star will simply go through.)
-- 
-- One important lemma:  If an edge is not in the propagation of S,
-- its weight is unaffected.
-- 
-- NOTE: Not sure if I need this before.  It's interesting,
-- but I'm not sure if it helps with the proof for the reduction.
-- --------------------------------------------------------------------
-- theorem hebb_star_is_fixed_point (net : BFNN) (S : Set ‚Ñï) : 
--   hebb (hebb_star net S) S ‚â° hebb_star net S := by 
-- --------------------------------------------------------------------
--   sorry


-- ANOTHER THEOREM
-- that I didn't end up using
-- --------------------------------------------------------------------
-- theorem propagate_reach_inclusion (net : BFNN) : ‚àÄ (S : Set ‚Ñï),
--   propagate net S ‚äÜ reachable net S := by
-- --------------------------------------------------------------------
--   sorry

-- --------------------------------------------------------------------
-- lemma minimal_cause_helper (net : BFNN) : ‚àÄ (A B : Set ‚Ñï), ‚àÄ (n : ‚Ñï),
--   n ‚àà reachedby net B
--   ‚Üí (n ‚àà propagate net A
--   ‚Üî n ‚àà propagate net (A ‚à© reachedby net B)) := by
-- --------------------------------------------------------------------
--   intro (A : Set ‚Ñï) (B : Set ‚Ñï)
--   intro (n : ‚Ñï)
--   intro (h‚ÇÅ : n ‚àà reachedby net B)
--   simp only [Membership.mem, Set.Mem, propagate]

--   -- By induction on the layer of the net containing n
--   generalize hL : layer net n = L
--   induction L using Nat.case_strong_induction_on generalizing n
  
--   -- Base Step
--   case hz => 
--     apply Iff.intro
--     case mp => 
--       intro h‚ÇÇ
--       simp only [propagate_acc]
--       simp only [propagate_acc] at h‚ÇÇ
--       exact ‚ü®h‚ÇÇ, h‚ÇÅ‚ü©

--     case mpr => 
--       intro h‚ÇÇ
--       simp only [propagate_acc]
--       simp only [propagate_acc] at h‚ÇÇ
--       exact h‚ÇÇ.1

--   -- Inductive Step
--   case hi k IH => 
--     apply Iff.intro

--     -- Forward Direction
--     case mp => 
--       intro h‚ÇÇ

--       -- By cases; either n ‚àà A or not.
--       by_cases n ‚àà A
--       case pos => 
--         -- This case is trivial (just apply Extens)
--         rw [symm hL]
--         have h‚ÇÉ : n ‚àà A ‚à© reachedby net B := ‚ü®h, h‚ÇÅ‚ü© 
--         exact @propagate_acc_is_extens net _ _ h‚ÇÉ
--       case neg => 
--         -- If n ‚àâ A, then n ‚àâ A ‚à© reachedby net B
--         have h‚ÇÉ : n ‚àâ A ‚à© reachedby net B := 
--           fun n_in_A => absurd n_in_A.1 h
        
--         -- Just some simplifications and rewriting definitions
--         rw [simp_propagate_acc net h] at h‚ÇÇ
--         rw [simp_propagate_acc net h‚ÇÉ]

--         -- TODO: This is the stuff that should go in the activ_agree lemma!
--         simp
--         simp at h‚ÇÇ
--         convert h‚ÇÇ using 4
--         rename_i i
--         generalize hm : List.get! (predecessors net.graph n).data i = m
--         generalize hLm : layer net m = Lm

--         -- Apply the inductive hypothesis!
--         have h‚ÇÑ : m ‚àà preds net n := by
--           rw [symm hm]
--           simp [preds]
--           exact get!_mem (predecessors net.graph n).data i
--         have h‚ÇÖ : Lm ‚â§ k := by
--           rw [symm hLm]
--           apply Nat.lt_succ.mp
--           rw [symm hL]
--           exact preds_decreasing net m n h‚ÇÑ
--         have h‚ÇÜ : m ‚àà reachedby net B :=
--           match h‚ÇÅ with
--           | ‚ü®x, hx‚ü© => ‚ü®x, ‚ü®hx.1, hasPath_trans _ (preds_path _ h‚ÇÑ) hx.2‚ü©‚ü©
--         exact (symm (IH Lm h‚ÇÖ m h‚ÇÜ hLm).to_eq).to_iff

--     -- Backwards Direction (should be similar)
--     case mpr =>
--       intro h‚ÇÇ

--       -- By cases; either n ‚àà A or not.
--       by_cases n ‚àà A
--       case pos => 
--         -- This case is trivial (just apply Extens)
--         rw [symm hL]
--         exact @propagate_acc_is_extens net _ _ h
--       case neg => 
--         -- If n ‚àâ A, then n ‚àâ A ‚à© reachedby net B
--         have h‚ÇÉ : n ‚àâ A ‚à© reachedby net B := 
--           fun n_in_A => absurd n_in_A.1 h
        
--         -- Just some simplifications and rewriting definitions
--         rw [simp_propagate_acc net h‚ÇÉ] at h‚ÇÇ
--         rw [simp_propagate_acc net h]

--         -- TODO: This is the stuff that should go in the activ_agree lemma!
--         simp
--         simp at h‚ÇÇ
--         convert h‚ÇÇ using 4
--         rename_i i
--         generalize hm : List.get! (predecessors net.graph n).data i = m
--         generalize hLm : layer net m = Lm

--         -- Apply the inductive hypothesis!
--         have h‚ÇÑ : m ‚àà preds net n := by
--           rw [symm hm]
--           simp [preds]
--           exact get!_mem (predecessors net.graph n).data i
--         have h‚ÇÖ : Lm ‚â§ k := by
--           rw [symm hLm]
--           apply Nat.lt_succ.mp
--           rw [symm hL]
--           exact preds_decreasing net m n h‚ÇÑ
--         have h‚ÇÜ : m ‚àà reachedby net B :=
--           match h‚ÇÅ with
--           | ‚ü®x, hx‚ü© => ‚ü®x, ‚ü®hx.1, hasPath_trans _ (preds_path _ h‚ÇÑ) hx.2‚ü©‚ü©
--         exact IH Lm h‚ÇÖ m h‚ÇÜ hLm


-- -- This is the actual property I want, re-written with conditionals
-- -- in mind
-- --------------------------------------------------------------------
-- theorem minimal_cause (net : BFNN) : ‚àÄ (A B : Set ‚Ñï),
--   B ‚äÜ propagate net A
--   ‚Üî B ‚äÜ propagate net (A ‚à© reachedby net B) := by
-- --------------------------------------------------------------------
--   intro (A : Set ‚Ñï) (B : Set ‚Ñï)
--   apply Iff.intro
--   case mp => exact fun h‚ÇÅ n h‚ÇÇ => (minimal_cause_helper net _ _ _ 
--     (reachedby_is_extens _ _ h‚ÇÇ)).mp (h‚ÇÅ h‚ÇÇ)
--   case mpr => exact fun h‚ÇÅ n h‚ÇÇ => (minimal_cause_helper net _ _ _ 
--     (reachedby_is_extens _ _ h‚ÇÇ)).mpr (h‚ÇÅ h‚ÇÇ)



/-‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  Update Policy: "Make neurons fire together"
  (without regard for the edges of the net)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê-/

/-
*Notes*
The basic idea is that we take a subset A of the graph, and
increase our preference for nodes in A as much as possible.
We do this by: 1) completing the graph A, and 2) maximizing all
of the edges within this completed graph.  The overall effect
is that for all neurons m, n ‚àà A, m fires exactly when n fires.

But this operation results in a graph with cycles -- so [A] Prop(B)
is not well-defined!  But we can do something equivalent:
Take all the nodes in A, and quotient them.  This results in a
net where all the nodes m, n ‚àà A "fire as one".

So we need:
  [Def] Define 'complete_and_max' update
  [Def] Define 'fire_together' update (the quotient structure)
  [Thm] Prove that the two updates are equivalent.
        (We can't use Prop for this -- maybe I can prove
         that the 'activ' for both nets agrees?)
  [Thm] Find a reduction for 'fire_together' and prove it.
        (It should look like the dual of Lexicographic Upgrade)
  [Cor] 'fire_together' is the dual of Lexicographic Upgrade
    [Depends] 
      Preference Models
      Lexicographic Upgrade
      Lexicographic Upgrade reduction
      Model translation from pref models ‚ü∑ nets
-/

-- def complete_and_max (net : BFNN) (A : Set ‚Ñï) : BFNN :=
--   sorry

-- def fire_together (net : BFNN) (A : Set ‚Ñï) : BFNN :=
-- { net with
--   graph := sorry
--   activation := sorry
--   rate := sorry

--   -- binary := sorry
--   binary := sorry
--   acyclic := sorry
--   activ_nondecr := sorry
--   activ_pos := sorry
-- }


-- /-‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
--   Subnets
-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê-/

-- -- A net net‚ÇÅ is a subnet of net‚ÇÇ (net‚ÇÅ ‚âº net‚ÇÇ) iff for all
-- -- sets S, every node activated in the propagation of S
-- -- in net‚ÇÅ is activated in the propagation of S in net‚ÇÇ.
-- -- (They both have the same *propagation structure*)
-- def subnet (net‚ÇÅ net‚ÇÇ : BFNN) : Prop :=
--   ‚àÄ (S : Set ‚Ñï), propagate net‚ÇÅ S ‚äÜ propagate net‚ÇÇ S

-- infixl:65   " ‚âº " => subnet


-- -- Two nets are equivalent if they have the same 
-- -- *propagation structure* (i.e. if their propagations agree 
-- -- for every set S)
-- def net_eq (net‚ÇÅ net‚ÇÇ : BFNN) : Prop :=
--   net‚ÇÅ ‚âº net‚ÇÇ ‚àß net‚ÇÇ ‚âº net‚ÇÅ

-- infixl:65   " ‚â° " => net_eq


-- -- A super easy example, just to briefly test ‚âº and ‚â°
-- example : example_net ‚â° example_net :=
--   ‚ü®fun _ _ h => h, fun _ _ h => h‚ü©  
